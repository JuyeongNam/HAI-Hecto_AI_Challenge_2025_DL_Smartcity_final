{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OdM98q3YTu9q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_NUM = 1\n",
        "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.set_device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h_z37-sCVW_6"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    \"IMG_SIZE\": 224,\n",
        "    \"NUM_CLASSES\": 396,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 500,\n",
        "    \"LR\": 1e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-2,\n",
        "    \"PATIENCE\": 5,\n",
        "    \"SEED\": 42,\n",
        "    # AMP & Gradâ€‘clip\n",
        "    \"USE_AMP\": False,\n",
        "    \"GRAD_CLIP\": 1.0,\n",
        "}\n",
        "\n",
        "BASE_DIR = Path(r\"D:/dacon_HAI\")\n",
        "PATHS = {\n",
        "    \"BASE\": BASE_DIR,\n",
        "    \"TRAIN\": BASE_DIR / \"open\" / \"train\",\n",
        "    \"TEST\":  BASE_DIR / \"open\" / \"test\",\n",
        "    \"CKPT\":  BASE_DIR / \"checkpoints\",\n",
        "    \"LOG\":   BASE_DIR / \"logs\",\n",
        "    \"SUBMIT\": BASE_DIR / \"submission\",\n",
        "}\n",
        "for p in PATHS.values():\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Normalization stats ---\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FXjhJninT2py"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "seed_everything(CFG[\"SEED\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdr2jbS-Oixh"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = []\n",
        "\n",
        "        if self.is_test:\n",
        "            for fname in sorted(os.listdir(root_dir)):\n",
        "                if fname.lower().endswith('.jpg'):\n",
        "                    img_path = os.path.join(root_dir, fname)\n",
        "                    self.samples.append((img_path,))\n",
        "        else:\n",
        "            self.classes = sorted(os.listdir(root_dir))\n",
        "            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "            for cls_name in self.classes:\n",
        "                cls_folder = os.path.join(root_dir, cls_name)\n",
        "                for fname in os.listdir(cls_folder):\n",
        "                    if fname.lower().endswith('.jpg'):\n",
        "                        img_path = os.path.join(cls_folder, fname)\n",
        "                        label = self.class_to_idx[cls_name]\n",
        "                        self.samples.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            img_path = self.samples[idx][0]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            img_path, label = self.samples[idx]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "spatial_aug = transforms.RandomChoice([\n",
        "    transforms.RandomResizedCrop(CFG[\"IMG_SIZE\"], scale=(0.8, 1.0)),\n",
        "    transforms.CenterCrop(CFG[\"IMG_SIZE\"]),\n",
        "])\n",
        "geom_aug = transforms.RandomChoice([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.RandomVerticalFlip(p=1.0),\n",
        "    transforms.RandomRotation(degrees=(-30, 30), interpolation=transforms.InterpolationMode.BILINEAR, fill=0),\n",
        "])\n",
        "pixel_aug = transforms.RandomChoice([\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 5.0)),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "])\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "    transforms.RandomApply([spatial_aug], p=0.5),\n",
        "    transforms.RandomApply([geom_aug],    p=0.5),\n",
        "    transforms.RandomApply([pixel_aug],   p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKc8X1W9UL1d",
        "outputId": "910f38aa-c1fd-4b4d-dcb5-5604176242e0"
      },
      "outputs": [],
      "source": [
        "full_ds = CustomImageDataset(PATHS[\"TRAIN\"], transform=train_tf, is_test=False)\n",
        "print(f\"Full data: {len(full_ds):,}\")\n",
        "\n",
        "targets = [label for _, label in full_ds.samples]\n",
        "class_names = full_ds.classes\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=CFG['SEED']\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = Subset(CustomImageDataset(PATHS[\"TRAIN\"], transform=train_tf, is_test=False), train_idx)\n",
        "val_dataset = Subset(CustomImageDataset(PATHS[\"TRAIN\"], transform=val_tf, is_test=False), val_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "test_root = PATHS[\"TEST\"]\n",
        "test_dataset = CustomImageDataset(test_root, transform=val_tf, is_test=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(f\"Number of train imgs: {len(train_dataset)}, Number of valid imgs: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resnext50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                        Output Shape              Param #\n",
            "===============================================================================================\n",
            "Resnext50                                     [1, 396]                  --\n",
            "â”œâ”€ResNet: 1-1                                 [1, 2048]                 --\n",
            "â”‚    â””â”€Conv2d: 2-1                            [1, 64, 112, 112]         9,408\n",
            "â”‚    â””â”€BatchNorm2d: 2-2                       [1, 64, 112, 112]         128\n",
            "â”‚    â””â”€ReLU: 2-3                              [1, 64, 112, 112]         --\n",
            "â”‚    â””â”€MaxPool2d: 2-4                         [1, 64, 56, 56]           --\n",
            "â”‚    â””â”€Sequential: 2-5                        [1, 256, 56, 56]          --\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-1                   [1, 256, 56, 56]          63,488\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-2                   [1, 256, 56, 56]          71,168\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-3                   [1, 256, 56, 56]          71,168\n",
            "â”‚    â””â”€Sequential: 2-6                        [1, 512, 28, 28]          --\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-4                   [1, 512, 28, 28]          349,184\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-5                   [1, 512, 28, 28]          282,624\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-6                   [1, 512, 28, 28]          282,624\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-7                   [1, 512, 28, 28]          282,624\n",
            "â”‚    â””â”€Sequential: 2-7                        [1, 1024, 14, 14]         --\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-8                   [1, 1024, 14, 14]         1,390,592\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-9                   [1, 1024, 14, 14]         1,126,400\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-10                  [1, 1024, 14, 14]         1,126,400\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-11                  [1, 1024, 14, 14]         1,126,400\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-12                  [1, 1024, 14, 14]         1,126,400\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-13                  [1, 1024, 14, 14]         1,126,400\n",
            "â”‚    â””â”€Sequential: 2-8                        [1, 2048, 7, 7]           --\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-14                  [1, 2048, 7, 7]           5,550,080\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-15                  [1, 2048, 7, 7]           4,497,408\n",
            "â”‚    â”‚    â””â”€Bottleneck: 3-16                  [1, 2048, 7, 7]           4,497,408\n",
            "â”‚    â””â”€AdaptiveAvgPool2d: 2-9                 [1, 2048, 1, 1]           --\n",
            "â”‚    â””â”€Identity: 2-10                         [1, 2048]                 --\n",
            "â”œâ”€Linear: 1-2                                 [1, 396]                  811,404\n",
            "===============================================================================================\n",
            "Total params: 23,791,308\n",
            "Trainable params: 23,791,308\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 4.23\n",
            "===============================================================================================\n",
            "Input size (MB): 0.60\n",
            "Forward/backward pass size (MB): 230.41\n",
            "Params size (MB): 95.17\n",
            "Estimated Total Size (MB): 326.18\n",
            "===============================================================================================\n"
          ]
        }
      ],
      "source": [
        "class Resnext50(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Resnext50, self).__init__()\n",
        "        self.backbone = models.resnext50_32x4d(pretrained=True)\n",
        "        self.feature_dim = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.head = nn.Linear(self.feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "    \n",
        "model = Resnext50(num_classes=CFG['NUM_CLASSES'])\n",
        "print(summary(model, input_size=(1, 3, CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"],)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Hyper-params**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
        "\n",
        "steps_per_epoch = len(train_loader)\n",
        "warmup_steps = 5 * steps_per_epoch\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / float(max(1, warmup_steps))\n",
        "    progress = (step - warmup_steps) / float(max(1, CFG[\"EPOCHS\"] * steps_per_epoch - warmup_steps))\n",
        "    return 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress)))\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv6vSEwcYBF2"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 5, delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "        self.stop = False\n",
        "\n",
        "    def __call__(self, metric: float):\n",
        "        if self.best is None or metric < self.best - self.delta:\n",
        "            self.best = metric\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "early_stopper = EarlyStopping(patience=CFG[\"PATIENCE\"], delta=0.001)\n",
        "\n",
        "log_file = PATHS[\"LOG\"] / \"train_Resnext50.log\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(message)s\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(log_file, mode=\"w\"),\n",
        "    ],\n",
        ")\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Train net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 14:48:27,964 Epoch 1/500 | Time 590.4s | TrainLoss 5.8568 | ValLoss 5.1895 | ValAcc 18.80% | LogLoss 5.1897\n",
            "2025-06-12 14:48:28,133 [Checkpoint] Saved at epoch 1 (LogLoss 5.1897)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 14:59:49,784 Epoch 2/500 | Time 681.6s | TrainLoss 4.0863 | ValLoss 2.0532 | ValAcc 62.24% | LogLoss 2.0531\n",
            "2025-06-12 14:59:49,956 [Checkpoint] Saved at epoch 2 (LogLoss 2.0531)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:11:14,069 Epoch 3/500 | Time 684.1s | TrainLoss 1.5152 | ValLoss 0.5807 | ValAcc 84.29% | LogLoss 0.5806\n",
            "2025-06-12 15:11:14,267 [Checkpoint] Saved at epoch 3 (LogLoss 0.5806)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:22:31,082 Epoch 4/500 | Time 676.8s | TrainLoss 0.5673 | ValLoss 0.3476 | ValAcc 88.23% | LogLoss 0.3476\n",
            "2025-06-12 15:22:31,273 [Checkpoint] Saved at epoch 4 (LogLoss 0.3476)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:34:17,036 Epoch 5/500 | Time 705.8s | TrainLoss 0.3607 | ValLoss 0.2742 | ValAcc 91.33% | LogLoss 0.2743\n",
            "2025-06-12 15:34:17,226 [Checkpoint] Saved at epoch 5 (LogLoss 0.2743)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:47:54,925 Epoch 6/500 | Time 817.7s | TrainLoss 0.2782 | ValLoss 0.2482 | ValAcc 91.54% | LogLoss 0.2482\n",
            "2025-06-12 15:47:55,107 [Checkpoint] Saved at epoch 6 (LogLoss 0.2482)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:01:20,291 Epoch 7/500 | Time 805.2s | TrainLoss 0.2137 | ValLoss 0.2241 | ValAcc 92.70% | LogLoss 0.2241\n",
            "2025-06-12 16:01:20,478 [Checkpoint] Saved at epoch 7 (LogLoss 0.2241)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:15:01,861 Epoch 8/500 | Time 821.4s | TrainLoss 0.1832 | ValLoss 0.2048 | ValAcc 93.03% | LogLoss 0.2049\n",
            "2025-06-12 16:15:02,039 [Checkpoint] Saved at epoch 8 (LogLoss 0.2049)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:28:34,368 Epoch 9/500 | Time 812.3s | TrainLoss 0.1622 | ValLoss 0.1843 | ValAcc 93.96% | LogLoss 0.1844\n",
            "2025-06-12 16:28:34,555 [Checkpoint] Saved at epoch 9 (LogLoss 0.1844)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:41:46,128 Epoch 10/500 | Time 791.6s | TrainLoss 0.1437 | ValLoss 0.2026 | ValAcc 93.53% | LogLoss 0.2027\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:54:40,501 Epoch 11/500 | Time 774.4s | TrainLoss 0.1454 | ValLoss 0.2213 | ValAcc 93.22% | LogLoss 0.2211\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:07:32,931 Epoch 12/500 | Time 772.4s | TrainLoss 0.1241 | ValLoss 0.1571 | ValAcc 94.82% | LogLoss 0.1572\n",
            "2025-06-12 17:07:33,127 [Checkpoint] Saved at epoch 12 (LogLoss 0.1572)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:20:32,954 Epoch 13/500 | Time 779.8s | TrainLoss 0.1073 | ValLoss 0.1854 | ValAcc 94.15% | LogLoss 0.1852\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:33:49,623 Epoch 14/500 | Time 796.7s | TrainLoss 0.1060 | ValLoss 0.1741 | ValAcc 94.70% | LogLoss 0.1741\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:46:53,190 Epoch 15/500 | Time 783.6s | TrainLoss 0.1030 | ValLoss 0.1721 | ValAcc 94.70% | LogLoss 0.1721\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:59:27,439 Epoch 16/500 | Time 754.2s | TrainLoss 0.0950 | ValLoss 0.1668 | ValAcc 95.04% | LogLoss 0.1669\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 18:11:56,380 Epoch 17/500 | Time 748.9s | TrainLoss 0.0873 | ValLoss 0.1671 | ValAcc 95.29% | LogLoss 0.1670\n",
            "2025-06-12 18:11:56,382 Early stopping triggered.\n",
            "2025-06-12 18:11:56,382 Total training time: 3:33:18\n"
          ]
        }
      ],
      "source": [
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"[{epoch}/{CFG['EPOCHS']}] Train\", leave=False)\n",
        "    for step, (imgs, labels) in enumerate(pbar, start=1):\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_probs.append(F.softmax(logits, dim=1).cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    class_idx = list(range(CFG[\"NUM_CLASSES\"]))\n",
        "    answer_df = pd.DataFrame({\"ID\": np.arange(len(all_labels)), \"label\": all_labels})\n",
        "    submission_df = pd.DataFrame(all_probs, columns=class_idx)\n",
        "    submission_df.insert(0, \"ID\", submission_df.index)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    logloss = log_loss(answer_df[\"label\"], all_probs, labels=class_idx)\n",
        "\n",
        "    return avg_val_loss, acc, logloss\n",
        "\n",
        "BEST_LOSS = float(\"inf\")\n",
        "start = time.time()\n",
        "for epoch in range(1, CFG[\"EPOCHS\"] + 1):\n",
        "    t0 = time.time()\n",
        "    tr_loss = train_one_epoch(epoch)\n",
        "    val_loss, val_acc, val_logloss = validate(epoch)\n",
        "    epoch_dur = time.time() - t0\n",
        "\n",
        "    logger.info(\n",
        "        f\"Epoch {epoch}/{CFG['EPOCHS']} | \"\n",
        "        f\"Time {epoch_dur:.1f}s | \"\n",
        "        f\"TrainLoss {tr_loss:.4f} | ValLoss {val_loss:.4f} | \"\n",
        "        f\"ValAcc {val_acc:.2f}% | LogLoss {val_logloss:.4f}\"\n",
        "    )\n",
        "\n",
        "    if val_logloss < BEST_LOSS:\n",
        "        BEST_LOSS = val_logloss\n",
        "        ckpt_path = PATHS[\"CKPT\"] / \"best_Resnext50_250612_f_b32.pth\"\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        logger.info(f\"[Checkpoint] Saved at epoch {epoch} (LogLoss {BEST_LOSS:.4f})\")\n",
        "\n",
        "    early_stopper(val_logloss)\n",
        "    if early_stopper.stop:\n",
        "        logger.info(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "logger.info(f\"Total training time: {datetime.timedelta(seconds=int(time.time() - start))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Eval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJJe3UnEgA6y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = Resnext50(num_classes=CFG['NUM_CLASSES'])\n",
        "\n",
        "model.load_state_dict(torch.load(PATHS[\"CKPT\"] / \"best_Resnext50_250612_f_b32.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        for prob in probs.cpu():\n",
        "            result = {\n",
        "                class_names[i]: prob[i].item()\n",
        "                for i in range(len(class_names))\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "pred = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submission\n",
        "submission = pd.read_csv(os.path.join(PATHS[\"SUBMIT\"], 'submission_JN.csv'), encoding='utf-8-sig')\n",
        "\n",
        "class_columns = submission.columns[1:]\n",
        "pred = pred[class_columns]\n",
        "\n",
        "submission[class_columns] = pred.values\n",
        "submission.to_csv(os.path.join(PATHS[\"SUBMIT\"], 'Resnext50_250612_b32_submission_JN.csv'), index=False, encoding='utf-8-sig')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO3li9OAnGka1if4d+usg7h",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1tXZe3gyQHzQnlNjadpS1QxSHl-_dRwZL",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "HAI_project",
      "language": "python",
      "name": "hai_project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
