import os
import shutil
import torch
import torch.nn as nn
import numpy as np
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torchvision.models import resnet18, ResNet18_Weights
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import log_loss

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
num_classes = 396
batch_size = 16
epochs = 10
lr = 3e-4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ê²½ë¡œ ì„¤ì •
base_path = "C:/Users/fbwod/Desktop/DL_Oldcar"
train_path = os.path.join(base_path, "train")
val_path = os.path.join(base_path, "val")

# âœ… val í´ë” ìƒì„± (í•œ ë²ˆë§Œ ì‹¤í–‰ë¨)
if not os.path.exists(val_path):
    print("ğŸ”§ val/ í´ë” ìƒì„± ì¤‘ (trainì—ì„œ 10% ë¶„ë¦¬)...")
    class_names = os.listdir(train_path)
    image_paths, labels = [], []
    for cls in class_names:
        cls_path = os.path.join(train_path, cls)
        files = [f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.png'))]
        for f in files:
            image_paths.append(os.path.join(cls_path, f))
            labels.append(cls)

    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)
    _, val_idx = next(splitter.split(image_paths, labels))

    for i in val_idx:
        src = image_paths[i]
        cls = labels[i]
        dst_dir = os.path.join(val_path, cls)
        os.makedirs(dst_dir, exist_ok=True)
        shutil.copy(src, os.path.join(dst_dir, os.path.basename(src)))
    print("âœ… val ë°ì´í„° ë¶„í•  ì™„ë£Œ.")

# ì „ì²˜ë¦¬ ì •ì˜
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

# ë°ì´í„°ì…‹ ë¡œë”©
train_dataset = ImageFolder(train_path, transform=train_transform)
val_dataset = ImageFolder(val_path, transform=val_transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# ëª¨ë¸ ì •ì˜ (ResNet18)
weights = ResNet18_Weights.DEFAULT
model = resnet18(weights=weights)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model.to(device)

# ì†ì‹¤í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

# í•™ìŠµ ë£¨í”„
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        correct += (outputs.argmax(1) == labels).sum().item()
        total += labels.size(0)

        if i % 10 == 0:
            print(f"Epoch {epoch+1} | Batch {i+1}/{len(train_loader)} | Loss: {loss.item():.4f}")

    train_acc = correct / total

    # ğŸ” Validation Accuracy + Log Loss
    model.eval()
    val_correct = 0
    val_total = 0
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            all_probs.append(probs.cpu().numpy())
            all_labels.append(labels.cpu().numpy())
            val_correct += (outputs.argmax(1) == labels).sum().item()
            val_total += labels.size(0)

    val_acc = val_correct / val_total
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    val_logloss = log_loss(all_labels, all_probs, labels=np.arange(num_classes))

    print(f"[Epoch {epoch+1}] Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val LogLoss: {val_logloss:.4f}, Loss: {running_loss:.4f}")

# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), "resnet18_finetuned.pth")
print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: resnet18_finetuned.pth")
