{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58398b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: networkx in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\fbwod\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\fbwod\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2645beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     --------------------------------------- 11.1/11.1 MB 10.1 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\fbwod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "     ---------------------------------------- 41.3/41.3 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "     -------------------------------------- 307.7/307.7 KB 9.6 MB/s eta 0:00:00\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\fbwod\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0fd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8bc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
    "num_classes = 396\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "base_path = \"C:/Users/fbwod/Desktop/DL_Oldcar\"\n",
    "train_path = os.path.join(base_path, \"train\")\n",
    "val_path = os.path.join(base_path, \"val\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163c37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ val Ìè¥Îçî ÏÉùÏÑ± (Ìïú Î≤àÎßå Ïã§ÌñâÎê®)\n",
    "if not os.path.exists(val_path):\n",
    "    print(\"üîß val/ Ìè¥Îçî ÏÉùÏÑ± Ï§ë (trainÏóêÏÑú 10% Î∂ÑÎ¶¨)...\")\n",
    "    class_names = os.listdir(train_path)\n",
    "    image_paths, labels = [], []\n",
    "    for cls in class_names:\n",
    "        cls_path = os.path.join(train_path, cls)\n",
    "        files = [f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.png'))]\n",
    "        for f in files:\n",
    "            image_paths.append(os.path.join(cls_path, f))\n",
    "            labels.append(cls)\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "    _, val_idx = next(splitter.split(image_paths, labels))\n",
    "\n",
    "    for i in val_idx:\n",
    "        src = image_paths[i]\n",
    "        cls = labels[i]\n",
    "        dst_dir = os.path.join(val_path, cls)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        shutil.copy(src, os.path.join(dst_dir, os.path.basename(src)))\n",
    "    print(\"‚úÖ val Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† ÏôÑÎ£å.\")\n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎî©\n",
    "train_dataset = ImageFolder(train_path, transform=train_transform)\n",
    "val_dataset = ImageFolder(val_path, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d9b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 1/2072 | Loss: 6.0656\n",
      "Epoch 1 | Batch 11/2072 | Loss: 6.3511\n",
      "Epoch 1 | Batch 21/2072 | Loss: 6.1276\n",
      "Epoch 1 | Batch 31/2072 | Loss: 5.7313\n",
      "Epoch 1 | Batch 41/2072 | Loss: 6.5180\n",
      "Epoch 1 | Batch 51/2072 | Loss: 5.9967\n",
      "Epoch 1 | Batch 61/2072 | Loss: 6.4439\n",
      "Epoch 1 | Batch 71/2072 | Loss: 6.0497\n",
      "Epoch 1 | Batch 81/2072 | Loss: 6.2242\n",
      "Epoch 1 | Batch 91/2072 | Loss: 5.8233\n",
      "Epoch 1 | Batch 101/2072 | Loss: 6.8509\n",
      "Epoch 1 | Batch 111/2072 | Loss: 6.0142\n",
      "Epoch 1 | Batch 121/2072 | Loss: 5.3857\n",
      "Epoch 1 | Batch 131/2072 | Loss: 5.6796\n",
      "Epoch 1 | Batch 141/2072 | Loss: 5.6563\n",
      "Epoch 1 | Batch 151/2072 | Loss: 5.6854\n",
      "Epoch 1 | Batch 161/2072 | Loss: 5.4866\n",
      "Epoch 1 | Batch 171/2072 | Loss: 5.5678\n",
      "Epoch 1 | Batch 181/2072 | Loss: 5.1941\n",
      "Epoch 1 | Batch 191/2072 | Loss: 5.4829\n",
      "Epoch 1 | Batch 201/2072 | Loss: 4.8271\n",
      "Epoch 1 | Batch 211/2072 | Loss: 4.7423\n",
      "Epoch 1 | Batch 221/2072 | Loss: 5.0510\n",
      "Epoch 1 | Batch 231/2072 | Loss: 5.7995\n",
      "Epoch 1 | Batch 241/2072 | Loss: 5.2103\n",
      "Epoch 1 | Batch 251/2072 | Loss: 4.9420\n",
      "Epoch 1 | Batch 261/2072 | Loss: 4.5081\n",
      "Epoch 1 | Batch 271/2072 | Loss: 3.9562\n",
      "Epoch 1 | Batch 281/2072 | Loss: 4.1788\n",
      "Epoch 1 | Batch 291/2072 | Loss: 3.9243\n",
      "Epoch 1 | Batch 301/2072 | Loss: 3.9843\n",
      "Epoch 1 | Batch 311/2072 | Loss: 4.2844\n",
      "Epoch 1 | Batch 321/2072 | Loss: 4.6055\n",
      "Epoch 1 | Batch 331/2072 | Loss: 3.2533\n",
      "Epoch 1 | Batch 341/2072 | Loss: 4.2762\n",
      "Epoch 1 | Batch 351/2072 | Loss: 3.9675\n",
      "Epoch 1 | Batch 361/2072 | Loss: 4.2210\n",
      "Epoch 1 | Batch 371/2072 | Loss: 4.2594\n",
      "Epoch 1 | Batch 381/2072 | Loss: 3.7499\n",
      "Epoch 1 | Batch 391/2072 | Loss: 3.5077\n",
      "Epoch 1 | Batch 401/2072 | Loss: 4.6812\n",
      "Epoch 1 | Batch 411/2072 | Loss: 3.4991\n",
      "Epoch 1 | Batch 421/2072 | Loss: 3.5366\n",
      "Epoch 1 | Batch 431/2072 | Loss: 3.6136\n",
      "Epoch 1 | Batch 441/2072 | Loss: 3.8737\n",
      "Epoch 1 | Batch 451/2072 | Loss: 3.9401\n",
      "Epoch 1 | Batch 461/2072 | Loss: 3.9387\n",
      "Epoch 1 | Batch 471/2072 | Loss: 3.9289\n",
      "Epoch 1 | Batch 481/2072 | Loss: 3.7230\n",
      "Epoch 1 | Batch 491/2072 | Loss: 2.8329\n",
      "Epoch 1 | Batch 501/2072 | Loss: 2.6140\n",
      "Epoch 1 | Batch 511/2072 | Loss: 2.7062\n",
      "Epoch 1 | Batch 521/2072 | Loss: 2.3971\n",
      "Epoch 1 | Batch 531/2072 | Loss: 3.6645\n",
      "Epoch 1 | Batch 541/2072 | Loss: 2.2308\n",
      "Epoch 1 | Batch 551/2072 | Loss: 2.6515\n",
      "Epoch 1 | Batch 561/2072 | Loss: 2.7098\n",
      "Epoch 1 | Batch 571/2072 | Loss: 3.0039\n",
      "Epoch 1 | Batch 581/2072 | Loss: 3.3449\n",
      "Epoch 1 | Batch 591/2072 | Loss: 2.5555\n",
      "Epoch 1 | Batch 601/2072 | Loss: 2.9257\n",
      "Epoch 1 | Batch 611/2072 | Loss: 2.6355\n",
      "Epoch 1 | Batch 621/2072 | Loss: 2.5277\n",
      "Epoch 1 | Batch 631/2072 | Loss: 2.1871\n",
      "Epoch 1 | Batch 641/2072 | Loss: 2.2317\n",
      "Epoch 1 | Batch 651/2072 | Loss: 2.5284\n",
      "Epoch 1 | Batch 661/2072 | Loss: 2.8031\n",
      "Epoch 1 | Batch 671/2072 | Loss: 2.5489\n",
      "Epoch 1 | Batch 681/2072 | Loss: 2.2161\n",
      "Epoch 1 | Batch 691/2072 | Loss: 2.1975\n",
      "Epoch 1 | Batch 701/2072 | Loss: 2.5112\n",
      "Epoch 1 | Batch 711/2072 | Loss: 1.6439\n",
      "Epoch 1 | Batch 721/2072 | Loss: 2.3488\n",
      "Epoch 1 | Batch 731/2072 | Loss: 2.0379\n",
      "Epoch 1 | Batch 741/2072 | Loss: 1.5346\n",
      "Epoch 1 | Batch 751/2072 | Loss: 2.5149\n",
      "Epoch 1 | Batch 761/2072 | Loss: 1.7799\n",
      "Epoch 1 | Batch 771/2072 | Loss: 2.1899\n",
      "Epoch 1 | Batch 781/2072 | Loss: 1.8138\n",
      "Epoch 1 | Batch 791/2072 | Loss: 1.9296\n",
      "Epoch 1 | Batch 801/2072 | Loss: 2.0739\n",
      "Epoch 1 | Batch 811/2072 | Loss: 1.5283\n",
      "Epoch 1 | Batch 821/2072 | Loss: 1.8227\n",
      "Epoch 1 | Batch 831/2072 | Loss: 2.0137\n",
      "Epoch 1 | Batch 841/2072 | Loss: 2.4209\n",
      "Epoch 1 | Batch 851/2072 | Loss: 2.1894\n",
      "Epoch 1 | Batch 861/2072 | Loss: 1.9073\n",
      "Epoch 1 | Batch 871/2072 | Loss: 1.4863\n",
      "Epoch 1 | Batch 881/2072 | Loss: 1.3185\n",
      "Epoch 1 | Batch 891/2072 | Loss: 1.3149\n",
      "Epoch 1 | Batch 901/2072 | Loss: 2.3248\n",
      "Epoch 1 | Batch 911/2072 | Loss: 1.2600\n",
      "Epoch 1 | Batch 921/2072 | Loss: 1.8347\n",
      "Epoch 1 | Batch 931/2072 | Loss: 1.7282\n",
      "Epoch 1 | Batch 941/2072 | Loss: 1.6701\n",
      "Epoch 1 | Batch 951/2072 | Loss: 1.2980\n",
      "Epoch 1 | Batch 961/2072 | Loss: 1.2866\n",
      "Epoch 1 | Batch 971/2072 | Loss: 1.6092\n",
      "Epoch 1 | Batch 981/2072 | Loss: 1.3412\n",
      "Epoch 1 | Batch 991/2072 | Loss: 1.3280\n",
      "Epoch 1 | Batch 1001/2072 | Loss: 1.6304\n",
      "Epoch 1 | Batch 1011/2072 | Loss: 1.3126\n",
      "Epoch 1 | Batch 1021/2072 | Loss: 1.1871\n",
      "Epoch 1 | Batch 1031/2072 | Loss: 2.2816\n",
      "Epoch 1 | Batch 1041/2072 | Loss: 1.5788\n",
      "Epoch 1 | Batch 1051/2072 | Loss: 1.3258\n",
      "Epoch 1 | Batch 1061/2072 | Loss: 1.5868\n",
      "Epoch 1 | Batch 1071/2072 | Loss: 1.2476\n",
      "Epoch 1 | Batch 1081/2072 | Loss: 1.6764\n",
      "Epoch 1 | Batch 1091/2072 | Loss: 1.6313\n",
      "Epoch 1 | Batch 1101/2072 | Loss: 1.0300\n",
      "Epoch 1 | Batch 1111/2072 | Loss: 1.6786\n",
      "Epoch 1 | Batch 1121/2072 | Loss: 1.0723\n",
      "Epoch 1 | Batch 1131/2072 | Loss: 1.0628\n",
      "Epoch 1 | Batch 1141/2072 | Loss: 1.4837\n",
      "Epoch 1 | Batch 1151/2072 | Loss: 1.7689\n",
      "Epoch 1 | Batch 1161/2072 | Loss: 1.6511\n",
      "Epoch 1 | Batch 1171/2072 | Loss: 0.7911\n",
      "Epoch 1 | Batch 1181/2072 | Loss: 1.1486\n",
      "Epoch 1 | Batch 1191/2072 | Loss: 1.4117\n",
      "Epoch 1 | Batch 1201/2072 | Loss: 1.4162\n",
      "Epoch 1 | Batch 1211/2072 | Loss: 1.5831\n",
      "Epoch 1 | Batch 1221/2072 | Loss: 1.1556\n",
      "Epoch 1 | Batch 1231/2072 | Loss: 1.0536\n",
      "Epoch 1 | Batch 1241/2072 | Loss: 1.2459\n",
      "Epoch 1 | Batch 1251/2072 | Loss: 1.6243\n",
      "Epoch 1 | Batch 1261/2072 | Loss: 1.3548\n",
      "Epoch 1 | Batch 1271/2072 | Loss: 1.3285\n",
      "Epoch 1 | Batch 1281/2072 | Loss: 1.5466\n",
      "Epoch 1 | Batch 1291/2072 | Loss: 1.0299\n",
      "Epoch 1 | Batch 1301/2072 | Loss: 0.7735\n",
      "Epoch 1 | Batch 1311/2072 | Loss: 0.9126\n",
      "Epoch 1 | Batch 1321/2072 | Loss: 1.0385\n",
      "Epoch 1 | Batch 1331/2072 | Loss: 0.7087\n",
      "Epoch 1 | Batch 1341/2072 | Loss: 1.4664\n",
      "Epoch 1 | Batch 1351/2072 | Loss: 1.1136\n",
      "Epoch 1 | Batch 1361/2072 | Loss: 0.6124\n",
      "Epoch 1 | Batch 1371/2072 | Loss: 1.1138\n",
      "Epoch 1 | Batch 1381/2072 | Loss: 0.7136\n",
      "Epoch 1 | Batch 1391/2072 | Loss: 1.1068\n",
      "Epoch 1 | Batch 1401/2072 | Loss: 1.1700\n",
      "Epoch 1 | Batch 1411/2072 | Loss: 0.7113\n",
      "Epoch 1 | Batch 1421/2072 | Loss: 1.2905\n",
      "Epoch 1 | Batch 1431/2072 | Loss: 0.6751\n",
      "Epoch 1 | Batch 1441/2072 | Loss: 1.3188\n",
      "Epoch 1 | Batch 1451/2072 | Loss: 0.9622\n",
      "Epoch 1 | Batch 1461/2072 | Loss: 1.5406\n",
      "Epoch 1 | Batch 1471/2072 | Loss: 0.6500\n",
      "Epoch 1 | Batch 1481/2072 | Loss: 1.0246\n",
      "Epoch 1 | Batch 1491/2072 | Loss: 1.9449\n",
      "Epoch 1 | Batch 1501/2072 | Loss: 0.6096\n",
      "Epoch 1 | Batch 1511/2072 | Loss: 1.2603\n",
      "Epoch 1 | Batch 1521/2072 | Loss: 0.7976\n",
      "Epoch 1 | Batch 1531/2072 | Loss: 1.1111\n",
      "Epoch 1 | Batch 1541/2072 | Loss: 0.7681\n",
      "Epoch 1 | Batch 1551/2072 | Loss: 0.7349\n",
      "Epoch 1 | Batch 1561/2072 | Loss: 0.9670\n",
      "Epoch 1 | Batch 1571/2072 | Loss: 0.9040\n",
      "Epoch 1 | Batch 1581/2072 | Loss: 1.1948\n",
      "Epoch 1 | Batch 1591/2072 | Loss: 0.6282\n",
      "Epoch 1 | Batch 1601/2072 | Loss: 0.8024\n",
      "Epoch 1 | Batch 1611/2072 | Loss: 0.6530\n",
      "Epoch 1 | Batch 1621/2072 | Loss: 0.6671\n",
      "Epoch 1 | Batch 1631/2072 | Loss: 0.7802\n",
      "Epoch 1 | Batch 1641/2072 | Loss: 0.7080\n",
      "Epoch 1 | Batch 1651/2072 | Loss: 0.6298\n",
      "Epoch 1 | Batch 1661/2072 | Loss: 0.6226\n",
      "Epoch 1 | Batch 1671/2072 | Loss: 1.0575\n",
      "Epoch 1 | Batch 1681/2072 | Loss: 0.5876\n",
      "Epoch 1 | Batch 1691/2072 | Loss: 0.8237\n",
      "Epoch 1 | Batch 1701/2072 | Loss: 0.7785\n",
      "Epoch 1 | Batch 1711/2072 | Loss: 0.8693\n",
      "Epoch 1 | Batch 1721/2072 | Loss: 0.8799\n",
      "Epoch 1 | Batch 1731/2072 | Loss: 0.6044\n",
      "Epoch 1 | Batch 1741/2072 | Loss: 0.7905\n",
      "Epoch 1 | Batch 1751/2072 | Loss: 0.8725\n",
      "Epoch 1 | Batch 1761/2072 | Loss: 1.1221\n",
      "Epoch 1 | Batch 1771/2072 | Loss: 0.7230\n",
      "Epoch 1 | Batch 1781/2072 | Loss: 0.7845\n",
      "Epoch 1 | Batch 1791/2072 | Loss: 0.6005\n",
      "Epoch 1 | Batch 1801/2072 | Loss: 0.6980\n",
      "Epoch 1 | Batch 1811/2072 | Loss: 0.4873\n",
      "Epoch 1 | Batch 1821/2072 | Loss: 0.6236\n",
      "Epoch 1 | Batch 1831/2072 | Loss: 0.8403\n",
      "Epoch 1 | Batch 1841/2072 | Loss: 0.7727\n",
      "Epoch 1 | Batch 1851/2072 | Loss: 0.9527\n",
      "Epoch 1 | Batch 1861/2072 | Loss: 0.7672\n",
      "Epoch 1 | Batch 1871/2072 | Loss: 1.2982\n",
      "Epoch 1 | Batch 1881/2072 | Loss: 0.4865\n",
      "Epoch 1 | Batch 1891/2072 | Loss: 0.7308\n",
      "Epoch 1 | Batch 1901/2072 | Loss: 0.5961\n",
      "Epoch 1 | Batch 1911/2072 | Loss: 0.7872\n",
      "Epoch 1 | Batch 1921/2072 | Loss: 0.5918\n",
      "Epoch 1 | Batch 1931/2072 | Loss: 0.3594\n",
      "Epoch 1 | Batch 1941/2072 | Loss: 0.3972\n",
      "Epoch 1 | Batch 1951/2072 | Loss: 0.5897\n",
      "Epoch 1 | Batch 1961/2072 | Loss: 0.8591\n",
      "Epoch 1 | Batch 1971/2072 | Loss: 1.0115\n",
      "Epoch 1 | Batch 1981/2072 | Loss: 1.7110\n",
      "Epoch 1 | Batch 1991/2072 | Loss: 0.2493\n",
      "Epoch 1 | Batch 2001/2072 | Loss: 0.7246\n",
      "Epoch 1 | Batch 2011/2072 | Loss: 0.6299\n",
      "Epoch 1 | Batch 2021/2072 | Loss: 0.4656\n",
      "Epoch 1 | Batch 2031/2072 | Loss: 0.8043\n",
      "Epoch 1 | Batch 2041/2072 | Loss: 0.7526\n",
      "Epoch 1 | Batch 2051/2072 | Loss: 1.1084\n",
      "Epoch 1 | Batch 2061/2072 | Loss: 0.5939\n",
      "Epoch 1 | Batch 2071/2072 | Loss: 0.7704\n",
      "[Epoch 1] Train Acc: 0.5418, Val Acc: 0.8485, Val LogLoss: 0.4906, Loss: 4594.9639\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 2 | Batch 1/2072 | Loss: 0.5625\n",
      "Epoch 2 | Batch 11/2072 | Loss: 0.7036\n",
      "Epoch 2 | Batch 21/2072 | Loss: 0.7309\n",
      "Epoch 2 | Batch 31/2072 | Loss: 0.6143\n",
      "Epoch 2 | Batch 41/2072 | Loss: 0.6078\n",
      "Epoch 2 | Batch 51/2072 | Loss: 0.6071\n",
      "Epoch 2 | Batch 61/2072 | Loss: 0.5721\n",
      "Epoch 2 | Batch 71/2072 | Loss: 0.2337\n",
      "Epoch 2 | Batch 81/2072 | Loss: 0.6234\n",
      "Epoch 2 | Batch 91/2072 | Loss: 0.2961\n",
      "Epoch 2 | Batch 101/2072 | Loss: 0.3084\n",
      "Epoch 2 | Batch 111/2072 | Loss: 0.2372\n",
      "Epoch 2 | Batch 121/2072 | Loss: 0.5238\n",
      "Epoch 2 | Batch 131/2072 | Loss: 0.4413\n",
      "Epoch 2 | Batch 141/2072 | Loss: 0.4475\n",
      "Epoch 2 | Batch 151/2072 | Loss: 0.7352\n",
      "Epoch 2 | Batch 161/2072 | Loss: 0.3899\n",
      "Epoch 2 | Batch 171/2072 | Loss: 0.2240\n",
      "Epoch 2 | Batch 181/2072 | Loss: 0.5369\n",
      "Epoch 2 | Batch 191/2072 | Loss: 0.6314\n",
      "Epoch 2 | Batch 201/2072 | Loss: 0.5526\n",
      "Epoch 2 | Batch 211/2072 | Loss: 1.1285\n",
      "Epoch 2 | Batch 221/2072 | Loss: 0.3807\n",
      "Epoch 2 | Batch 231/2072 | Loss: 0.6027\n",
      "Epoch 2 | Batch 241/2072 | Loss: 0.3775\n",
      "Epoch 2 | Batch 251/2072 | Loss: 0.7691\n",
      "Epoch 2 | Batch 261/2072 | Loss: 0.5001\n",
      "Epoch 2 | Batch 271/2072 | Loss: 0.6720\n",
      "Epoch 2 | Batch 281/2072 | Loss: 0.4842\n",
      "Epoch 2 | Batch 291/2072 | Loss: 0.6405\n",
      "Epoch 2 | Batch 301/2072 | Loss: 0.4648\n",
      "Epoch 2 | Batch 311/2072 | Loss: 0.8449\n",
      "Epoch 2 | Batch 321/2072 | Loss: 0.5092\n",
      "Epoch 2 | Batch 331/2072 | Loss: 0.8830\n",
      "Epoch 2 | Batch 341/2072 | Loss: 0.6585\n",
      "Epoch 2 | Batch 351/2072 | Loss: 0.4237\n",
      "Epoch 2 | Batch 361/2072 | Loss: 0.4631\n",
      "Epoch 2 | Batch 371/2072 | Loss: 0.4958\n",
      "Epoch 2 | Batch 381/2072 | Loss: 0.6231\n",
      "Epoch 2 | Batch 391/2072 | Loss: 0.3357\n",
      "Epoch 2 | Batch 401/2072 | Loss: 0.8763\n",
      "Epoch 2 | Batch 411/2072 | Loss: 0.4284\n",
      "Epoch 2 | Batch 421/2072 | Loss: 1.0197\n",
      "Epoch 2 | Batch 431/2072 | Loss: 0.4839\n",
      "Epoch 2 | Batch 441/2072 | Loss: 0.5065\n",
      "Epoch 2 | Batch 451/2072 | Loss: 0.4530\n",
      "Epoch 2 | Batch 461/2072 | Loss: 0.4269\n",
      "Epoch 2 | Batch 471/2072 | Loss: 0.5201\n",
      "Epoch 2 | Batch 481/2072 | Loss: 0.7300\n",
      "Epoch 2 | Batch 491/2072 | Loss: 0.5054\n",
      "Epoch 2 | Batch 501/2072 | Loss: 0.3758\n",
      "Epoch 2 | Batch 511/2072 | Loss: 0.5680\n",
      "Epoch 2 | Batch 521/2072 | Loss: 0.4062\n",
      "Epoch 2 | Batch 531/2072 | Loss: 0.8827\n",
      "Epoch 2 | Batch 541/2072 | Loss: 0.2658\n",
      "Epoch 2 | Batch 551/2072 | Loss: 0.4326\n",
      "Epoch 2 | Batch 561/2072 | Loss: 0.6777\n",
      "Epoch 2 | Batch 571/2072 | Loss: 0.8053\n",
      "Epoch 2 | Batch 581/2072 | Loss: 0.4010\n",
      "Epoch 2 | Batch 591/2072 | Loss: 0.2379\n",
      "Epoch 2 | Batch 601/2072 | Loss: 0.3260\n",
      "Epoch 2 | Batch 611/2072 | Loss: 0.2612\n",
      "Epoch 2 | Batch 621/2072 | Loss: 0.3126\n",
      "Epoch 2 | Batch 631/2072 | Loss: 0.2485\n",
      "Epoch 2 | Batch 641/2072 | Loss: 0.8626\n",
      "Epoch 2 | Batch 651/2072 | Loss: 0.2312\n",
      "Epoch 2 | Batch 661/2072 | Loss: 0.5434\n",
      "Epoch 2 | Batch 671/2072 | Loss: 0.2129\n",
      "Epoch 2 | Batch 681/2072 | Loss: 0.4595\n",
      "Epoch 2 | Batch 691/2072 | Loss: 0.3695\n",
      "Epoch 2 | Batch 701/2072 | Loss: 0.2763\n",
      "Epoch 2 | Batch 711/2072 | Loss: 0.4978\n",
      "Epoch 2 | Batch 721/2072 | Loss: 0.8876\n",
      "Epoch 2 | Batch 731/2072 | Loss: 0.5062\n",
      "Epoch 2 | Batch 741/2072 | Loss: 0.5021\n",
      "Epoch 2 | Batch 751/2072 | Loss: 0.5430\n",
      "Epoch 2 | Batch 761/2072 | Loss: 0.3774\n",
      "Epoch 2 | Batch 771/2072 | Loss: 0.4643\n",
      "Epoch 2 | Batch 781/2072 | Loss: 0.7160\n",
      "Epoch 2 | Batch 791/2072 | Loss: 0.7256\n",
      "Epoch 2 | Batch 801/2072 | Loss: 0.3414\n",
      "Epoch 2 | Batch 811/2072 | Loss: 0.5898\n",
      "Epoch 2 | Batch 821/2072 | Loss: 0.5654\n",
      "Epoch 2 | Batch 831/2072 | Loss: 0.3631\n",
      "Epoch 2 | Batch 841/2072 | Loss: 0.3954\n",
      "Epoch 2 | Batch 851/2072 | Loss: 0.2983\n",
      "Epoch 2 | Batch 861/2072 | Loss: 0.5874\n",
      "Epoch 2 | Batch 871/2072 | Loss: 0.4265\n",
      "Epoch 2 | Batch 881/2072 | Loss: 0.6745\n",
      "Epoch 2 | Batch 891/2072 | Loss: 0.5323\n",
      "Epoch 2 | Batch 901/2072 | Loss: 0.2548\n",
      "Epoch 2 | Batch 911/2072 | Loss: 0.7209\n",
      "Epoch 2 | Batch 921/2072 | Loss: 0.3566\n",
      "Epoch 2 | Batch 931/2072 | Loss: 0.7811\n",
      "Epoch 2 | Batch 941/2072 | Loss: 0.5054\n",
      "Epoch 2 | Batch 951/2072 | Loss: 0.2971\n",
      "Epoch 2 | Batch 961/2072 | Loss: 0.3606\n",
      "Epoch 2 | Batch 971/2072 | Loss: 0.3002\n",
      "Epoch 2 | Batch 981/2072 | Loss: 0.8316\n",
      "Epoch 2 | Batch 991/2072 | Loss: 0.3350\n",
      "Epoch 2 | Batch 1001/2072 | Loss: 0.3787\n",
      "Epoch 2 | Batch 1011/2072 | Loss: 0.4128\n",
      "Epoch 2 | Batch 1021/2072 | Loss: 0.5585\n",
      "Epoch 2 | Batch 1031/2072 | Loss: 0.6867\n",
      "Epoch 2 | Batch 1041/2072 | Loss: 0.1452\n",
      "Epoch 2 | Batch 1051/2072 | Loss: 0.2845\n",
      "Epoch 2 | Batch 1061/2072 | Loss: 0.8700\n",
      "Epoch 2 | Batch 1071/2072 | Loss: 0.5729\n",
      "Epoch 2 | Batch 1081/2072 | Loss: 0.1672\n",
      "Epoch 2 | Batch 1091/2072 | Loss: 0.3896\n",
      "Epoch 2 | Batch 1101/2072 | Loss: 0.1404\n",
      "Epoch 2 | Batch 1111/2072 | Loss: 0.1968\n",
      "Epoch 2 | Batch 1121/2072 | Loss: 0.1634\n",
      "Epoch 2 | Batch 1131/2072 | Loss: 0.4293\n",
      "Epoch 2 | Batch 1141/2072 | Loss: 0.3820\n",
      "Epoch 2 | Batch 1151/2072 | Loss: 0.3224\n",
      "Epoch 2 | Batch 1161/2072 | Loss: 0.3789\n",
      "Epoch 2 | Batch 1171/2072 | Loss: 0.3876\n",
      "Epoch 2 | Batch 1181/2072 | Loss: 0.3745\n",
      "Epoch 2 | Batch 1191/2072 | Loss: 0.4587\n",
      "Epoch 2 | Batch 1201/2072 | Loss: 0.4996\n",
      "Epoch 2 | Batch 1211/2072 | Loss: 0.3807\n",
      "Epoch 2 | Batch 1221/2072 | Loss: 0.8138\n",
      "Epoch 2 | Batch 1231/2072 | Loss: 0.3699\n",
      "Epoch 2 | Batch 1241/2072 | Loss: 0.2863\n",
      "Epoch 2 | Batch 1251/2072 | Loss: 0.4269\n",
      "Epoch 2 | Batch 1261/2072 | Loss: 0.2774\n",
      "Epoch 2 | Batch 1271/2072 | Loss: 0.4445\n",
      "Epoch 2 | Batch 1281/2072 | Loss: 0.2828\n",
      "Epoch 2 | Batch 1291/2072 | Loss: 0.8472\n",
      "Epoch 2 | Batch 1301/2072 | Loss: 0.6978\n",
      "Epoch 2 | Batch 1311/2072 | Loss: 0.7839\n",
      "Epoch 2 | Batch 1321/2072 | Loss: 0.4190\n",
      "Epoch 2 | Batch 1331/2072 | Loss: 0.1724\n",
      "Epoch 2 | Batch 1341/2072 | Loss: 0.5522\n",
      "Epoch 2 | Batch 1351/2072 | Loss: 0.3904\n",
      "Epoch 2 | Batch 1361/2072 | Loss: 0.4410\n",
      "Epoch 2 | Batch 1371/2072 | Loss: 0.7054\n",
      "Epoch 2 | Batch 1381/2072 | Loss: 0.2825\n",
      "Epoch 2 | Batch 1391/2072 | Loss: 1.2045\n",
      "Epoch 2 | Batch 1401/2072 | Loss: 0.3786\n",
      "Epoch 2 | Batch 1411/2072 | Loss: 0.4781\n",
      "Epoch 2 | Batch 1421/2072 | Loss: 0.3360\n",
      "Epoch 2 | Batch 1431/2072 | Loss: 0.3448\n",
      "Epoch 2 | Batch 1441/2072 | Loss: 0.3807\n",
      "Epoch 2 | Batch 1451/2072 | Loss: 0.4358\n",
      "Epoch 2 | Batch 1461/2072 | Loss: 0.5036\n",
      "Epoch 2 | Batch 1471/2072 | Loss: 0.9121\n",
      "Epoch 2 | Batch 1481/2072 | Loss: 0.2085\n",
      "Epoch 2 | Batch 1491/2072 | Loss: 0.2503\n",
      "Epoch 2 | Batch 1501/2072 | Loss: 0.1507\n",
      "Epoch 2 | Batch 1511/2072 | Loss: 0.6646\n",
      "Epoch 2 | Batch 1521/2072 | Loss: 0.7195\n",
      "Epoch 2 | Batch 1531/2072 | Loss: 0.6897\n",
      "Epoch 2 | Batch 1541/2072 | Loss: 0.5412\n",
      "Epoch 2 | Batch 1551/2072 | Loss: 0.3259\n",
      "Epoch 2 | Batch 1561/2072 | Loss: 0.2613\n",
      "Epoch 2 | Batch 1571/2072 | Loss: 0.2219\n",
      "Epoch 2 | Batch 1581/2072 | Loss: 0.5570\n",
      "Epoch 2 | Batch 1591/2072 | Loss: 0.2348\n",
      "Epoch 2 | Batch 1601/2072 | Loss: 0.3902\n",
      "Epoch 2 | Batch 1611/2072 | Loss: 0.5993\n",
      "Epoch 2 | Batch 1621/2072 | Loss: 0.4739\n",
      "Epoch 2 | Batch 1631/2072 | Loss: 0.4091\n",
      "Epoch 2 | Batch 1641/2072 | Loss: 0.3125\n",
      "Epoch 2 | Batch 1651/2072 | Loss: 0.4710\n",
      "Epoch 2 | Batch 1661/2072 | Loss: 0.2087\n",
      "Epoch 2 | Batch 1671/2072 | Loss: 0.5279\n",
      "Epoch 2 | Batch 1681/2072 | Loss: 0.3154\n",
      "Epoch 2 | Batch 1691/2072 | Loss: 0.4146\n",
      "Epoch 2 | Batch 1701/2072 | Loss: 0.2815\n",
      "Epoch 2 | Batch 1711/2072 | Loss: 0.8986\n",
      "Epoch 2 | Batch 1721/2072 | Loss: 0.4739\n",
      "Epoch 2 | Batch 1731/2072 | Loss: 0.2152\n",
      "Epoch 2 | Batch 1741/2072 | Loss: 0.5237\n",
      "Epoch 2 | Batch 1751/2072 | Loss: 0.6629\n",
      "Epoch 2 | Batch 1761/2072 | Loss: 0.4054\n",
      "Epoch 2 | Batch 1771/2072 | Loss: 0.3257\n",
      "Epoch 2 | Batch 1781/2072 | Loss: 0.4565\n",
      "Epoch 2 | Batch 1791/2072 | Loss: 0.9224\n",
      "Epoch 2 | Batch 1801/2072 | Loss: 0.2795\n",
      "Epoch 2 | Batch 1811/2072 | Loss: 0.3192\n",
      "Epoch 2 | Batch 1821/2072 | Loss: 0.4762\n",
      "Epoch 2 | Batch 1831/2072 | Loss: 0.1547\n",
      "Epoch 2 | Batch 1841/2072 | Loss: 0.2344\n",
      "Epoch 2 | Batch 1851/2072 | Loss: 0.3395\n",
      "Epoch 2 | Batch 1861/2072 | Loss: 0.6099\n",
      "Epoch 2 | Batch 1871/2072 | Loss: 0.1483\n",
      "Epoch 2 | Batch 1881/2072 | Loss: 0.8081\n",
      "Epoch 2 | Batch 1891/2072 | Loss: 0.3411\n",
      "Epoch 2 | Batch 1901/2072 | Loss: 0.4453\n",
      "Epoch 2 | Batch 1911/2072 | Loss: 0.0702\n",
      "Epoch 2 | Batch 1921/2072 | Loss: 0.4788\n",
      "Epoch 2 | Batch 1931/2072 | Loss: 0.3021\n",
      "Epoch 2 | Batch 1941/2072 | Loss: 0.7950\n",
      "Epoch 2 | Batch 1951/2072 | Loss: 0.3237\n",
      "Epoch 2 | Batch 1961/2072 | Loss: 0.7483\n",
      "Epoch 2 | Batch 1971/2072 | Loss: 0.5612\n",
      "Epoch 2 | Batch 1981/2072 | Loss: 0.3989\n",
      "Epoch 2 | Batch 1991/2072 | Loss: 0.4564\n",
      "Epoch 2 | Batch 2001/2072 | Loss: 0.5953\n",
      "Epoch 2 | Batch 2011/2072 | Loss: 0.4259\n",
      "Epoch 2 | Batch 2021/2072 | Loss: 0.2467\n",
      "Epoch 2 | Batch 2031/2072 | Loss: 0.8068\n",
      "Epoch 2 | Batch 2041/2072 | Loss: 0.2064\n",
      "Epoch 2 | Batch 2051/2072 | Loss: 0.2458\n",
      "Epoch 2 | Batch 2061/2072 | Loss: 0.6962\n",
      "Epoch 2 | Batch 2071/2072 | Loss: 0.3599\n",
      "[Epoch 2] Train Acc: 0.8695, Val Acc: 0.9134, Val LogLoss: 0.2710, Loss: 965.3201\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 3 | Batch 1/2072 | Loss: 0.3208\n",
      "Epoch 3 | Batch 11/2072 | Loss: 0.3491\n",
      "Epoch 3 | Batch 21/2072 | Loss: 0.1827\n",
      "Epoch 3 | Batch 31/2072 | Loss: 0.6877\n",
      "Epoch 3 | Batch 41/2072 | Loss: 0.1146\n",
      "Epoch 3 | Batch 51/2072 | Loss: 0.1428\n",
      "Epoch 3 | Batch 61/2072 | Loss: 0.3938\n",
      "Epoch 3 | Batch 71/2072 | Loss: 0.2539\n",
      "Epoch 3 | Batch 81/2072 | Loss: 0.2834\n",
      "Epoch 3 | Batch 91/2072 | Loss: 0.1113\n",
      "Epoch 3 | Batch 101/2072 | Loss: 0.4485\n",
      "Epoch 3 | Batch 111/2072 | Loss: 0.2646\n",
      "Epoch 3 | Batch 121/2072 | Loss: 0.2417\n",
      "Epoch 3 | Batch 131/2072 | Loss: 0.1095\n",
      "Epoch 3 | Batch 141/2072 | Loss: 0.5317\n",
      "Epoch 3 | Batch 151/2072 | Loss: 0.1244\n",
      "Epoch 3 | Batch 161/2072 | Loss: 0.1368\n",
      "Epoch 3 | Batch 171/2072 | Loss: 0.5816\n",
      "Epoch 3 | Batch 181/2072 | Loss: 0.2696\n",
      "Epoch 3 | Batch 191/2072 | Loss: 0.1099\n",
      "Epoch 3 | Batch 201/2072 | Loss: 0.1827\n",
      "Epoch 3 | Batch 211/2072 | Loss: 0.1747\n",
      "Epoch 3 | Batch 221/2072 | Loss: 0.1636\n",
      "Epoch 3 | Batch 231/2072 | Loss: 0.2140\n",
      "Epoch 3 | Batch 241/2072 | Loss: 0.3543\n",
      "Epoch 3 | Batch 251/2072 | Loss: 0.2091\n",
      "Epoch 3 | Batch 261/2072 | Loss: 0.2579\n",
      "Epoch 3 | Batch 271/2072 | Loss: 0.2627\n",
      "Epoch 3 | Batch 281/2072 | Loss: 0.1455\n",
      "Epoch 3 | Batch 291/2072 | Loss: 0.0457\n",
      "Epoch 3 | Batch 301/2072 | Loss: 0.0959\n",
      "Epoch 3 | Batch 311/2072 | Loss: 0.1406\n",
      "Epoch 3 | Batch 321/2072 | Loss: 0.6547\n",
      "Epoch 3 | Batch 331/2072 | Loss: 0.3491\n",
      "Epoch 3 | Batch 341/2072 | Loss: 0.1910\n",
      "Epoch 3 | Batch 351/2072 | Loss: 0.3221\n",
      "Epoch 3 | Batch 361/2072 | Loss: 0.1615\n",
      "Epoch 3 | Batch 371/2072 | Loss: 0.2035\n",
      "Epoch 3 | Batch 381/2072 | Loss: 0.7338\n",
      "Epoch 3 | Batch 391/2072 | Loss: 0.2619\n",
      "Epoch 3 | Batch 401/2072 | Loss: 0.2569\n",
      "Epoch 3 | Batch 411/2072 | Loss: 0.0892\n",
      "Epoch 3 | Batch 421/2072 | Loss: 0.0863\n",
      "Epoch 3 | Batch 431/2072 | Loss: 0.2597\n",
      "Epoch 3 | Batch 441/2072 | Loss: 0.2929\n",
      "Epoch 3 | Batch 451/2072 | Loss: 0.3187\n",
      "Epoch 3 | Batch 461/2072 | Loss: 0.1584\n",
      "Epoch 3 | Batch 471/2072 | Loss: 0.5421\n",
      "Epoch 3 | Batch 481/2072 | Loss: 0.6978\n",
      "Epoch 3 | Batch 491/2072 | Loss: 0.3421\n",
      "Epoch 3 | Batch 501/2072 | Loss: 0.4492\n",
      "Epoch 3 | Batch 511/2072 | Loss: 0.0818\n",
      "Epoch 3 | Batch 521/2072 | Loss: 0.1592\n",
      "Epoch 3 | Batch 531/2072 | Loss: 0.4206\n",
      "Epoch 3 | Batch 541/2072 | Loss: 0.1250\n",
      "Epoch 3 | Batch 551/2072 | Loss: 0.4115\n",
      "Epoch 3 | Batch 561/2072 | Loss: 0.3193\n",
      "Epoch 3 | Batch 571/2072 | Loss: 0.1756\n",
      "Epoch 3 | Batch 581/2072 | Loss: 0.5649\n",
      "Epoch 3 | Batch 591/2072 | Loss: 0.1966\n",
      "Epoch 3 | Batch 601/2072 | Loss: 0.2294\n",
      "Epoch 3 | Batch 611/2072 | Loss: 0.2814\n",
      "Epoch 3 | Batch 621/2072 | Loss: 0.1322\n",
      "Epoch 3 | Batch 631/2072 | Loss: 0.3638\n",
      "Epoch 3 | Batch 641/2072 | Loss: 0.2328\n",
      "Epoch 3 | Batch 651/2072 | Loss: 0.1653\n",
      "Epoch 3 | Batch 661/2072 | Loss: 0.2023\n",
      "Epoch 3 | Batch 671/2072 | Loss: 0.2805\n",
      "Epoch 3 | Batch 681/2072 | Loss: 0.2239\n",
      "Epoch 3 | Batch 691/2072 | Loss: 0.1881\n",
      "Epoch 3 | Batch 701/2072 | Loss: 0.5708\n",
      "Epoch 3 | Batch 711/2072 | Loss: 0.1230\n",
      "Epoch 3 | Batch 721/2072 | Loss: 0.2662\n",
      "Epoch 3 | Batch 731/2072 | Loss: 0.3216\n",
      "Epoch 3 | Batch 741/2072 | Loss: 0.4782\n",
      "Epoch 3 | Batch 751/2072 | Loss: 0.5323\n",
      "Epoch 3 | Batch 761/2072 | Loss: 0.1638\n",
      "Epoch 3 | Batch 771/2072 | Loss: 0.5343\n",
      "Epoch 3 | Batch 781/2072 | Loss: 0.4572\n",
      "Epoch 3 | Batch 791/2072 | Loss: 0.4828\n",
      "Epoch 3 | Batch 801/2072 | Loss: 0.0938\n",
      "Epoch 3 | Batch 811/2072 | Loss: 0.2618\n",
      "Epoch 3 | Batch 821/2072 | Loss: 0.6743\n",
      "Epoch 3 | Batch 831/2072 | Loss: 0.2021\n",
      "Epoch 3 | Batch 841/2072 | Loss: 0.4347\n",
      "Epoch 3 | Batch 851/2072 | Loss: 0.3018\n",
      "Epoch 3 | Batch 861/2072 | Loss: 0.4005\n",
      "Epoch 3 | Batch 871/2072 | Loss: 0.1651\n",
      "Epoch 3 | Batch 881/2072 | Loss: 0.2666\n",
      "Epoch 3 | Batch 891/2072 | Loss: 0.2335\n",
      "Epoch 3 | Batch 901/2072 | Loss: 0.1653\n",
      "Epoch 3 | Batch 911/2072 | Loss: 0.2019\n",
      "Epoch 3 | Batch 921/2072 | Loss: 0.5148\n",
      "Epoch 3 | Batch 931/2072 | Loss: 0.1582\n",
      "Epoch 3 | Batch 941/2072 | Loss: 0.3331\n",
      "Epoch 3 | Batch 951/2072 | Loss: 0.1903\n",
      "Epoch 3 | Batch 961/2072 | Loss: 0.0491\n",
      "Epoch 3 | Batch 971/2072 | Loss: 0.4757\n",
      "Epoch 3 | Batch 981/2072 | Loss: 0.5399\n",
      "Epoch 3 | Batch 991/2072 | Loss: 0.1816\n",
      "Epoch 3 | Batch 1001/2072 | Loss: 0.3272\n",
      "Epoch 3 | Batch 1011/2072 | Loss: 0.3923\n",
      "Epoch 3 | Batch 1021/2072 | Loss: 0.1006\n",
      "Epoch 3 | Batch 1031/2072 | Loss: 0.1876\n",
      "Epoch 3 | Batch 1041/2072 | Loss: 0.1136\n",
      "Epoch 3 | Batch 1051/2072 | Loss: 0.3613\n",
      "Epoch 3 | Batch 1061/2072 | Loss: 0.1795\n",
      "Epoch 3 | Batch 1071/2072 | Loss: 0.1033\n",
      "Epoch 3 | Batch 1081/2072 | Loss: 0.1351\n",
      "Epoch 3 | Batch 1091/2072 | Loss: 0.1200\n",
      "Epoch 3 | Batch 1101/2072 | Loss: 0.7881\n",
      "Epoch 3 | Batch 1111/2072 | Loss: 0.2178\n",
      "Epoch 3 | Batch 1121/2072 | Loss: 0.2614\n",
      "Epoch 3 | Batch 1131/2072 | Loss: 0.2242\n",
      "Epoch 3 | Batch 1141/2072 | Loss: 0.0478\n",
      "Epoch 3 | Batch 1151/2072 | Loss: 0.3080\n",
      "Epoch 3 | Batch 1161/2072 | Loss: 0.3084\n",
      "Epoch 3 | Batch 1171/2072 | Loss: 0.2508\n",
      "Epoch 3 | Batch 1181/2072 | Loss: 0.0807\n",
      "Epoch 3 | Batch 1191/2072 | Loss: 0.1727\n",
      "Epoch 3 | Batch 1201/2072 | Loss: 0.4500\n",
      "Epoch 3 | Batch 1211/2072 | Loss: 0.2559\n",
      "Epoch 3 | Batch 1221/2072 | Loss: 0.2627\n",
      "Epoch 3 | Batch 1231/2072 | Loss: 0.1000\n",
      "Epoch 3 | Batch 1241/2072 | Loss: 0.0533\n",
      "Epoch 3 | Batch 1251/2072 | Loss: 0.2036\n",
      "Epoch 3 | Batch 1261/2072 | Loss: 0.1745\n",
      "Epoch 3 | Batch 1271/2072 | Loss: 0.1874\n",
      "Epoch 3 | Batch 1281/2072 | Loss: 0.0580\n",
      "Epoch 3 | Batch 1291/2072 | Loss: 0.2945\n",
      "Epoch 3 | Batch 1301/2072 | Loss: 0.0725\n",
      "Epoch 3 | Batch 1311/2072 | Loss: 0.8147\n",
      "Epoch 3 | Batch 1321/2072 | Loss: 0.2830\n",
      "Epoch 3 | Batch 1331/2072 | Loss: 0.3686\n",
      "Epoch 3 | Batch 1341/2072 | Loss: 0.4466\n",
      "Epoch 3 | Batch 1351/2072 | Loss: 0.0886\n",
      "Epoch 3 | Batch 1361/2072 | Loss: 0.3381\n",
      "Epoch 3 | Batch 1371/2072 | Loss: 0.2041\n",
      "Epoch 3 | Batch 1381/2072 | Loss: 0.0951\n",
      "Epoch 3 | Batch 1391/2072 | Loss: 0.4101\n",
      "Epoch 3 | Batch 1401/2072 | Loss: 0.0723\n",
      "Epoch 3 | Batch 1411/2072 | Loss: 0.2615\n",
      "Epoch 3 | Batch 1421/2072 | Loss: 0.2822\n",
      "Epoch 3 | Batch 1431/2072 | Loss: 0.0531\n",
      "Epoch 3 | Batch 1441/2072 | Loss: 0.2591\n",
      "Epoch 3 | Batch 1451/2072 | Loss: 0.1590\n",
      "Epoch 3 | Batch 1461/2072 | Loss: 0.2254\n",
      "Epoch 3 | Batch 1471/2072 | Loss: 0.5930\n",
      "Epoch 3 | Batch 1481/2072 | Loss: 0.3895\n",
      "Epoch 3 | Batch 1491/2072 | Loss: 0.1611\n",
      "Epoch 3 | Batch 1501/2072 | Loss: 0.1731\n",
      "Epoch 3 | Batch 1511/2072 | Loss: 0.2044\n",
      "Epoch 3 | Batch 1521/2072 | Loss: 0.3780\n",
      "Epoch 3 | Batch 1531/2072 | Loss: 0.4690\n",
      "Epoch 3 | Batch 1541/2072 | Loss: 0.2652\n",
      "Epoch 3 | Batch 1551/2072 | Loss: 0.0818\n",
      "Epoch 3 | Batch 1561/2072 | Loss: 0.0720\n",
      "Epoch 3 | Batch 1571/2072 | Loss: 0.4080\n",
      "Epoch 3 | Batch 1581/2072 | Loss: 0.2401\n",
      "Epoch 3 | Batch 1591/2072 | Loss: 0.7541\n",
      "Epoch 3 | Batch 1601/2072 | Loss: 0.3488\n",
      "Epoch 3 | Batch 1611/2072 | Loss: 0.4274\n",
      "Epoch 3 | Batch 1621/2072 | Loss: 0.1129\n",
      "Epoch 3 | Batch 1631/2072 | Loss: 0.1458\n",
      "Epoch 3 | Batch 1641/2072 | Loss: 0.1938\n",
      "Epoch 3 | Batch 1651/2072 | Loss: 0.0525\n",
      "Epoch 3 | Batch 1661/2072 | Loss: 0.4961\n",
      "Epoch 3 | Batch 1671/2072 | Loss: 0.3068\n",
      "Epoch 3 | Batch 1681/2072 | Loss: 0.3174\n",
      "Epoch 3 | Batch 1691/2072 | Loss: 0.3410\n",
      "Epoch 3 | Batch 1701/2072 | Loss: 0.1856\n",
      "Epoch 3 | Batch 1711/2072 | Loss: 0.1349\n",
      "Epoch 3 | Batch 1721/2072 | Loss: 0.6838\n",
      "Epoch 3 | Batch 1731/2072 | Loss: 0.3308\n",
      "Epoch 3 | Batch 1741/2072 | Loss: 0.6405\n",
      "Epoch 3 | Batch 1751/2072 | Loss: 0.4453\n",
      "Epoch 3 | Batch 1761/2072 | Loss: 0.4709\n",
      "Epoch 3 | Batch 1771/2072 | Loss: 0.7577\n",
      "Epoch 3 | Batch 1781/2072 | Loss: 0.2198\n",
      "Epoch 3 | Batch 1791/2072 | Loss: 0.0796\n",
      "Epoch 3 | Batch 1801/2072 | Loss: 0.0590\n",
      "Epoch 3 | Batch 1811/2072 | Loss: 0.1981\n",
      "Epoch 3 | Batch 1821/2072 | Loss: 0.3528\n",
      "Epoch 3 | Batch 1831/2072 | Loss: 0.1903\n",
      "Epoch 3 | Batch 1841/2072 | Loss: 0.5483\n",
      "Epoch 3 | Batch 1851/2072 | Loss: 0.4711\n",
      "Epoch 3 | Batch 1861/2072 | Loss: 0.3312\n",
      "Epoch 3 | Batch 1871/2072 | Loss: 0.2284\n",
      "Epoch 3 | Batch 1881/2072 | Loss: 0.6614\n",
      "Epoch 3 | Batch 1891/2072 | Loss: 0.4905\n",
      "Epoch 3 | Batch 1901/2072 | Loss: 0.3725\n",
      "Epoch 3 | Batch 1911/2072 | Loss: 0.5421\n",
      "Epoch 3 | Batch 1921/2072 | Loss: 0.8538\n",
      "Epoch 3 | Batch 1931/2072 | Loss: 0.2245\n",
      "Epoch 3 | Batch 1941/2072 | Loss: 0.2924\n",
      "Epoch 3 | Batch 1951/2072 | Loss: 0.5157\n",
      "Epoch 3 | Batch 1961/2072 | Loss: 0.3189\n",
      "Epoch 3 | Batch 1971/2072 | Loss: 0.6497\n",
      "Epoch 3 | Batch 1981/2072 | Loss: 0.3444\n",
      "Epoch 3 | Batch 1991/2072 | Loss: 0.5093\n",
      "Epoch 3 | Batch 2001/2072 | Loss: 0.4887\n",
      "Epoch 3 | Batch 2011/2072 | Loss: 0.3093\n",
      "Epoch 3 | Batch 2021/2072 | Loss: 0.4161\n",
      "Epoch 3 | Batch 2031/2072 | Loss: 0.2643\n",
      "Epoch 3 | Batch 2041/2072 | Loss: 0.3043\n",
      "Epoch 3 | Batch 2051/2072 | Loss: 0.2065\n",
      "Epoch 3 | Batch 2061/2072 | Loss: 0.3357\n",
      "Epoch 3 | Batch 2071/2072 | Loss: 0.0758\n",
      "[Epoch 3] Train Acc: 0.9080, Val Acc: 0.9255, Val LogLoss: 0.2199, Loss: 626.0815\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 4 | Batch 1/2072 | Loss: 0.4116\n",
      "Epoch 4 | Batch 11/2072 | Loss: 0.2447\n",
      "Epoch 4 | Batch 21/2072 | Loss: 0.4208\n",
      "Epoch 4 | Batch 31/2072 | Loss: 0.2530\n",
      "Epoch 4 | Batch 41/2072 | Loss: 0.4318\n",
      "Epoch 4 | Batch 51/2072 | Loss: 0.2068\n",
      "Epoch 4 | Batch 61/2072 | Loss: 0.3733\n",
      "Epoch 4 | Batch 71/2072 | Loss: 0.1901\n",
      "Epoch 4 | Batch 81/2072 | Loss: 0.2223\n",
      "Epoch 4 | Batch 91/2072 | Loss: 0.0296\n",
      "Epoch 4 | Batch 101/2072 | Loss: 0.0920\n",
      "Epoch 4 | Batch 111/2072 | Loss: 0.4830\n",
      "Epoch 4 | Batch 121/2072 | Loss: 0.3082\n",
      "Epoch 4 | Batch 131/2072 | Loss: 0.0381\n",
      "Epoch 4 | Batch 141/2072 | Loss: 0.0505\n",
      "Epoch 4 | Batch 151/2072 | Loss: 1.0033\n",
      "Epoch 4 | Batch 161/2072 | Loss: 0.0300\n",
      "Epoch 4 | Batch 171/2072 | Loss: 0.4186\n",
      "Epoch 4 | Batch 181/2072 | Loss: 0.1154\n",
      "Epoch 4 | Batch 191/2072 | Loss: 0.1414\n",
      "Epoch 4 | Batch 201/2072 | Loss: 0.0408\n",
      "Epoch 4 | Batch 211/2072 | Loss: 0.3093\n",
      "Epoch 4 | Batch 221/2072 | Loss: 0.2999\n",
      "Epoch 4 | Batch 231/2072 | Loss: 0.0829\n",
      "Epoch 4 | Batch 241/2072 | Loss: 0.5644\n",
      "Epoch 4 | Batch 251/2072 | Loss: 0.2662\n",
      "Epoch 4 | Batch 261/2072 | Loss: 0.2409\n",
      "Epoch 4 | Batch 271/2072 | Loss: 0.0807\n",
      "Epoch 4 | Batch 281/2072 | Loss: 0.3700\n",
      "Epoch 4 | Batch 291/2072 | Loss: 0.0483\n",
      "Epoch 4 | Batch 301/2072 | Loss: 0.0850\n",
      "Epoch 4 | Batch 311/2072 | Loss: 0.3308\n",
      "Epoch 4 | Batch 321/2072 | Loss: 0.0739\n",
      "Epoch 4 | Batch 331/2072 | Loss: 0.2972\n",
      "Epoch 4 | Batch 341/2072 | Loss: 0.1061\n",
      "Epoch 4 | Batch 351/2072 | Loss: 0.6149\n",
      "Epoch 4 | Batch 361/2072 | Loss: 0.0649\n",
      "Epoch 4 | Batch 371/2072 | Loss: 0.1345\n",
      "Epoch 4 | Batch 381/2072 | Loss: 0.2440\n",
      "Epoch 4 | Batch 391/2072 | Loss: 0.0608\n",
      "Epoch 4 | Batch 401/2072 | Loss: 0.2907\n",
      "Epoch 4 | Batch 411/2072 | Loss: 0.0641\n",
      "Epoch 4 | Batch 421/2072 | Loss: 0.1578\n",
      "Epoch 4 | Batch 431/2072 | Loss: 0.2655\n",
      "Epoch 4 | Batch 441/2072 | Loss: 0.0910\n",
      "Epoch 4 | Batch 451/2072 | Loss: 0.1309\n",
      "Epoch 4 | Batch 461/2072 | Loss: 0.0246\n",
      "Epoch 4 | Batch 471/2072 | Loss: 0.0804\n",
      "Epoch 4 | Batch 481/2072 | Loss: 0.0155\n",
      "Epoch 4 | Batch 491/2072 | Loss: 0.1248\n",
      "Epoch 4 | Batch 501/2072 | Loss: 0.1162\n",
      "Epoch 4 | Batch 511/2072 | Loss: 0.3052\n",
      "Epoch 4 | Batch 521/2072 | Loss: 0.2511\n",
      "Epoch 4 | Batch 531/2072 | Loss: 0.0336\n",
      "Epoch 4 | Batch 541/2072 | Loss: 0.2616\n",
      "Epoch 4 | Batch 551/2072 | Loss: 0.1160\n",
      "Epoch 4 | Batch 561/2072 | Loss: 0.2340\n",
      "Epoch 4 | Batch 571/2072 | Loss: 0.3407\n",
      "Epoch 4 | Batch 581/2072 | Loss: 0.3956\n",
      "Epoch 4 | Batch 591/2072 | Loss: 0.1012\n",
      "Epoch 4 | Batch 601/2072 | Loss: 0.2488\n",
      "Epoch 4 | Batch 611/2072 | Loss: 0.1231\n",
      "Epoch 4 | Batch 621/2072 | Loss: 0.3963\n",
      "Epoch 4 | Batch 631/2072 | Loss: 0.2409\n",
      "Epoch 4 | Batch 641/2072 | Loss: 0.2897\n",
      "Epoch 4 | Batch 651/2072 | Loss: 0.6169\n",
      "Epoch 4 | Batch 661/2072 | Loss: 0.0868\n",
      "Epoch 4 | Batch 671/2072 | Loss: 0.1668\n",
      "Epoch 4 | Batch 681/2072 | Loss: 0.1204\n",
      "Epoch 4 | Batch 691/2072 | Loss: 0.1554\n",
      "Epoch 4 | Batch 701/2072 | Loss: 0.3080\n",
      "Epoch 4 | Batch 711/2072 | Loss: 0.1009\n",
      "Epoch 4 | Batch 721/2072 | Loss: 0.1738\n",
      "Epoch 4 | Batch 731/2072 | Loss: 0.1942\n",
      "Epoch 4 | Batch 741/2072 | Loss: 0.1311\n",
      "Epoch 4 | Batch 751/2072 | Loss: 0.1209\n",
      "Epoch 4 | Batch 761/2072 | Loss: 0.2136\n",
      "Epoch 4 | Batch 771/2072 | Loss: 0.1123\n",
      "Epoch 4 | Batch 781/2072 | Loss: 0.6997\n",
      "Epoch 4 | Batch 791/2072 | Loss: 0.1183\n",
      "Epoch 4 | Batch 801/2072 | Loss: 0.5536\n",
      "Epoch 4 | Batch 811/2072 | Loss: 1.0293\n",
      "Epoch 4 | Batch 821/2072 | Loss: 0.2636\n",
      "Epoch 4 | Batch 831/2072 | Loss: 0.1654\n",
      "Epoch 4 | Batch 841/2072 | Loss: 0.5934\n",
      "Epoch 4 | Batch 851/2072 | Loss: 0.5515\n",
      "Epoch 4 | Batch 861/2072 | Loss: 0.0370\n",
      "Epoch 4 | Batch 871/2072 | Loss: 0.4583\n",
      "Epoch 4 | Batch 881/2072 | Loss: 0.1243\n",
      "Epoch 4 | Batch 891/2072 | Loss: 0.2099\n",
      "Epoch 4 | Batch 901/2072 | Loss: 0.5132\n",
      "Epoch 4 | Batch 911/2072 | Loss: 0.2853\n",
      "Epoch 4 | Batch 921/2072 | Loss: 0.4279\n",
      "Epoch 4 | Batch 931/2072 | Loss: 0.2197\n",
      "Epoch 4 | Batch 941/2072 | Loss: 0.7045\n",
      "Epoch 4 | Batch 951/2072 | Loss: 0.2540\n",
      "Epoch 4 | Batch 961/2072 | Loss: 0.4380\n",
      "Epoch 4 | Batch 971/2072 | Loss: 0.3046\n",
      "Epoch 4 | Batch 981/2072 | Loss: 0.4080\n",
      "Epoch 4 | Batch 991/2072 | Loss: 0.1678\n",
      "Epoch 4 | Batch 1001/2072 | Loss: 0.2031\n",
      "Epoch 4 | Batch 1011/2072 | Loss: 0.1901\n",
      "Epoch 4 | Batch 1021/2072 | Loss: 0.3515\n",
      "Epoch 4 | Batch 1031/2072 | Loss: 0.5307\n",
      "Epoch 4 | Batch 1041/2072 | Loss: 0.1294\n",
      "Epoch 4 | Batch 1051/2072 | Loss: 0.0910\n",
      "Epoch 4 | Batch 1061/2072 | Loss: 0.3898\n",
      "Epoch 4 | Batch 1071/2072 | Loss: 0.3143\n",
      "Epoch 4 | Batch 1081/2072 | Loss: 0.4882\n",
      "Epoch 4 | Batch 1091/2072 | Loss: 0.0288\n",
      "Epoch 4 | Batch 1101/2072 | Loss: 0.0408\n",
      "Epoch 4 | Batch 1111/2072 | Loss: 0.1178\n",
      "Epoch 4 | Batch 1121/2072 | Loss: 0.2629\n",
      "Epoch 4 | Batch 1131/2072 | Loss: 0.2710\n",
      "Epoch 4 | Batch 1141/2072 | Loss: 0.2120\n",
      "Epoch 4 | Batch 1151/2072 | Loss: 0.2918\n",
      "Epoch 4 | Batch 1161/2072 | Loss: 0.2991\n",
      "Epoch 4 | Batch 1171/2072 | Loss: 0.5073\n",
      "Epoch 4 | Batch 1181/2072 | Loss: 0.2132\n",
      "Epoch 4 | Batch 1191/2072 | Loss: 0.2431\n",
      "Epoch 4 | Batch 1201/2072 | Loss: 0.1707\n",
      "Epoch 4 | Batch 1211/2072 | Loss: 0.1334\n",
      "Epoch 4 | Batch 1221/2072 | Loss: 0.3557\n",
      "Epoch 4 | Batch 1231/2072 | Loss: 0.5460\n",
      "Epoch 4 | Batch 1241/2072 | Loss: 0.3851\n",
      "Epoch 4 | Batch 1251/2072 | Loss: 0.0709\n",
      "Epoch 4 | Batch 1261/2072 | Loss: 0.0540\n",
      "Epoch 4 | Batch 1271/2072 | Loss: 0.2929\n",
      "Epoch 4 | Batch 1281/2072 | Loss: 0.0463\n",
      "Epoch 4 | Batch 1291/2072 | Loss: 0.2726\n",
      "Epoch 4 | Batch 1301/2072 | Loss: 0.0842\n",
      "Epoch 4 | Batch 1311/2072 | Loss: 0.0564\n",
      "Epoch 4 | Batch 1321/2072 | Loss: 0.0554\n",
      "Epoch 4 | Batch 1331/2072 | Loss: 0.5231\n",
      "Epoch 4 | Batch 1341/2072 | Loss: 0.1157\n",
      "Epoch 4 | Batch 1351/2072 | Loss: 0.1162\n",
      "Epoch 4 | Batch 1361/2072 | Loss: 0.1199\n",
      "Epoch 4 | Batch 1371/2072 | Loss: 0.0348\n",
      "Epoch 4 | Batch 1381/2072 | Loss: 0.1531\n",
      "Epoch 4 | Batch 1391/2072 | Loss: 0.6177\n",
      "Epoch 4 | Batch 1401/2072 | Loss: 0.2513\n",
      "Epoch 4 | Batch 1411/2072 | Loss: 0.3578\n",
      "Epoch 4 | Batch 1421/2072 | Loss: 0.1474\n",
      "Epoch 4 | Batch 1431/2072 | Loss: 0.3858\n",
      "Epoch 4 | Batch 1441/2072 | Loss: 0.1886\n",
      "Epoch 4 | Batch 1451/2072 | Loss: 0.1152\n",
      "Epoch 4 | Batch 1461/2072 | Loss: 0.1678\n",
      "Epoch 4 | Batch 1471/2072 | Loss: 0.1669\n",
      "Epoch 4 | Batch 1481/2072 | Loss: 0.0296\n",
      "Epoch 4 | Batch 1491/2072 | Loss: 0.1123\n",
      "Epoch 4 | Batch 1501/2072 | Loss: 0.1262\n",
      "Epoch 4 | Batch 1511/2072 | Loss: 0.2674\n",
      "Epoch 4 | Batch 1521/2072 | Loss: 0.2771\n",
      "Epoch 4 | Batch 1531/2072 | Loss: 0.0863\n",
      "Epoch 4 | Batch 1541/2072 | Loss: 0.2694\n",
      "Epoch 4 | Batch 1551/2072 | Loss: 0.1821\n",
      "Epoch 4 | Batch 1561/2072 | Loss: 0.1456\n",
      "Epoch 4 | Batch 1571/2072 | Loss: 0.1091\n",
      "Epoch 4 | Batch 1581/2072 | Loss: 0.1392\n",
      "Epoch 4 | Batch 1591/2072 | Loss: 0.1115\n",
      "Epoch 4 | Batch 1601/2072 | Loss: 0.0228\n",
      "Epoch 4 | Batch 1611/2072 | Loss: 0.3025\n",
      "Epoch 4 | Batch 1621/2072 | Loss: 0.0514\n",
      "Epoch 4 | Batch 1631/2072 | Loss: 0.2427\n",
      "Epoch 4 | Batch 1641/2072 | Loss: 0.1818\n",
      "Epoch 4 | Batch 1651/2072 | Loss: 0.2020\n",
      "Epoch 4 | Batch 1661/2072 | Loss: 0.1952\n",
      "Epoch 4 | Batch 1671/2072 | Loss: 0.1935\n",
      "Epoch 4 | Batch 1681/2072 | Loss: 0.2146\n",
      "Epoch 4 | Batch 1691/2072 | Loss: 0.2012\n",
      "Epoch 4 | Batch 1701/2072 | Loss: 0.3079\n",
      "Epoch 4 | Batch 1711/2072 | Loss: 0.0514\n",
      "Epoch 4 | Batch 1721/2072 | Loss: 0.3871\n",
      "Epoch 4 | Batch 1731/2072 | Loss: 0.6019\n",
      "Epoch 4 | Batch 1741/2072 | Loss: 0.3200\n",
      "Epoch 4 | Batch 1751/2072 | Loss: 0.2390\n",
      "Epoch 4 | Batch 1761/2072 | Loss: 0.7713\n",
      "Epoch 4 | Batch 1771/2072 | Loss: 0.1921\n",
      "Epoch 4 | Batch 1781/2072 | Loss: 0.0896\n",
      "Epoch 4 | Batch 1791/2072 | Loss: 0.2877\n",
      "Epoch 4 | Batch 1801/2072 | Loss: 0.0805\n",
      "Epoch 4 | Batch 1811/2072 | Loss: 0.1332\n",
      "Epoch 4 | Batch 1821/2072 | Loss: 0.0487\n",
      "Epoch 4 | Batch 1831/2072 | Loss: 0.2760\n",
      "Epoch 4 | Batch 1841/2072 | Loss: 0.1680\n",
      "Epoch 4 | Batch 1851/2072 | Loss: 0.2080\n",
      "Epoch 4 | Batch 1861/2072 | Loss: 0.3665\n",
      "Epoch 4 | Batch 1871/2072 | Loss: 0.0801\n",
      "Epoch 4 | Batch 1881/2072 | Loss: 0.3300\n",
      "Epoch 4 | Batch 1891/2072 | Loss: 0.2001\n",
      "Epoch 4 | Batch 1901/2072 | Loss: 0.1443\n",
      "Epoch 4 | Batch 1911/2072 | Loss: 0.0795\n",
      "Epoch 4 | Batch 1921/2072 | Loss: 0.2039\n",
      "Epoch 4 | Batch 1931/2072 | Loss: 0.1616\n",
      "Epoch 4 | Batch 1941/2072 | Loss: 0.1487\n",
      "Epoch 4 | Batch 1951/2072 | Loss: 0.0855\n",
      "Epoch 4 | Batch 1961/2072 | Loss: 0.0900\n",
      "Epoch 4 | Batch 1971/2072 | Loss: 0.1426\n",
      "Epoch 4 | Batch 1981/2072 | Loss: 0.0135\n",
      "Epoch 4 | Batch 1991/2072 | Loss: 0.0676\n",
      "Epoch 4 | Batch 2001/2072 | Loss: 0.0371\n",
      "Epoch 4 | Batch 2011/2072 | Loss: 0.5571\n",
      "Epoch 4 | Batch 2021/2072 | Loss: 0.2834\n",
      "Epoch 4 | Batch 2031/2072 | Loss: 0.0786\n",
      "Epoch 4 | Batch 2041/2072 | Loss: 0.4441\n",
      "Epoch 4 | Batch 2051/2072 | Loss: 0.1504\n",
      "Epoch 4 | Batch 2061/2072 | Loss: 0.9247\n",
      "Epoch 4 | Batch 2071/2072 | Loss: 0.0915\n",
      "[Epoch 4] Train Acc: 0.9259, Val Acc: 0.9433, Val LogLoss: 0.1696, Loss: 494.4419\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 5 | Batch 1/2072 | Loss: 0.0945\n",
      "Epoch 5 | Batch 11/2072 | Loss: 0.1498\n",
      "Epoch 5 | Batch 21/2072 | Loss: 0.5019\n",
      "Epoch 5 | Batch 31/2072 | Loss: 0.1944\n",
      "Epoch 5 | Batch 41/2072 | Loss: 0.1839\n",
      "Epoch 5 | Batch 51/2072 | Loss: 0.1003\n",
      "Epoch 5 | Batch 61/2072 | Loss: 0.0860\n",
      "Epoch 5 | Batch 71/2072 | Loss: 0.0506\n",
      "Epoch 5 | Batch 81/2072 | Loss: 0.0577\n",
      "Epoch 5 | Batch 91/2072 | Loss: 0.2456\n",
      "Epoch 5 | Batch 101/2072 | Loss: 0.0121\n",
      "Epoch 5 | Batch 111/2072 | Loss: 0.0284\n",
      "Epoch 5 | Batch 121/2072 | Loss: 0.0133\n",
      "Epoch 5 | Batch 131/2072 | Loss: 0.0870\n",
      "Epoch 5 | Batch 141/2072 | Loss: 0.0927\n",
      "Epoch 5 | Batch 151/2072 | Loss: 0.1549\n",
      "Epoch 5 | Batch 161/2072 | Loss: 0.1667\n",
      "Epoch 5 | Batch 171/2072 | Loss: 0.1508\n",
      "Epoch 5 | Batch 181/2072 | Loss: 0.3556\n",
      "Epoch 5 | Batch 191/2072 | Loss: 0.0558\n",
      "Epoch 5 | Batch 201/2072 | Loss: 0.0232\n",
      "Epoch 5 | Batch 211/2072 | Loss: 0.3144\n",
      "Epoch 5 | Batch 221/2072 | Loss: 0.0196\n",
      "Epoch 5 | Batch 231/2072 | Loss: 0.2729\n",
      "Epoch 5 | Batch 241/2072 | Loss: 0.1722\n",
      "Epoch 5 | Batch 251/2072 | Loss: 0.0682\n",
      "Epoch 5 | Batch 261/2072 | Loss: 0.0781\n",
      "Epoch 5 | Batch 271/2072 | Loss: 0.2804\n",
      "Epoch 5 | Batch 281/2072 | Loss: 0.0586\n",
      "Epoch 5 | Batch 291/2072 | Loss: 0.0132\n",
      "Epoch 5 | Batch 301/2072 | Loss: 0.0948\n",
      "Epoch 5 | Batch 311/2072 | Loss: 0.1356\n",
      "Epoch 5 | Batch 321/2072 | Loss: 0.1871\n",
      "Epoch 5 | Batch 331/2072 | Loss: 0.0478\n",
      "Epoch 5 | Batch 341/2072 | Loss: 0.0309\n",
      "Epoch 5 | Batch 351/2072 | Loss: 0.2232\n",
      "Epoch 5 | Batch 361/2072 | Loss: 0.1136\n",
      "Epoch 5 | Batch 371/2072 | Loss: 0.1746\n",
      "Epoch 5 | Batch 381/2072 | Loss: 0.0072\n",
      "Epoch 5 | Batch 391/2072 | Loss: 0.2802\n",
      "Epoch 5 | Batch 401/2072 | Loss: 0.0930\n",
      "Epoch 5 | Batch 411/2072 | Loss: 0.0057\n",
      "Epoch 5 | Batch 421/2072 | Loss: 0.0227\n",
      "Epoch 5 | Batch 431/2072 | Loss: 0.3021\n",
      "Epoch 5 | Batch 441/2072 | Loss: 0.1772\n",
      "Epoch 5 | Batch 451/2072 | Loss: 0.0619\n",
      "Epoch 5 | Batch 461/2072 | Loss: 0.2753\n",
      "Epoch 5 | Batch 471/2072 | Loss: 0.0164\n",
      "Epoch 5 | Batch 481/2072 | Loss: 0.1063\n",
      "Epoch 5 | Batch 491/2072 | Loss: 0.2761\n",
      "Epoch 5 | Batch 501/2072 | Loss: 0.2914\n",
      "Epoch 5 | Batch 511/2072 | Loss: 0.1908\n",
      "Epoch 5 | Batch 521/2072 | Loss: 0.0643\n",
      "Epoch 5 | Batch 531/2072 | Loss: 0.3179\n",
      "Epoch 5 | Batch 541/2072 | Loss: 0.1489\n",
      "Epoch 5 | Batch 551/2072 | Loss: 0.2611\n",
      "Epoch 5 | Batch 561/2072 | Loss: 0.2004\n",
      "Epoch 5 | Batch 571/2072 | Loss: 0.1559\n",
      "Epoch 5 | Batch 581/2072 | Loss: 0.2505\n",
      "Epoch 5 | Batch 591/2072 | Loss: 0.6098\n",
      "Epoch 5 | Batch 601/2072 | Loss: 0.0668\n",
      "Epoch 5 | Batch 611/2072 | Loss: 0.1070\n",
      "Epoch 5 | Batch 621/2072 | Loss: 0.0165\n",
      "Epoch 5 | Batch 631/2072 | Loss: 0.0827\n",
      "Epoch 5 | Batch 641/2072 | Loss: 0.1731\n",
      "Epoch 5 | Batch 651/2072 | Loss: 0.0167\n",
      "Epoch 5 | Batch 661/2072 | Loss: 0.2032\n",
      "Epoch 5 | Batch 671/2072 | Loss: 0.2853\n",
      "Epoch 5 | Batch 681/2072 | Loss: 0.2076\n",
      "Epoch 5 | Batch 691/2072 | Loss: 0.0979\n",
      "Epoch 5 | Batch 701/2072 | Loss: 0.0137\n",
      "Epoch 5 | Batch 711/2072 | Loss: 0.2112\n",
      "Epoch 5 | Batch 721/2072 | Loss: 0.0953\n",
      "Epoch 5 | Batch 731/2072 | Loss: 0.1274\n",
      "Epoch 5 | Batch 741/2072 | Loss: 0.0816\n",
      "Epoch 5 | Batch 751/2072 | Loss: 0.1807\n",
      "Epoch 5 | Batch 761/2072 | Loss: 0.1027\n",
      "Epoch 5 | Batch 771/2072 | Loss: 0.0401\n",
      "Epoch 5 | Batch 781/2072 | Loss: 0.0308\n",
      "Epoch 5 | Batch 791/2072 | Loss: 0.1090\n",
      "Epoch 5 | Batch 801/2072 | Loss: 0.3951\n",
      "Epoch 5 | Batch 811/2072 | Loss: 0.0118\n",
      "Epoch 5 | Batch 821/2072 | Loss: 0.2480\n",
      "Epoch 5 | Batch 831/2072 | Loss: 0.1464\n",
      "Epoch 5 | Batch 841/2072 | Loss: 0.1066\n",
      "Epoch 5 | Batch 851/2072 | Loss: 0.4378\n",
      "Epoch 5 | Batch 861/2072 | Loss: 0.0843\n",
      "Epoch 5 | Batch 871/2072 | Loss: 0.2151\n",
      "Epoch 5 | Batch 881/2072 | Loss: 0.1746\n",
      "Epoch 5 | Batch 891/2072 | Loss: 0.4052\n",
      "Epoch 5 | Batch 901/2072 | Loss: 0.1714\n",
      "Epoch 5 | Batch 911/2072 | Loss: 0.2795\n",
      "Epoch 5 | Batch 921/2072 | Loss: 0.1278\n",
      "Epoch 5 | Batch 931/2072 | Loss: 0.0561\n",
      "Epoch 5 | Batch 941/2072 | Loss: 0.0440\n",
      "Epoch 5 | Batch 951/2072 | Loss: 0.1247\n",
      "Epoch 5 | Batch 961/2072 | Loss: 0.0483\n",
      "Epoch 5 | Batch 971/2072 | Loss: 0.0339\n",
      "Epoch 5 | Batch 981/2072 | Loss: 0.1063\n",
      "Epoch 5 | Batch 991/2072 | Loss: 0.4767\n",
      "Epoch 5 | Batch 1001/2072 | Loss: 0.5077\n",
      "Epoch 5 | Batch 1011/2072 | Loss: 0.2784\n",
      "Epoch 5 | Batch 1021/2072 | Loss: 0.2091\n",
      "Epoch 5 | Batch 1031/2072 | Loss: 0.0521\n",
      "Epoch 5 | Batch 1041/2072 | Loss: 0.6562\n",
      "Epoch 5 | Batch 1051/2072 | Loss: 0.0633\n",
      "Epoch 5 | Batch 1061/2072 | Loss: 0.0452\n",
      "Epoch 5 | Batch 1071/2072 | Loss: 0.3957\n",
      "Epoch 5 | Batch 1081/2072 | Loss: 0.0443\n",
      "Epoch 5 | Batch 1091/2072 | Loss: 0.2086\n",
      "Epoch 5 | Batch 1101/2072 | Loss: 0.1695\n",
      "Epoch 5 | Batch 1111/2072 | Loss: 0.1950\n",
      "Epoch 5 | Batch 1121/2072 | Loss: 0.1263\n",
      "Epoch 5 | Batch 1131/2072 | Loss: 0.2151\n",
      "Epoch 5 | Batch 1141/2072 | Loss: 0.1392\n",
      "Epoch 5 | Batch 1151/2072 | Loss: 0.0825\n",
      "Epoch 5 | Batch 1161/2072 | Loss: 0.6286\n",
      "Epoch 5 | Batch 1171/2072 | Loss: 0.1726\n",
      "Epoch 5 | Batch 1181/2072 | Loss: 0.0835\n",
      "Epoch 5 | Batch 1191/2072 | Loss: 0.4879\n",
      "Epoch 5 | Batch 1201/2072 | Loss: 0.0948\n",
      "Epoch 5 | Batch 1211/2072 | Loss: 0.0878\n",
      "Epoch 5 | Batch 1221/2072 | Loss: 0.0584\n",
      "Epoch 5 | Batch 1231/2072 | Loss: 0.3204\n",
      "Epoch 5 | Batch 1241/2072 | Loss: 0.0872\n",
      "Epoch 5 | Batch 1251/2072 | Loss: 0.1796\n",
      "Epoch 5 | Batch 1261/2072 | Loss: 0.0952\n",
      "Epoch 5 | Batch 1271/2072 | Loss: 0.3957\n",
      "Epoch 5 | Batch 1281/2072 | Loss: 0.3649\n",
      "Epoch 5 | Batch 1291/2072 | Loss: 0.3562\n",
      "Epoch 5 | Batch 1301/2072 | Loss: 0.1204\n",
      "Epoch 5 | Batch 1311/2072 | Loss: 0.0860\n",
      "Epoch 5 | Batch 1321/2072 | Loss: 0.2783\n",
      "Epoch 5 | Batch 1331/2072 | Loss: 0.2270\n",
      "Epoch 5 | Batch 1341/2072 | Loss: 0.3530\n",
      "Epoch 5 | Batch 1351/2072 | Loss: 0.2690\n",
      "Epoch 5 | Batch 1361/2072 | Loss: 0.2289\n",
      "Epoch 5 | Batch 1371/2072 | Loss: 0.3123\n",
      "Epoch 5 | Batch 1381/2072 | Loss: 0.1769\n",
      "Epoch 5 | Batch 1391/2072 | Loss: 0.1076\n",
      "Epoch 5 | Batch 1401/2072 | Loss: 0.4036\n",
      "Epoch 5 | Batch 1411/2072 | Loss: 0.5124\n",
      "Epoch 5 | Batch 1421/2072 | Loss: 0.7213\n",
      "Epoch 5 | Batch 1431/2072 | Loss: 0.0645\n",
      "Epoch 5 | Batch 1441/2072 | Loss: 0.5502\n",
      "Epoch 5 | Batch 1451/2072 | Loss: 0.1414\n",
      "Epoch 5 | Batch 1461/2072 | Loss: 0.0859\n",
      "Epoch 5 | Batch 1471/2072 | Loss: 0.4221\n",
      "Epoch 5 | Batch 1481/2072 | Loss: 0.2421\n",
      "Epoch 5 | Batch 1491/2072 | Loss: 0.2401\n",
      "Epoch 5 | Batch 1501/2072 | Loss: 0.0414\n",
      "Epoch 5 | Batch 1511/2072 | Loss: 0.3915\n",
      "Epoch 5 | Batch 1521/2072 | Loss: 0.0292\n",
      "Epoch 5 | Batch 1531/2072 | Loss: 0.1127\n",
      "Epoch 5 | Batch 1541/2072 | Loss: 0.1514\n",
      "Epoch 5 | Batch 1551/2072 | Loss: 0.1794\n",
      "Epoch 5 | Batch 1561/2072 | Loss: 0.2174\n",
      "Epoch 5 | Batch 1571/2072 | Loss: 0.3963\n",
      "Epoch 5 | Batch 1581/2072 | Loss: 0.0809\n",
      "Epoch 5 | Batch 1591/2072 | Loss: 0.0092\n",
      "Epoch 5 | Batch 1601/2072 | Loss: 0.5413\n",
      "Epoch 5 | Batch 1611/2072 | Loss: 0.1328\n",
      "Epoch 5 | Batch 1621/2072 | Loss: 0.0232\n",
      "Epoch 5 | Batch 1631/2072 | Loss: 0.1807\n",
      "Epoch 5 | Batch 1641/2072 | Loss: 0.2654\n",
      "Epoch 5 | Batch 1651/2072 | Loss: 0.3803\n",
      "Epoch 5 | Batch 1661/2072 | Loss: 0.0818\n",
      "Epoch 5 | Batch 1671/2072 | Loss: 0.0847\n",
      "Epoch 5 | Batch 1681/2072 | Loss: 0.5712\n",
      "Epoch 5 | Batch 1691/2072 | Loss: 0.1630\n",
      "Epoch 5 | Batch 1701/2072 | Loss: 0.5180\n",
      "Epoch 5 | Batch 1711/2072 | Loss: 0.1580\n",
      "Epoch 5 | Batch 1721/2072 | Loss: 0.3206\n",
      "Epoch 5 | Batch 1731/2072 | Loss: 0.3381\n",
      "Epoch 5 | Batch 1741/2072 | Loss: 0.0774\n",
      "Epoch 5 | Batch 1751/2072 | Loss: 0.2242\n",
      "Epoch 5 | Batch 1761/2072 | Loss: 0.3293\n",
      "Epoch 5 | Batch 1771/2072 | Loss: 0.4511\n",
      "Epoch 5 | Batch 1781/2072 | Loss: 0.0878\n",
      "Epoch 5 | Batch 1791/2072 | Loss: 0.1941\n",
      "Epoch 5 | Batch 1801/2072 | Loss: 0.4237\n",
      "Epoch 5 | Batch 1811/2072 | Loss: 0.0312\n",
      "Epoch 5 | Batch 1821/2072 | Loss: 0.1087\n",
      "Epoch 5 | Batch 1831/2072 | Loss: 0.3706\n",
      "Epoch 5 | Batch 1841/2072 | Loss: 0.1687\n",
      "Epoch 5 | Batch 1851/2072 | Loss: 0.0393\n",
      "Epoch 5 | Batch 1861/2072 | Loss: 0.3102\n",
      "Epoch 5 | Batch 1871/2072 | Loss: 0.4206\n",
      "Epoch 5 | Batch 1881/2072 | Loss: 0.6037\n",
      "Epoch 5 | Batch 1891/2072 | Loss: 0.1725\n",
      "Epoch 5 | Batch 1901/2072 | Loss: 0.1301\n",
      "Epoch 5 | Batch 1911/2072 | Loss: 0.2087\n",
      "Epoch 5 | Batch 1921/2072 | Loss: 0.0512\n",
      "Epoch 5 | Batch 1931/2072 | Loss: 0.3395\n",
      "Epoch 5 | Batch 1941/2072 | Loss: 0.1460\n",
      "Epoch 5 | Batch 1951/2072 | Loss: 0.1309\n",
      "Epoch 5 | Batch 1961/2072 | Loss: 0.2640\n",
      "Epoch 5 | Batch 1971/2072 | Loss: 0.0874\n",
      "Epoch 5 | Batch 1981/2072 | Loss: 0.9054\n",
      "Epoch 5 | Batch 1991/2072 | Loss: 0.4018\n",
      "Epoch 5 | Batch 2001/2072 | Loss: 0.2321\n",
      "Epoch 5 | Batch 2011/2072 | Loss: 0.0936\n",
      "Epoch 5 | Batch 2021/2072 | Loss: 0.1020\n",
      "Epoch 5 | Batch 2031/2072 | Loss: 0.1685\n",
      "Epoch 5 | Batch 2041/2072 | Loss: 0.2057\n",
      "Epoch 5 | Batch 2051/2072 | Loss: 0.7314\n",
      "Epoch 5 | Batch 2061/2072 | Loss: 0.0518\n",
      "Epoch 5 | Batch 2071/2072 | Loss: 0.1023\n",
      "[Epoch 5] Train Acc: 0.9389, Val Acc: 0.9553, Val LogLoss: 0.1200, Loss: 396.0327\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 6 | Batch 1/2072 | Loss: 0.0548\n",
      "Epoch 6 | Batch 11/2072 | Loss: 0.1455\n",
      "Epoch 6 | Batch 21/2072 | Loss: 0.1442\n",
      "Epoch 6 | Batch 31/2072 | Loss: 0.1023\n",
      "Epoch 6 | Batch 41/2072 | Loss: 0.0936\n",
      "Epoch 6 | Batch 51/2072 | Loss: 0.0522\n",
      "Epoch 6 | Batch 61/2072 | Loss: 0.1150\n",
      "Epoch 6 | Batch 71/2072 | Loss: 0.2706\n",
      "Epoch 6 | Batch 81/2072 | Loss: 0.0114\n",
      "Epoch 6 | Batch 91/2072 | Loss: 0.4719\n",
      "Epoch 6 | Batch 101/2072 | Loss: 0.0392\n",
      "Epoch 6 | Batch 111/2072 | Loss: 0.2035\n",
      "Epoch 6 | Batch 121/2072 | Loss: 0.1361\n",
      "Epoch 6 | Batch 131/2072 | Loss: 0.2007\n",
      "Epoch 6 | Batch 141/2072 | Loss: 0.1177\n",
      "Epoch 6 | Batch 151/2072 | Loss: 0.1418\n",
      "Epoch 6 | Batch 161/2072 | Loss: 0.0717\n",
      "Epoch 6 | Batch 171/2072 | Loss: 0.1194\n",
      "Epoch 6 | Batch 181/2072 | Loss: 0.0178\n",
      "Epoch 6 | Batch 191/2072 | Loss: 0.0150\n",
      "Epoch 6 | Batch 201/2072 | Loss: 0.0746\n",
      "Epoch 6 | Batch 211/2072 | Loss: 0.1918\n",
      "Epoch 6 | Batch 221/2072 | Loss: 0.1216\n",
      "Epoch 6 | Batch 231/2072 | Loss: 0.1627\n",
      "Epoch 6 | Batch 241/2072 | Loss: 0.0547\n",
      "Epoch 6 | Batch 251/2072 | Loss: 0.2169\n",
      "Epoch 6 | Batch 261/2072 | Loss: 0.1078\n",
      "Epoch 6 | Batch 271/2072 | Loss: 0.2553\n",
      "Epoch 6 | Batch 281/2072 | Loss: 0.1760\n",
      "Epoch 6 | Batch 291/2072 | Loss: 0.0189\n",
      "Epoch 6 | Batch 301/2072 | Loss: 0.5350\n",
      "Epoch 6 | Batch 311/2072 | Loss: 0.0199\n",
      "Epoch 6 | Batch 321/2072 | Loss: 0.0171\n",
      "Epoch 6 | Batch 331/2072 | Loss: 0.1650\n",
      "Epoch 6 | Batch 341/2072 | Loss: 0.0225\n",
      "Epoch 6 | Batch 351/2072 | Loss: 0.0515\n",
      "Epoch 6 | Batch 361/2072 | Loss: 0.3344\n",
      "Epoch 6 | Batch 371/2072 | Loss: 0.0062\n",
      "Epoch 6 | Batch 381/2072 | Loss: 0.0186\n",
      "Epoch 6 | Batch 391/2072 | Loss: 0.1589\n",
      "Epoch 6 | Batch 401/2072 | Loss: 0.0871\n",
      "Epoch 6 | Batch 411/2072 | Loss: 0.3690\n",
      "Epoch 6 | Batch 421/2072 | Loss: 0.0547\n",
      "Epoch 6 | Batch 431/2072 | Loss: 0.2326\n",
      "Epoch 6 | Batch 441/2072 | Loss: 0.1800\n",
      "Epoch 6 | Batch 451/2072 | Loss: 0.1493\n",
      "Epoch 6 | Batch 461/2072 | Loss: 0.2372\n",
      "Epoch 6 | Batch 471/2072 | Loss: 0.0115\n",
      "Epoch 6 | Batch 481/2072 | Loss: 0.0814\n",
      "Epoch 6 | Batch 491/2072 | Loss: 0.0428\n",
      "Epoch 6 | Batch 501/2072 | Loss: 0.0382\n",
      "Epoch 6 | Batch 511/2072 | Loss: 0.1021\n",
      "Epoch 6 | Batch 521/2072 | Loss: 0.4835\n",
      "Epoch 6 | Batch 531/2072 | Loss: 0.0693\n",
      "Epoch 6 | Batch 541/2072 | Loss: 0.2269\n",
      "Epoch 6 | Batch 551/2072 | Loss: 0.0737\n",
      "Epoch 6 | Batch 561/2072 | Loss: 0.1634\n",
      "Epoch 6 | Batch 571/2072 | Loss: 0.1628\n",
      "Epoch 6 | Batch 581/2072 | Loss: 0.3059\n",
      "Epoch 6 | Batch 591/2072 | Loss: 0.0609\n",
      "Epoch 6 | Batch 601/2072 | Loss: 0.0429\n",
      "Epoch 6 | Batch 611/2072 | Loss: 0.5394\n",
      "Epoch 6 | Batch 621/2072 | Loss: 0.1273\n",
      "Epoch 6 | Batch 631/2072 | Loss: 0.2626\n",
      "Epoch 6 | Batch 641/2072 | Loss: 0.0253\n",
      "Epoch 6 | Batch 651/2072 | Loss: 0.1548\n",
      "Epoch 6 | Batch 661/2072 | Loss: 0.0877\n",
      "Epoch 6 | Batch 671/2072 | Loss: 0.0839\n",
      "Epoch 6 | Batch 681/2072 | Loss: 0.0203\n",
      "Epoch 6 | Batch 691/2072 | Loss: 0.3514\n",
      "Epoch 6 | Batch 701/2072 | Loss: 0.0351\n",
      "Epoch 6 | Batch 711/2072 | Loss: 0.0800\n",
      "Epoch 6 | Batch 721/2072 | Loss: 0.0182\n",
      "Epoch 6 | Batch 731/2072 | Loss: 0.1693\n",
      "Epoch 6 | Batch 741/2072 | Loss: 0.0115\n",
      "Epoch 6 | Batch 751/2072 | Loss: 0.1358\n",
      "Epoch 6 | Batch 761/2072 | Loss: 0.0586\n",
      "Epoch 6 | Batch 771/2072 | Loss: 0.0152\n",
      "Epoch 6 | Batch 781/2072 | Loss: 0.1543\n",
      "Epoch 6 | Batch 791/2072 | Loss: 0.0427\n",
      "Epoch 6 | Batch 801/2072 | Loss: 0.1746\n",
      "Epoch 6 | Batch 811/2072 | Loss: 0.1306\n",
      "Epoch 6 | Batch 821/2072 | Loss: 0.2313\n",
      "Epoch 6 | Batch 831/2072 | Loss: 0.0050\n",
      "Epoch 6 | Batch 841/2072 | Loss: 0.0379\n",
      "Epoch 6 | Batch 851/2072 | Loss: 0.0704\n",
      "Epoch 6 | Batch 861/2072 | Loss: 0.2325\n",
      "Epoch 6 | Batch 871/2072 | Loss: 0.0982\n",
      "Epoch 6 | Batch 881/2072 | Loss: 0.1709\n",
      "Epoch 6 | Batch 891/2072 | Loss: 0.0562\n",
      "Epoch 6 | Batch 901/2072 | Loss: 0.0488\n",
      "Epoch 6 | Batch 911/2072 | Loss: 0.0994\n",
      "Epoch 6 | Batch 921/2072 | Loss: 0.0607\n",
      "Epoch 6 | Batch 931/2072 | Loss: 0.1627\n",
      "Epoch 6 | Batch 941/2072 | Loss: 0.1365\n",
      "Epoch 6 | Batch 951/2072 | Loss: 0.2515\n",
      "Epoch 6 | Batch 961/2072 | Loss: 0.1159\n",
      "Epoch 6 | Batch 971/2072 | Loss: 0.2071\n",
      "Epoch 6 | Batch 981/2072 | Loss: 0.1721\n",
      "Epoch 6 | Batch 991/2072 | Loss: 0.0342\n",
      "Epoch 6 | Batch 1001/2072 | Loss: 0.0888\n",
      "Epoch 6 | Batch 1011/2072 | Loss: 0.2011\n",
      "Epoch 6 | Batch 1021/2072 | Loss: 0.0202\n",
      "Epoch 6 | Batch 1031/2072 | Loss: 0.5928\n",
      "Epoch 6 | Batch 1041/2072 | Loss: 0.1803\n",
      "Epoch 6 | Batch 1051/2072 | Loss: 0.1429\n",
      "Epoch 6 | Batch 1061/2072 | Loss: 0.1120\n",
      "Epoch 6 | Batch 1071/2072 | Loss: 0.0783\n",
      "Epoch 6 | Batch 1081/2072 | Loss: 0.0437\n",
      "Epoch 6 | Batch 1091/2072 | Loss: 0.5179\n",
      "Epoch 6 | Batch 1101/2072 | Loss: 0.0684\n",
      "Epoch 6 | Batch 1111/2072 | Loss: 0.0398\n",
      "Epoch 6 | Batch 1121/2072 | Loss: 0.2310\n",
      "Epoch 6 | Batch 1131/2072 | Loss: 0.0298\n",
      "Epoch 6 | Batch 1141/2072 | Loss: 0.4263\n",
      "Epoch 6 | Batch 1151/2072 | Loss: 0.0884\n",
      "Epoch 6 | Batch 1161/2072 | Loss: 0.0729\n",
      "Epoch 6 | Batch 1171/2072 | Loss: 0.0944\n",
      "Epoch 6 | Batch 1181/2072 | Loss: 0.1082\n",
      "Epoch 6 | Batch 1191/2072 | Loss: 0.0262\n",
      "Epoch 6 | Batch 1201/2072 | Loss: 0.4220\n",
      "Epoch 6 | Batch 1211/2072 | Loss: 0.0197\n",
      "Epoch 6 | Batch 1221/2072 | Loss: 0.1739\n",
      "Epoch 6 | Batch 1231/2072 | Loss: 0.2354\n",
      "Epoch 6 | Batch 1241/2072 | Loss: 0.0851\n",
      "Epoch 6 | Batch 1251/2072 | Loss: 0.0251\n",
      "Epoch 6 | Batch 1261/2072 | Loss: 0.1889\n",
      "Epoch 6 | Batch 1271/2072 | Loss: 0.0293\n",
      "Epoch 6 | Batch 1281/2072 | Loss: 0.0557\n",
      "Epoch 6 | Batch 1291/2072 | Loss: 0.4751\n",
      "Epoch 6 | Batch 1301/2072 | Loss: 0.1200\n",
      "Epoch 6 | Batch 1311/2072 | Loss: 0.1107\n",
      "Epoch 6 | Batch 1321/2072 | Loss: 0.0874\n",
      "Epoch 6 | Batch 1331/2072 | Loss: 0.1206\n",
      "Epoch 6 | Batch 1341/2072 | Loss: 0.1450\n",
      "Epoch 6 | Batch 1351/2072 | Loss: 0.0320\n",
      "Epoch 6 | Batch 1361/2072 | Loss: 0.1477\n",
      "Epoch 6 | Batch 1371/2072 | Loss: 0.0429\n",
      "Epoch 6 | Batch 1381/2072 | Loss: 0.0796\n",
      "Epoch 6 | Batch 1391/2072 | Loss: 0.5750\n",
      "Epoch 6 | Batch 1401/2072 | Loss: 0.0619\n",
      "Epoch 6 | Batch 1411/2072 | Loss: 0.2863\n",
      "Epoch 6 | Batch 1421/2072 | Loss: 0.0277\n",
      "Epoch 6 | Batch 1431/2072 | Loss: 0.0372\n",
      "Epoch 6 | Batch 1441/2072 | Loss: 0.0638\n",
      "Epoch 6 | Batch 1451/2072 | Loss: 0.1584\n",
      "Epoch 6 | Batch 1461/2072 | Loss: 0.1043\n",
      "Epoch 6 | Batch 1471/2072 | Loss: 0.1644\n",
      "Epoch 6 | Batch 1481/2072 | Loss: 0.0973\n",
      "Epoch 6 | Batch 1491/2072 | Loss: 0.1580\n",
      "Epoch 6 | Batch 1501/2072 | Loss: 0.0673\n",
      "Epoch 6 | Batch 1511/2072 | Loss: 0.1577\n",
      "Epoch 6 | Batch 1521/2072 | Loss: 0.0211\n",
      "Epoch 6 | Batch 1531/2072 | Loss: 0.0144\n",
      "Epoch 6 | Batch 1541/2072 | Loss: 0.0251\n",
      "Epoch 6 | Batch 1551/2072 | Loss: 0.3006\n",
      "Epoch 6 | Batch 1561/2072 | Loss: 0.1344\n",
      "Epoch 6 | Batch 1571/2072 | Loss: 0.0171\n",
      "Epoch 6 | Batch 1581/2072 | Loss: 0.0390\n",
      "Epoch 6 | Batch 1591/2072 | Loss: 0.1617\n",
      "Epoch 6 | Batch 1601/2072 | Loss: 0.1041\n",
      "Epoch 6 | Batch 1611/2072 | Loss: 0.5069\n",
      "Epoch 6 | Batch 1621/2072 | Loss: 0.1984\n",
      "Epoch 6 | Batch 1631/2072 | Loss: 0.0367\n",
      "Epoch 6 | Batch 1641/2072 | Loss: 0.3758\n",
      "Epoch 6 | Batch 1651/2072 | Loss: 0.0356\n",
      "Epoch 6 | Batch 1661/2072 | Loss: 0.0323\n",
      "Epoch 6 | Batch 1671/2072 | Loss: 0.1656\n",
      "Epoch 6 | Batch 1681/2072 | Loss: 0.0464\n",
      "Epoch 6 | Batch 1691/2072 | Loss: 0.0367\n",
      "Epoch 6 | Batch 1701/2072 | Loss: 0.1879\n",
      "Epoch 6 | Batch 1711/2072 | Loss: 0.0262\n",
      "Epoch 6 | Batch 1721/2072 | Loss: 0.1094\n",
      "Epoch 6 | Batch 1731/2072 | Loss: 0.1584\n",
      "Epoch 6 | Batch 1741/2072 | Loss: 0.1513\n",
      "Epoch 6 | Batch 1751/2072 | Loss: 0.1037\n",
      "Epoch 6 | Batch 1761/2072 | Loss: 0.2163\n",
      "Epoch 6 | Batch 1771/2072 | Loss: 0.3356\n",
      "Epoch 6 | Batch 1781/2072 | Loss: 0.0109\n",
      "Epoch 6 | Batch 1791/2072 | Loss: 0.2519\n",
      "Epoch 6 | Batch 1801/2072 | Loss: 0.1239\n",
      "Epoch 6 | Batch 1811/2072 | Loss: 0.2069\n",
      "Epoch 6 | Batch 1821/2072 | Loss: 0.1974\n",
      "Epoch 6 | Batch 1831/2072 | Loss: 0.1027\n",
      "Epoch 6 | Batch 1841/2072 | Loss: 0.1335\n",
      "Epoch 6 | Batch 1851/2072 | Loss: 0.4055\n",
      "Epoch 6 | Batch 1861/2072 | Loss: 0.0612\n",
      "Epoch 6 | Batch 1871/2072 | Loss: 0.1610\n",
      "Epoch 6 | Batch 1881/2072 | Loss: 0.0415\n",
      "Epoch 6 | Batch 1891/2072 | Loss: 0.2596\n",
      "Epoch 6 | Batch 1901/2072 | Loss: 0.2073\n",
      "Epoch 6 | Batch 1911/2072 | Loss: 0.1098\n",
      "Epoch 6 | Batch 1921/2072 | Loss: 0.0749\n",
      "Epoch 6 | Batch 1931/2072 | Loss: 0.0907\n",
      "Epoch 6 | Batch 1941/2072 | Loss: 0.0422\n",
      "Epoch 6 | Batch 1951/2072 | Loss: 0.0197\n",
      "Epoch 6 | Batch 1961/2072 | Loss: 0.1128\n",
      "Epoch 6 | Batch 1971/2072 | Loss: 0.0391\n",
      "Epoch 6 | Batch 1981/2072 | Loss: 0.3229\n",
      "Epoch 6 | Batch 1991/2072 | Loss: 0.5563\n",
      "Epoch 6 | Batch 2001/2072 | Loss: 0.0362\n",
      "Epoch 6 | Batch 2011/2072 | Loss: 0.0893\n",
      "Epoch 6 | Batch 2021/2072 | Loss: 0.2373\n",
      "Epoch 6 | Batch 2031/2072 | Loss: 0.2118\n",
      "Epoch 6 | Batch 2041/2072 | Loss: 0.2289\n",
      "Epoch 6 | Batch 2051/2072 | Loss: 0.0075\n",
      "Epoch 6 | Batch 2061/2072 | Loss: 0.1438\n",
      "Epoch 6 | Batch 2071/2072 | Loss: 0.0541\n",
      "[Epoch 6] Train Acc: 0.9474, Val Acc: 0.9572, Val LogLoss: 0.1176, Loss: 345.0298\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 7 | Batch 1/2072 | Loss: 0.0422\n",
      "Epoch 7 | Batch 11/2072 | Loss: 0.3519\n",
      "Epoch 7 | Batch 21/2072 | Loss: 0.2251\n",
      "Epoch 7 | Batch 31/2072 | Loss: 0.2489\n",
      "Epoch 7 | Batch 41/2072 | Loss: 0.0128\n",
      "Epoch 7 | Batch 51/2072 | Loss: 0.0869\n",
      "Epoch 7 | Batch 61/2072 | Loss: 0.2015\n",
      "Epoch 7 | Batch 71/2072 | Loss: 0.0277\n",
      "Epoch 7 | Batch 81/2072 | Loss: 0.1862\n",
      "Epoch 7 | Batch 91/2072 | Loss: 0.0945\n",
      "Epoch 7 | Batch 101/2072 | Loss: 0.0332\n",
      "Epoch 7 | Batch 111/2072 | Loss: 0.1867\n",
      "Epoch 7 | Batch 121/2072 | Loss: 0.1789\n",
      "Epoch 7 | Batch 131/2072 | Loss: 0.1192\n",
      "Epoch 7 | Batch 141/2072 | Loss: 0.0873\n",
      "Epoch 7 | Batch 151/2072 | Loss: 0.1707\n",
      "Epoch 7 | Batch 161/2072 | Loss: 0.0671\n",
      "Epoch 7 | Batch 171/2072 | Loss: 0.2466\n",
      "Epoch 7 | Batch 181/2072 | Loss: 0.0125\n",
      "Epoch 7 | Batch 191/2072 | Loss: 0.0484\n",
      "Epoch 7 | Batch 201/2072 | Loss: 0.1057\n",
      "Epoch 7 | Batch 211/2072 | Loss: 0.0159\n",
      "Epoch 7 | Batch 221/2072 | Loss: 0.0432\n",
      "Epoch 7 | Batch 231/2072 | Loss: 0.2875\n",
      "Epoch 7 | Batch 241/2072 | Loss: 0.0677\n",
      "Epoch 7 | Batch 251/2072 | Loss: 0.0221\n",
      "Epoch 7 | Batch 261/2072 | Loss: 0.0163\n",
      "Epoch 7 | Batch 271/2072 | Loss: 0.0397\n",
      "Epoch 7 | Batch 281/2072 | Loss: 0.1735\n",
      "Epoch 7 | Batch 291/2072 | Loss: 0.1840\n",
      "Epoch 7 | Batch 301/2072 | Loss: 0.2616\n",
      "Epoch 7 | Batch 311/2072 | Loss: 0.0349\n",
      "Epoch 7 | Batch 321/2072 | Loss: 0.2207\n",
      "Epoch 7 | Batch 331/2072 | Loss: 0.1070\n",
      "Epoch 7 | Batch 341/2072 | Loss: 0.1374\n",
      "Epoch 7 | Batch 351/2072 | Loss: 0.1627\n",
      "Epoch 7 | Batch 361/2072 | Loss: 0.1133\n",
      "Epoch 7 | Batch 371/2072 | Loss: 0.1297\n",
      "Epoch 7 | Batch 381/2072 | Loss: 0.3359\n",
      "Epoch 7 | Batch 391/2072 | Loss: 0.0416\n",
      "Epoch 7 | Batch 401/2072 | Loss: 0.1832\n",
      "Epoch 7 | Batch 411/2072 | Loss: 0.0550\n",
      "Epoch 7 | Batch 421/2072 | Loss: 0.2015\n",
      "Epoch 7 | Batch 431/2072 | Loss: 0.1447\n",
      "Epoch 7 | Batch 441/2072 | Loss: 0.2483\n",
      "Epoch 7 | Batch 451/2072 | Loss: 0.0921\n",
      "Epoch 7 | Batch 461/2072 | Loss: 0.2420\n",
      "Epoch 7 | Batch 471/2072 | Loss: 0.1149\n",
      "Epoch 7 | Batch 481/2072 | Loss: 0.1748\n",
      "Epoch 7 | Batch 491/2072 | Loss: 0.0976\n",
      "Epoch 7 | Batch 501/2072 | Loss: 0.0636\n",
      "Epoch 7 | Batch 511/2072 | Loss: 0.0042\n",
      "Epoch 7 | Batch 521/2072 | Loss: 0.0306\n",
      "Epoch 7 | Batch 531/2072 | Loss: 0.2044\n",
      "Epoch 7 | Batch 541/2072 | Loss: 0.0121\n",
      "Epoch 7 | Batch 551/2072 | Loss: 0.0444\n",
      "Epoch 7 | Batch 561/2072 | Loss: 0.0848\n",
      "Epoch 7 | Batch 571/2072 | Loss: 0.1132\n",
      "Epoch 7 | Batch 581/2072 | Loss: 0.0067\n",
      "Epoch 7 | Batch 591/2072 | Loss: 0.0942\n",
      "Epoch 7 | Batch 601/2072 | Loss: 0.0300\n",
      "Epoch 7 | Batch 611/2072 | Loss: 0.1424\n",
      "Epoch 7 | Batch 621/2072 | Loss: 0.0330\n",
      "Epoch 7 | Batch 631/2072 | Loss: 0.0102\n",
      "Epoch 7 | Batch 641/2072 | Loss: 0.0132\n",
      "Epoch 7 | Batch 651/2072 | Loss: 0.0842\n",
      "Epoch 7 | Batch 661/2072 | Loss: 0.0915\n",
      "Epoch 7 | Batch 671/2072 | Loss: 0.1490\n",
      "Epoch 7 | Batch 681/2072 | Loss: 0.1574\n",
      "Epoch 7 | Batch 691/2072 | Loss: 0.0341\n",
      "Epoch 7 | Batch 701/2072 | Loss: 0.1227\n",
      "Epoch 7 | Batch 711/2072 | Loss: 0.0543\n",
      "Epoch 7 | Batch 721/2072 | Loss: 0.1905\n",
      "Epoch 7 | Batch 731/2072 | Loss: 0.0067\n",
      "Epoch 7 | Batch 741/2072 | Loss: 0.0035\n",
      "Epoch 7 | Batch 751/2072 | Loss: 0.0075\n",
      "Epoch 7 | Batch 761/2072 | Loss: 0.1527\n",
      "Epoch 7 | Batch 771/2072 | Loss: 0.0653\n",
      "Epoch 7 | Batch 781/2072 | Loss: 0.0420\n",
      "Epoch 7 | Batch 791/2072 | Loss: 0.0994\n",
      "Epoch 7 | Batch 801/2072 | Loss: 0.1644\n",
      "Epoch 7 | Batch 811/2072 | Loss: 0.0367\n",
      "Epoch 7 | Batch 821/2072 | Loss: 0.1628\n",
      "Epoch 7 | Batch 831/2072 | Loss: 0.2037\n",
      "Epoch 7 | Batch 841/2072 | Loss: 0.2736\n",
      "Epoch 7 | Batch 851/2072 | Loss: 0.0069\n",
      "Epoch 7 | Batch 861/2072 | Loss: 0.2917\n",
      "Epoch 7 | Batch 871/2072 | Loss: 0.4928\n",
      "Epoch 7 | Batch 881/2072 | Loss: 0.0908\n",
      "Epoch 7 | Batch 891/2072 | Loss: 0.3401\n",
      "Epoch 7 | Batch 901/2072 | Loss: 0.0437\n",
      "Epoch 7 | Batch 911/2072 | Loss: 0.0903\n",
      "Epoch 7 | Batch 921/2072 | Loss: 0.0122\n",
      "Epoch 7 | Batch 931/2072 | Loss: 0.1800\n",
      "Epoch 7 | Batch 941/2072 | Loss: 0.1009\n",
      "Epoch 7 | Batch 951/2072 | Loss: 0.3043\n",
      "Epoch 7 | Batch 961/2072 | Loss: 0.3600\n",
      "Epoch 7 | Batch 971/2072 | Loss: 0.1882\n",
      "Epoch 7 | Batch 981/2072 | Loss: 0.0956\n",
      "Epoch 7 | Batch 991/2072 | Loss: 0.0665\n",
      "Epoch 7 | Batch 1001/2072 | Loss: 0.0352\n",
      "Epoch 7 | Batch 1011/2072 | Loss: 0.0060\n",
      "Epoch 7 | Batch 1021/2072 | Loss: 0.0774\n",
      "Epoch 7 | Batch 1031/2072 | Loss: 0.2135\n",
      "Epoch 7 | Batch 1041/2072 | Loss: 0.0555\n",
      "Epoch 7 | Batch 1051/2072 | Loss: 0.0639\n",
      "Epoch 7 | Batch 1061/2072 | Loss: 0.1628\n",
      "Epoch 7 | Batch 1071/2072 | Loss: 0.1756\n",
      "Epoch 7 | Batch 1081/2072 | Loss: 0.1721\n",
      "Epoch 7 | Batch 1091/2072 | Loss: 0.0527\n",
      "Epoch 7 | Batch 1101/2072 | Loss: 0.1270\n",
      "Epoch 7 | Batch 1111/2072 | Loss: 0.0774\n",
      "Epoch 7 | Batch 1121/2072 | Loss: 0.1030\n",
      "Epoch 7 | Batch 1131/2072 | Loss: 0.0444\n",
      "Epoch 7 | Batch 1141/2072 | Loss: 0.0638\n",
      "Epoch 7 | Batch 1151/2072 | Loss: 0.4742\n",
      "Epoch 7 | Batch 1161/2072 | Loss: 0.2673\n",
      "Epoch 7 | Batch 1171/2072 | Loss: 0.1266\n",
      "Epoch 7 | Batch 1181/2072 | Loss: 0.3497\n",
      "Epoch 7 | Batch 1191/2072 | Loss: 0.0525\n",
      "Epoch 7 | Batch 1201/2072 | Loss: 0.2391\n",
      "Epoch 7 | Batch 1211/2072 | Loss: 0.1053\n",
      "Epoch 7 | Batch 1221/2072 | Loss: 0.2253\n",
      "Epoch 7 | Batch 1231/2072 | Loss: 0.1416\n",
      "Epoch 7 | Batch 1241/2072 | Loss: 0.1044\n",
      "Epoch 7 | Batch 1251/2072 | Loss: 0.0070\n",
      "Epoch 7 | Batch 1261/2072 | Loss: 0.2324\n",
      "Epoch 7 | Batch 1271/2072 | Loss: 0.1694\n",
      "Epoch 7 | Batch 1281/2072 | Loss: 0.0884\n",
      "Epoch 7 | Batch 1291/2072 | Loss: 0.0456\n",
      "Epoch 7 | Batch 1301/2072 | Loss: 0.0365\n",
      "Epoch 7 | Batch 1311/2072 | Loss: 0.0302\n",
      "Epoch 7 | Batch 1321/2072 | Loss: 0.2375\n",
      "Epoch 7 | Batch 1331/2072 | Loss: 0.0095\n",
      "Epoch 7 | Batch 1341/2072 | Loss: 0.3010\n",
      "Epoch 7 | Batch 1351/2072 | Loss: 0.1968\n",
      "Epoch 7 | Batch 1361/2072 | Loss: 0.0180\n",
      "Epoch 7 | Batch 1371/2072 | Loss: 0.0492\n",
      "Epoch 7 | Batch 1381/2072 | Loss: 0.1562\n",
      "Epoch 7 | Batch 1391/2072 | Loss: 0.1654\n",
      "Epoch 7 | Batch 1401/2072 | Loss: 0.0957\n",
      "Epoch 7 | Batch 1411/2072 | Loss: 0.1950\n",
      "Epoch 7 | Batch 1421/2072 | Loss: 0.0739\n",
      "Epoch 7 | Batch 1431/2072 | Loss: 0.0536\n",
      "Epoch 7 | Batch 1441/2072 | Loss: 0.0922\n",
      "Epoch 7 | Batch 1451/2072 | Loss: 0.1240\n",
      "Epoch 7 | Batch 1461/2072 | Loss: 0.0939\n",
      "Epoch 7 | Batch 1471/2072 | Loss: 0.4542\n",
      "Epoch 7 | Batch 1481/2072 | Loss: 0.1059\n",
      "Epoch 7 | Batch 1491/2072 | Loss: 0.0937\n",
      "Epoch 7 | Batch 1501/2072 | Loss: 0.7764\n",
      "Epoch 7 | Batch 1511/2072 | Loss: 0.1510\n",
      "Epoch 7 | Batch 1521/2072 | Loss: 0.0598\n",
      "Epoch 7 | Batch 1531/2072 | Loss: 0.1453\n",
      "Epoch 7 | Batch 1541/2072 | Loss: 0.0430\n",
      "Epoch 7 | Batch 1551/2072 | Loss: 0.1768\n",
      "Epoch 7 | Batch 1561/2072 | Loss: 0.4185\n",
      "Epoch 7 | Batch 1571/2072 | Loss: 0.2073\n",
      "Epoch 7 | Batch 1581/2072 | Loss: 0.0194\n",
      "Epoch 7 | Batch 1591/2072 | Loss: 0.1471\n",
      "Epoch 7 | Batch 1601/2072 | Loss: 0.0543\n",
      "Epoch 7 | Batch 1611/2072 | Loss: 0.3602\n",
      "Epoch 7 | Batch 1621/2072 | Loss: 0.0115\n",
      "Epoch 7 | Batch 1631/2072 | Loss: 0.2963\n",
      "Epoch 7 | Batch 1641/2072 | Loss: 0.1477\n",
      "Epoch 7 | Batch 1651/2072 | Loss: 0.0736\n",
      "Epoch 7 | Batch 1661/2072 | Loss: 0.2188\n",
      "Epoch 7 | Batch 1671/2072 | Loss: 0.1185\n",
      "Epoch 7 | Batch 1681/2072 | Loss: 0.0275\n",
      "Epoch 7 | Batch 1691/2072 | Loss: 0.0368\n",
      "Epoch 7 | Batch 1701/2072 | Loss: 0.0648\n",
      "Epoch 7 | Batch 1711/2072 | Loss: 0.0631\n",
      "Epoch 7 | Batch 1721/2072 | Loss: 0.0409\n",
      "Epoch 7 | Batch 1731/2072 | Loss: 0.0785\n",
      "Epoch 7 | Batch 1741/2072 | Loss: 0.0994\n",
      "Epoch 7 | Batch 1751/2072 | Loss: 0.2048\n",
      "Epoch 7 | Batch 1761/2072 | Loss: 0.1296\n",
      "Epoch 7 | Batch 1771/2072 | Loss: 0.2380\n",
      "Epoch 7 | Batch 1781/2072 | Loss: 0.0196\n",
      "Epoch 7 | Batch 1791/2072 | Loss: 0.0128\n",
      "Epoch 7 | Batch 1801/2072 | Loss: 0.1696\n",
      "Epoch 7 | Batch 1811/2072 | Loss: 0.0075\n",
      "Epoch 7 | Batch 1821/2072 | Loss: 0.6172\n",
      "Epoch 7 | Batch 1831/2072 | Loss: 0.1493\n",
      "Epoch 7 | Batch 1841/2072 | Loss: 0.3508\n",
      "Epoch 7 | Batch 1851/2072 | Loss: 0.4295\n",
      "Epoch 7 | Batch 1861/2072 | Loss: 0.1061\n",
      "Epoch 7 | Batch 1871/2072 | Loss: 0.0151\n",
      "Epoch 7 | Batch 1881/2072 | Loss: 0.3465\n",
      "Epoch 7 | Batch 1891/2072 | Loss: 0.2703\n",
      "Epoch 7 | Batch 1901/2072 | Loss: 0.0714\n",
      "Epoch 7 | Batch 1911/2072 | Loss: 0.0317\n",
      "Epoch 7 | Batch 1921/2072 | Loss: 0.0391\n",
      "Epoch 7 | Batch 1931/2072 | Loss: 0.0469\n",
      "Epoch 7 | Batch 1941/2072 | Loss: 0.0833\n",
      "Epoch 7 | Batch 1951/2072 | Loss: 0.2891\n",
      "Epoch 7 | Batch 1961/2072 | Loss: 0.1485\n",
      "Epoch 7 | Batch 1971/2072 | Loss: 0.0289\n",
      "Epoch 7 | Batch 1981/2072 | Loss: 0.2877\n",
      "Epoch 7 | Batch 1991/2072 | Loss: 0.3112\n",
      "Epoch 7 | Batch 2001/2072 | Loss: 0.0634\n",
      "Epoch 7 | Batch 2011/2072 | Loss: 0.0782\n",
      "Epoch 7 | Batch 2021/2072 | Loss: 0.0419\n",
      "Epoch 7 | Batch 2031/2072 | Loss: 0.0666\n",
      "Epoch 7 | Batch 2041/2072 | Loss: 0.0687\n",
      "Epoch 7 | Batch 2051/2072 | Loss: 0.3123\n",
      "Epoch 7 | Batch 2061/2072 | Loss: 0.1491\n",
      "Epoch 7 | Batch 2071/2072 | Loss: 0.2599\n",
      "[Epoch 7] Train Acc: 0.9536, Val Acc: 0.9490, Val LogLoss: 0.1562, Loss: 300.9042\n",
      "‚ö†Ô∏è No improvement. EarlyStopping counter: 1/5\n",
      "Epoch 8 | Batch 1/2072 | Loss: 0.0191\n",
      "Epoch 8 | Batch 11/2072 | Loss: 0.0707\n",
      "Epoch 8 | Batch 21/2072 | Loss: 0.0263\n",
      "Epoch 8 | Batch 31/2072 | Loss: 0.2766\n",
      "Epoch 8 | Batch 41/2072 | Loss: 0.1846\n",
      "Epoch 8 | Batch 51/2072 | Loss: 0.2583\n",
      "Epoch 8 | Batch 61/2072 | Loss: 0.0143\n",
      "Epoch 8 | Batch 71/2072 | Loss: 0.0697\n",
      "Epoch 8 | Batch 81/2072 | Loss: 0.0539\n",
      "Epoch 8 | Batch 91/2072 | Loss: 0.1046\n",
      "Epoch 8 | Batch 101/2072 | Loss: 0.1128\n",
      "Epoch 8 | Batch 111/2072 | Loss: 0.1583\n",
      "Epoch 8 | Batch 121/2072 | Loss: 0.1414\n",
      "Epoch 8 | Batch 131/2072 | Loss: 0.1141\n",
      "Epoch 8 | Batch 141/2072 | Loss: 0.0340\n",
      "Epoch 8 | Batch 151/2072 | Loss: 0.0303\n",
      "Epoch 8 | Batch 161/2072 | Loss: 0.4601\n",
      "Epoch 8 | Batch 171/2072 | Loss: 0.0359\n",
      "Epoch 8 | Batch 181/2072 | Loss: 0.0184\n",
      "Epoch 8 | Batch 191/2072 | Loss: 0.0541\n",
      "Epoch 8 | Batch 201/2072 | Loss: 0.0840\n",
      "Epoch 8 | Batch 211/2072 | Loss: 0.0394\n",
      "Epoch 8 | Batch 221/2072 | Loss: 0.0167\n",
      "Epoch 8 | Batch 231/2072 | Loss: 0.1074\n",
      "Epoch 8 | Batch 241/2072 | Loss: 0.1089\n",
      "Epoch 8 | Batch 251/2072 | Loss: 0.0117\n",
      "Epoch 8 | Batch 261/2072 | Loss: 0.0170\n",
      "Epoch 8 | Batch 271/2072 | Loss: 0.0098\n",
      "Epoch 8 | Batch 281/2072 | Loss: 0.0478\n",
      "Epoch 8 | Batch 291/2072 | Loss: 0.0079\n",
      "Epoch 8 | Batch 301/2072 | Loss: 0.3604\n",
      "Epoch 8 | Batch 311/2072 | Loss: 0.0366\n",
      "Epoch 8 | Batch 321/2072 | Loss: 0.0369\n",
      "Epoch 8 | Batch 331/2072 | Loss: 0.0243\n",
      "Epoch 8 | Batch 341/2072 | Loss: 0.2808\n",
      "Epoch 8 | Batch 351/2072 | Loss: 0.0415\n",
      "Epoch 8 | Batch 361/2072 | Loss: 0.0368\n",
      "Epoch 8 | Batch 371/2072 | Loss: 0.2095\n",
      "Epoch 8 | Batch 381/2072 | Loss: 0.0666\n",
      "Epoch 8 | Batch 391/2072 | Loss: 0.0052\n",
      "Epoch 8 | Batch 401/2072 | Loss: 0.2143\n",
      "Epoch 8 | Batch 411/2072 | Loss: 0.2113\n",
      "Epoch 8 | Batch 421/2072 | Loss: 0.1597\n",
      "Epoch 8 | Batch 431/2072 | Loss: 0.0166\n",
      "Epoch 8 | Batch 441/2072 | Loss: 0.0209\n",
      "Epoch 8 | Batch 451/2072 | Loss: 0.0285\n",
      "Epoch 8 | Batch 461/2072 | Loss: 0.0118\n",
      "Epoch 8 | Batch 471/2072 | Loss: 0.1551\n",
      "Epoch 8 | Batch 481/2072 | Loss: 0.0151\n",
      "Epoch 8 | Batch 491/2072 | Loss: 0.1619\n",
      "Epoch 8 | Batch 501/2072 | Loss: 0.0961\n",
      "Epoch 8 | Batch 511/2072 | Loss: 0.0489\n",
      "Epoch 8 | Batch 521/2072 | Loss: 0.0975\n",
      "Epoch 8 | Batch 531/2072 | Loss: 0.3314\n",
      "Epoch 8 | Batch 541/2072 | Loss: 0.2606\n",
      "Epoch 8 | Batch 551/2072 | Loss: 0.0231\n",
      "Epoch 8 | Batch 561/2072 | Loss: 0.1243\n",
      "Epoch 8 | Batch 571/2072 | Loss: 0.0470\n",
      "Epoch 8 | Batch 581/2072 | Loss: 0.0394\n",
      "Epoch 8 | Batch 591/2072 | Loss: 0.1350\n",
      "Epoch 8 | Batch 601/2072 | Loss: 0.0058\n",
      "Epoch 8 | Batch 611/2072 | Loss: 0.0344\n",
      "Epoch 8 | Batch 621/2072 | Loss: 0.2791\n",
      "Epoch 8 | Batch 631/2072 | Loss: 0.0634\n",
      "Epoch 8 | Batch 641/2072 | Loss: 0.3648\n",
      "Epoch 8 | Batch 651/2072 | Loss: 0.1540\n",
      "Epoch 8 | Batch 661/2072 | Loss: 0.0373\n",
      "Epoch 8 | Batch 671/2072 | Loss: 0.0537\n",
      "Epoch 8 | Batch 681/2072 | Loss: 0.0703\n",
      "Epoch 8 | Batch 691/2072 | Loss: 0.0340\n",
      "Epoch 8 | Batch 701/2072 | Loss: 0.0057\n",
      "Epoch 8 | Batch 711/2072 | Loss: 0.1588\n",
      "Epoch 8 | Batch 721/2072 | Loss: 0.1588\n",
      "Epoch 8 | Batch 731/2072 | Loss: 0.2042\n",
      "Epoch 8 | Batch 741/2072 | Loss: 0.1925\n",
      "Epoch 8 | Batch 751/2072 | Loss: 0.0068\n",
      "Epoch 8 | Batch 761/2072 | Loss: 0.0201\n",
      "Epoch 8 | Batch 771/2072 | Loss: 0.0140\n",
      "Epoch 8 | Batch 781/2072 | Loss: 0.2260\n",
      "Epoch 8 | Batch 791/2072 | Loss: 0.0971\n",
      "Epoch 8 | Batch 801/2072 | Loss: 0.0980\n",
      "Epoch 8 | Batch 811/2072 | Loss: 0.0806\n",
      "Epoch 8 | Batch 821/2072 | Loss: 0.0041\n",
      "Epoch 8 | Batch 831/2072 | Loss: 0.3202\n",
      "Epoch 8 | Batch 841/2072 | Loss: 0.1557\n",
      "Epoch 8 | Batch 851/2072 | Loss: 0.0984\n",
      "Epoch 8 | Batch 861/2072 | Loss: 0.1248\n",
      "Epoch 8 | Batch 871/2072 | Loss: 0.0792\n",
      "Epoch 8 | Batch 881/2072 | Loss: 0.4423\n",
      "Epoch 8 | Batch 891/2072 | Loss: 0.1942\n",
      "Epoch 8 | Batch 901/2072 | Loss: 0.0131\n",
      "Epoch 8 | Batch 911/2072 | Loss: 0.0580\n",
      "Epoch 8 | Batch 921/2072 | Loss: 0.0151\n",
      "Epoch 8 | Batch 931/2072 | Loss: 0.1472\n",
      "Epoch 8 | Batch 941/2072 | Loss: 0.2326\n",
      "Epoch 8 | Batch 951/2072 | Loss: 0.2990\n",
      "Epoch 8 | Batch 961/2072 | Loss: 0.0809\n",
      "Epoch 8 | Batch 971/2072 | Loss: 0.2367\n",
      "Epoch 8 | Batch 981/2072 | Loss: 0.0319\n",
      "Epoch 8 | Batch 991/2072 | Loss: 0.0324\n",
      "Epoch 8 | Batch 1001/2072 | Loss: 0.0052\n",
      "Epoch 8 | Batch 1011/2072 | Loss: 0.4666\n",
      "Epoch 8 | Batch 1021/2072 | Loss: 0.0145\n",
      "Epoch 8 | Batch 1031/2072 | Loss: 0.0743\n",
      "Epoch 8 | Batch 1041/2072 | Loss: 0.0410\n",
      "Epoch 8 | Batch 1051/2072 | Loss: 0.1254\n",
      "Epoch 8 | Batch 1061/2072 | Loss: 0.0435\n",
      "Epoch 8 | Batch 1071/2072 | Loss: 0.0563\n",
      "Epoch 8 | Batch 1081/2072 | Loss: 0.0783\n",
      "Epoch 8 | Batch 1091/2072 | Loss: 0.0821\n",
      "Epoch 8 | Batch 1101/2072 | Loss: 0.0189\n",
      "Epoch 8 | Batch 1111/2072 | Loss: 0.1460\n",
      "Epoch 8 | Batch 1121/2072 | Loss: 0.1430\n",
      "Epoch 8 | Batch 1131/2072 | Loss: 0.0288\n",
      "Epoch 8 | Batch 1141/2072 | Loss: 0.0774\n",
      "Epoch 8 | Batch 1151/2072 | Loss: 0.0731\n",
      "Epoch 8 | Batch 1161/2072 | Loss: 0.2280\n",
      "Epoch 8 | Batch 1171/2072 | Loss: 0.0096\n",
      "Epoch 8 | Batch 1181/2072 | Loss: 0.1697\n",
      "Epoch 8 | Batch 1191/2072 | Loss: 0.0947\n",
      "Epoch 8 | Batch 1201/2072 | Loss: 0.2332\n",
      "Epoch 8 | Batch 1211/2072 | Loss: 0.6868\n",
      "Epoch 8 | Batch 1221/2072 | Loss: 0.1658\n",
      "Epoch 8 | Batch 1231/2072 | Loss: 0.3439\n",
      "Epoch 8 | Batch 1241/2072 | Loss: 0.0412\n",
      "Epoch 8 | Batch 1251/2072 | Loss: 0.2836\n",
      "Epoch 8 | Batch 1261/2072 | Loss: 0.2044\n",
      "Epoch 8 | Batch 1271/2072 | Loss: 0.0567\n",
      "Epoch 8 | Batch 1281/2072 | Loss: 0.1285\n",
      "Epoch 8 | Batch 1291/2072 | Loss: 0.4310\n",
      "Epoch 8 | Batch 1301/2072 | Loss: 0.0596\n",
      "Epoch 8 | Batch 1311/2072 | Loss: 0.1173\n",
      "Epoch 8 | Batch 1321/2072 | Loss: 0.1949\n",
      "Epoch 8 | Batch 1331/2072 | Loss: 0.0540\n",
      "Epoch 8 | Batch 1341/2072 | Loss: 0.2950\n",
      "Epoch 8 | Batch 1351/2072 | Loss: 0.1620\n",
      "Epoch 8 | Batch 1361/2072 | Loss: 0.0187\n",
      "Epoch 8 | Batch 1371/2072 | Loss: 0.1072\n",
      "Epoch 8 | Batch 1381/2072 | Loss: 0.0931\n",
      "Epoch 8 | Batch 1391/2072 | Loss: 0.1045\n",
      "Epoch 8 | Batch 1401/2072 | Loss: 0.0342\n",
      "Epoch 8 | Batch 1411/2072 | Loss: 0.0081\n",
      "Epoch 8 | Batch 1421/2072 | Loss: 0.0176\n",
      "Epoch 8 | Batch 1431/2072 | Loss: 0.0109\n",
      "Epoch 8 | Batch 1441/2072 | Loss: 0.1842\n",
      "Epoch 8 | Batch 1451/2072 | Loss: 0.3058\n",
      "Epoch 8 | Batch 1461/2072 | Loss: 0.0722\n",
      "Epoch 8 | Batch 1471/2072 | Loss: 0.0376\n",
      "Epoch 8 | Batch 1481/2072 | Loss: 0.0384\n",
      "Epoch 8 | Batch 1491/2072 | Loss: 0.0214\n",
      "Epoch 8 | Batch 1501/2072 | Loss: 0.0497\n",
      "Epoch 8 | Batch 1511/2072 | Loss: 0.2169\n",
      "Epoch 8 | Batch 1521/2072 | Loss: 0.5101\n",
      "Epoch 8 | Batch 1531/2072 | Loss: 0.0371\n",
      "Epoch 8 | Batch 1541/2072 | Loss: 0.0861\n",
      "Epoch 8 | Batch 1551/2072 | Loss: 0.1611\n",
      "Epoch 8 | Batch 1561/2072 | Loss: 0.0165\n",
      "Epoch 8 | Batch 1571/2072 | Loss: 0.2960\n",
      "Epoch 8 | Batch 1581/2072 | Loss: 0.3267\n",
      "Epoch 8 | Batch 1591/2072 | Loss: 0.0979\n",
      "Epoch 8 | Batch 1601/2072 | Loss: 0.1364\n",
      "Epoch 8 | Batch 1611/2072 | Loss: 0.1655\n",
      "Epoch 8 | Batch 1621/2072 | Loss: 0.2854\n",
      "Epoch 8 | Batch 1631/2072 | Loss: 0.0600\n",
      "Epoch 8 | Batch 1641/2072 | Loss: 0.2264\n",
      "Epoch 8 | Batch 1651/2072 | Loss: 0.0255\n",
      "Epoch 8 | Batch 1661/2072 | Loss: 0.0174\n",
      "Epoch 8 | Batch 1671/2072 | Loss: 0.0131\n",
      "Epoch 8 | Batch 1681/2072 | Loss: 0.0540\n",
      "Epoch 8 | Batch 1691/2072 | Loss: 0.0245\n",
      "Epoch 8 | Batch 1701/2072 | Loss: 0.0343\n",
      "Epoch 8 | Batch 1711/2072 | Loss: 0.1163\n",
      "Epoch 8 | Batch 1721/2072 | Loss: 0.0231\n",
      "Epoch 8 | Batch 1731/2072 | Loss: 0.3926\n",
      "Epoch 8 | Batch 1741/2072 | Loss: 0.0107\n",
      "Epoch 8 | Batch 1751/2072 | Loss: 0.0893\n",
      "Epoch 8 | Batch 1761/2072 | Loss: 0.0495\n",
      "Epoch 8 | Batch 1771/2072 | Loss: 0.0556\n",
      "Epoch 8 | Batch 1781/2072 | Loss: 0.4762\n",
      "Epoch 8 | Batch 1791/2072 | Loss: 0.0542\n",
      "Epoch 8 | Batch 1801/2072 | Loss: 0.2387\n",
      "Epoch 8 | Batch 1811/2072 | Loss: 0.3766\n",
      "Epoch 8 | Batch 1821/2072 | Loss: 0.2240\n",
      "Epoch 8 | Batch 1831/2072 | Loss: 0.2292\n",
      "Epoch 8 | Batch 1841/2072 | Loss: 0.1567\n",
      "Epoch 8 | Batch 1851/2072 | Loss: 0.3640\n",
      "Epoch 8 | Batch 1861/2072 | Loss: 0.1072\n",
      "Epoch 8 | Batch 1871/2072 | Loss: 0.0348\n",
      "Epoch 8 | Batch 1881/2072 | Loss: 0.3614\n",
      "Epoch 8 | Batch 1891/2072 | Loss: 0.3057\n",
      "Epoch 8 | Batch 1901/2072 | Loss: 0.2635\n",
      "Epoch 8 | Batch 1911/2072 | Loss: 0.6523\n",
      "Epoch 8 | Batch 1921/2072 | Loss: 0.2335\n",
      "Epoch 8 | Batch 1931/2072 | Loss: 0.3251\n",
      "Epoch 8 | Batch 1941/2072 | Loss: 0.2983\n",
      "Epoch 8 | Batch 1951/2072 | Loss: 0.0755\n",
      "Epoch 8 | Batch 1961/2072 | Loss: 0.0372\n",
      "Epoch 8 | Batch 1971/2072 | Loss: 0.2041\n",
      "Epoch 8 | Batch 1981/2072 | Loss: 0.3703\n",
      "Epoch 8 | Batch 1991/2072 | Loss: 0.1296\n",
      "Epoch 8 | Batch 2001/2072 | Loss: 0.1024\n",
      "Epoch 8 | Batch 2011/2072 | Loss: 0.0678\n",
      "Epoch 8 | Batch 2021/2072 | Loss: 0.0055\n",
      "Epoch 8 | Batch 2031/2072 | Loss: 0.0337\n",
      "Epoch 8 | Batch 2041/2072 | Loss: 0.0700\n",
      "Epoch 8 | Batch 2051/2072 | Loss: 0.0707\n",
      "Epoch 8 | Batch 2061/2072 | Loss: 0.2331\n",
      "Epoch 8 | Batch 2071/2072 | Loss: 0.0357\n",
      "[Epoch 8] Train Acc: 0.9605, Val Acc: 0.9756, Val LogLoss: 0.0682, Loss: 261.3246\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 9 | Batch 1/2072 | Loss: 0.0080\n",
      "Epoch 9 | Batch 11/2072 | Loss: 0.0199\n",
      "Epoch 9 | Batch 21/2072 | Loss: 0.0057\n",
      "Epoch 9 | Batch 31/2072 | Loss: 0.2967\n",
      "Epoch 9 | Batch 41/2072 | Loss: 0.3846\n",
      "Epoch 9 | Batch 51/2072 | Loss: 0.0500\n",
      "Epoch 9 | Batch 61/2072 | Loss: 0.0139\n",
      "Epoch 9 | Batch 71/2072 | Loss: 0.2536\n",
      "Epoch 9 | Batch 81/2072 | Loss: 0.1634\n",
      "Epoch 9 | Batch 91/2072 | Loss: 0.0432\n",
      "Epoch 9 | Batch 101/2072 | Loss: 0.1011\n",
      "Epoch 9 | Batch 111/2072 | Loss: 0.2104\n",
      "Epoch 9 | Batch 121/2072 | Loss: 0.0290\n",
      "Epoch 9 | Batch 131/2072 | Loss: 0.1071\n",
      "Epoch 9 | Batch 141/2072 | Loss: 0.1072\n",
      "Epoch 9 | Batch 151/2072 | Loss: 0.0500\n",
      "Epoch 9 | Batch 161/2072 | Loss: 0.0593\n",
      "Epoch 9 | Batch 171/2072 | Loss: 0.1930\n",
      "Epoch 9 | Batch 181/2072 | Loss: 0.0118\n",
      "Epoch 9 | Batch 191/2072 | Loss: 0.0635\n",
      "Epoch 9 | Batch 201/2072 | Loss: 0.0821\n",
      "Epoch 9 | Batch 211/2072 | Loss: 0.1250\n",
      "Epoch 9 | Batch 221/2072 | Loss: 0.0493\n",
      "Epoch 9 | Batch 231/2072 | Loss: 0.0103\n",
      "Epoch 9 | Batch 241/2072 | Loss: 0.0882\n",
      "Epoch 9 | Batch 251/2072 | Loss: 0.0571\n",
      "Epoch 9 | Batch 261/2072 | Loss: 0.0322\n",
      "Epoch 9 | Batch 271/2072 | Loss: 0.1626\n",
      "Epoch 9 | Batch 281/2072 | Loss: 0.0443\n",
      "Epoch 9 | Batch 291/2072 | Loss: 0.0985\n",
      "Epoch 9 | Batch 301/2072 | Loss: 0.0020\n",
      "Epoch 9 | Batch 311/2072 | Loss: 0.0093\n",
      "Epoch 9 | Batch 321/2072 | Loss: 0.0081\n",
      "Epoch 9 | Batch 331/2072 | Loss: 0.1170\n",
      "Epoch 9 | Batch 341/2072 | Loss: 0.0147\n",
      "Epoch 9 | Batch 351/2072 | Loss: 0.1851\n",
      "Epoch 9 | Batch 361/2072 | Loss: 0.1977\n",
      "Epoch 9 | Batch 371/2072 | Loss: 0.0688\n",
      "Epoch 9 | Batch 381/2072 | Loss: 0.2611\n",
      "Epoch 9 | Batch 391/2072 | Loss: 0.1863\n",
      "Epoch 9 | Batch 401/2072 | Loss: 0.0288\n",
      "Epoch 9 | Batch 411/2072 | Loss: 0.2132\n",
      "Epoch 9 | Batch 421/2072 | Loss: 0.1388\n",
      "Epoch 9 | Batch 431/2072 | Loss: 0.0036\n",
      "Epoch 9 | Batch 441/2072 | Loss: 0.1075\n",
      "Epoch 9 | Batch 451/2072 | Loss: 0.0182\n",
      "Epoch 9 | Batch 461/2072 | Loss: 0.0234\n",
      "Epoch 9 | Batch 471/2072 | Loss: 0.4353\n",
      "Epoch 9 | Batch 481/2072 | Loss: 0.0063\n",
      "Epoch 9 | Batch 491/2072 | Loss: 0.0813\n",
      "Epoch 9 | Batch 501/2072 | Loss: 0.0663\n",
      "Epoch 9 | Batch 511/2072 | Loss: 0.0656\n",
      "Epoch 9 | Batch 521/2072 | Loss: 0.6581\n",
      "Epoch 9 | Batch 531/2072 | Loss: 0.1531\n",
      "Epoch 9 | Batch 541/2072 | Loss: 0.0396\n",
      "Epoch 9 | Batch 551/2072 | Loss: 0.0956\n",
      "Epoch 9 | Batch 561/2072 | Loss: 0.1481\n",
      "Epoch 9 | Batch 571/2072 | Loss: 0.0293\n",
      "Epoch 9 | Batch 581/2072 | Loss: 0.0141\n",
      "Epoch 9 | Batch 591/2072 | Loss: 0.2156\n",
      "Epoch 9 | Batch 601/2072 | Loss: 0.0462\n",
      "Epoch 9 | Batch 611/2072 | Loss: 0.2086\n",
      "Epoch 9 | Batch 621/2072 | Loss: 0.0216\n",
      "Epoch 9 | Batch 631/2072 | Loss: 0.0829\n",
      "Epoch 9 | Batch 641/2072 | Loss: 0.0502\n",
      "Epoch 9 | Batch 651/2072 | Loss: 0.0430\n",
      "Epoch 9 | Batch 661/2072 | Loss: 0.0155\n",
      "Epoch 9 | Batch 671/2072 | Loss: 0.0920\n",
      "Epoch 9 | Batch 681/2072 | Loss: 0.0559\n",
      "Epoch 9 | Batch 691/2072 | Loss: 0.0864\n",
      "Epoch 9 | Batch 701/2072 | Loss: 0.1221\n",
      "Epoch 9 | Batch 711/2072 | Loss: 0.1250\n",
      "Epoch 9 | Batch 721/2072 | Loss: 0.0793\n",
      "Epoch 9 | Batch 731/2072 | Loss: 0.0524\n",
      "Epoch 9 | Batch 741/2072 | Loss: 0.0840\n",
      "Epoch 9 | Batch 751/2072 | Loss: 0.2270\n",
      "Epoch 9 | Batch 761/2072 | Loss: 0.0096\n",
      "Epoch 9 | Batch 771/2072 | Loss: 0.0552\n",
      "Epoch 9 | Batch 781/2072 | Loss: 0.0107\n",
      "Epoch 9 | Batch 791/2072 | Loss: 0.0027\n",
      "Epoch 9 | Batch 801/2072 | Loss: 0.0231\n",
      "Epoch 9 | Batch 811/2072 | Loss: 0.1142\n",
      "Epoch 9 | Batch 821/2072 | Loss: 0.4588\n",
      "Epoch 9 | Batch 831/2072 | Loss: 0.2631\n",
      "Epoch 9 | Batch 841/2072 | Loss: 0.1059\n",
      "Epoch 9 | Batch 851/2072 | Loss: 0.2439\n",
      "Epoch 9 | Batch 861/2072 | Loss: 0.1903\n",
      "Epoch 9 | Batch 871/2072 | Loss: 0.0266\n",
      "Epoch 9 | Batch 881/2072 | Loss: 0.0150\n",
      "Epoch 9 | Batch 891/2072 | Loss: 0.0637\n",
      "Epoch 9 | Batch 901/2072 | Loss: 0.3454\n",
      "Epoch 9 | Batch 911/2072 | Loss: 0.0043\n",
      "Epoch 9 | Batch 921/2072 | Loss: 0.0173\n",
      "Epoch 9 | Batch 931/2072 | Loss: 0.0733\n",
      "Epoch 9 | Batch 941/2072 | Loss: 0.1209\n",
      "Epoch 9 | Batch 951/2072 | Loss: 0.1654\n",
      "Epoch 9 | Batch 961/2072 | Loss: 0.0330\n",
      "Epoch 9 | Batch 971/2072 | Loss: 0.1147\n",
      "Epoch 9 | Batch 981/2072 | Loss: 0.0556\n",
      "Epoch 9 | Batch 991/2072 | Loss: 0.0517\n",
      "Epoch 9 | Batch 1001/2072 | Loss: 0.3097\n",
      "Epoch 9 | Batch 1011/2072 | Loss: 0.2855\n",
      "Epoch 9 | Batch 1021/2072 | Loss: 0.1618\n",
      "Epoch 9 | Batch 1031/2072 | Loss: 0.0097\n",
      "Epoch 9 | Batch 1041/2072 | Loss: 0.0140\n",
      "Epoch 9 | Batch 1051/2072 | Loss: 0.0230\n",
      "Epoch 9 | Batch 1061/2072 | Loss: 0.3647\n",
      "Epoch 9 | Batch 1071/2072 | Loss: 0.0236\n",
      "Epoch 9 | Batch 1081/2072 | Loss: 0.1440\n",
      "Epoch 9 | Batch 1091/2072 | Loss: 0.3674\n",
      "Epoch 9 | Batch 1101/2072 | Loss: 0.0544\n",
      "Epoch 9 | Batch 1111/2072 | Loss: 0.3177\n",
      "Epoch 9 | Batch 1121/2072 | Loss: 0.1660\n",
      "Epoch 9 | Batch 1131/2072 | Loss: 0.0610\n",
      "Epoch 9 | Batch 1141/2072 | Loss: 0.0107\n",
      "Epoch 9 | Batch 1151/2072 | Loss: 0.0861\n",
      "Epoch 9 | Batch 1161/2072 | Loss: 0.0099\n",
      "Epoch 9 | Batch 1171/2072 | Loss: 0.0334\n",
      "Epoch 9 | Batch 1181/2072 | Loss: 0.3496\n",
      "Epoch 9 | Batch 1191/2072 | Loss: 0.0171\n",
      "Epoch 9 | Batch 1201/2072 | Loss: 0.0753\n",
      "Epoch 9 | Batch 1211/2072 | Loss: 0.0248\n",
      "Epoch 9 | Batch 1221/2072 | Loss: 0.0125\n",
      "Epoch 9 | Batch 1231/2072 | Loss: 0.1816\n",
      "Epoch 9 | Batch 1241/2072 | Loss: 0.3569\n",
      "Epoch 9 | Batch 1251/2072 | Loss: 0.0649\n",
      "Epoch 9 | Batch 1261/2072 | Loss: 0.1852\n",
      "Epoch 9 | Batch 1271/2072 | Loss: 0.1442\n",
      "Epoch 9 | Batch 1281/2072 | Loss: 0.1956\n",
      "Epoch 9 | Batch 1291/2072 | Loss: 0.0969\n",
      "Epoch 9 | Batch 1301/2072 | Loss: 0.0579\n",
      "Epoch 9 | Batch 1311/2072 | Loss: 0.2110\n",
      "Epoch 9 | Batch 1321/2072 | Loss: 0.1434\n",
      "Epoch 9 | Batch 1331/2072 | Loss: 0.0888\n",
      "Epoch 9 | Batch 1341/2072 | Loss: 0.0103\n",
      "Epoch 9 | Batch 1351/2072 | Loss: 0.0539\n",
      "Epoch 9 | Batch 1361/2072 | Loss: 0.1552\n",
      "Epoch 9 | Batch 1371/2072 | Loss: 0.0969\n",
      "Epoch 9 | Batch 1381/2072 | Loss: 0.1818\n",
      "Epoch 9 | Batch 1391/2072 | Loss: 0.1140\n",
      "Epoch 9 | Batch 1401/2072 | Loss: 0.3867\n",
      "Epoch 9 | Batch 1411/2072 | Loss: 0.0904\n",
      "Epoch 9 | Batch 1421/2072 | Loss: 0.2020\n",
      "Epoch 9 | Batch 1431/2072 | Loss: 0.8371\n",
      "Epoch 9 | Batch 1441/2072 | Loss: 0.0877\n",
      "Epoch 9 | Batch 1451/2072 | Loss: 0.1866\n",
      "Epoch 9 | Batch 1461/2072 | Loss: 0.1142\n",
      "Epoch 9 | Batch 1471/2072 | Loss: 0.1392\n",
      "Epoch 9 | Batch 1481/2072 | Loss: 0.4145\n",
      "Epoch 9 | Batch 1491/2072 | Loss: 0.1847\n",
      "Epoch 9 | Batch 1501/2072 | Loss: 0.1534\n",
      "Epoch 9 | Batch 1511/2072 | Loss: 0.0286\n",
      "Epoch 9 | Batch 1521/2072 | Loss: 0.0066\n",
      "Epoch 9 | Batch 1531/2072 | Loss: 0.3675\n",
      "Epoch 9 | Batch 1541/2072 | Loss: 0.4039\n",
      "Epoch 9 | Batch 1551/2072 | Loss: 0.0316\n",
      "Epoch 9 | Batch 1561/2072 | Loss: 0.0086\n",
      "Epoch 9 | Batch 1571/2072 | Loss: 0.0150\n",
      "Epoch 9 | Batch 1581/2072 | Loss: 0.3554\n",
      "Epoch 9 | Batch 1591/2072 | Loss: 0.0573\n",
      "Epoch 9 | Batch 1601/2072 | Loss: 0.1584\n",
      "Epoch 9 | Batch 1611/2072 | Loss: 0.0856\n",
      "Epoch 9 | Batch 1621/2072 | Loss: 0.0669\n",
      "Epoch 9 | Batch 1631/2072 | Loss: 0.3926\n",
      "Epoch 9 | Batch 1641/2072 | Loss: 0.0038\n",
      "Epoch 9 | Batch 1651/2072 | Loss: 0.0958\n",
      "Epoch 9 | Batch 1661/2072 | Loss: 0.0868\n",
      "Epoch 9 | Batch 1671/2072 | Loss: 0.0589\n",
      "Epoch 9 | Batch 1681/2072 | Loss: 0.1835\n",
      "Epoch 9 | Batch 1691/2072 | Loss: 0.2563\n",
      "Epoch 9 | Batch 1701/2072 | Loss: 0.0382\n",
      "Epoch 9 | Batch 1711/2072 | Loss: 0.2102\n",
      "Epoch 9 | Batch 1721/2072 | Loss: 0.2188\n",
      "Epoch 9 | Batch 1731/2072 | Loss: 0.1157\n",
      "Epoch 9 | Batch 1741/2072 | Loss: 0.0409\n",
      "Epoch 9 | Batch 1751/2072 | Loss: 0.0050\n",
      "Epoch 9 | Batch 1761/2072 | Loss: 0.1249\n",
      "Epoch 9 | Batch 1771/2072 | Loss: 0.1049\n",
      "Epoch 9 | Batch 1781/2072 | Loss: 0.0682\n",
      "Epoch 9 | Batch 1791/2072 | Loss: 0.0193\n",
      "Epoch 9 | Batch 1801/2072 | Loss: 0.6781\n",
      "Epoch 9 | Batch 1811/2072 | Loss: 0.0150\n",
      "Epoch 9 | Batch 1821/2072 | Loss: 0.0311\n",
      "Epoch 9 | Batch 1831/2072 | Loss: 0.2610\n",
      "Epoch 9 | Batch 1841/2072 | Loss: 0.3283\n",
      "Epoch 9 | Batch 1851/2072 | Loss: 0.1877\n",
      "Epoch 9 | Batch 1861/2072 | Loss: 0.2376\n",
      "Epoch 9 | Batch 1871/2072 | Loss: 0.0163\n",
      "Epoch 9 | Batch 1881/2072 | Loss: 0.3741\n",
      "Epoch 9 | Batch 1891/2072 | Loss: 0.0081\n",
      "Epoch 9 | Batch 1901/2072 | Loss: 0.0590\n",
      "Epoch 9 | Batch 1911/2072 | Loss: 0.0099\n",
      "Epoch 9 | Batch 1921/2072 | Loss: 0.0354\n",
      "Epoch 9 | Batch 1931/2072 | Loss: 0.2678\n",
      "Epoch 9 | Batch 1941/2072 | Loss: 0.1250\n",
      "Epoch 9 | Batch 1951/2072 | Loss: 0.0418\n",
      "Epoch 9 | Batch 1961/2072 | Loss: 0.2719\n",
      "Epoch 9 | Batch 1971/2072 | Loss: 0.0460\n",
      "Epoch 9 | Batch 1981/2072 | Loss: 0.0662\n",
      "Epoch 9 | Batch 1991/2072 | Loss: 0.0118\n",
      "Epoch 9 | Batch 2001/2072 | Loss: 0.0189\n",
      "Epoch 9 | Batch 2011/2072 | Loss: 0.0102\n",
      "Epoch 9 | Batch 2021/2072 | Loss: 0.0337\n",
      "Epoch 9 | Batch 2031/2072 | Loss: 0.0703\n",
      "Epoch 9 | Batch 2041/2072 | Loss: 0.1818\n",
      "Epoch 9 | Batch 2051/2072 | Loss: 0.0259\n",
      "Epoch 9 | Batch 2061/2072 | Loss: 0.1877\n",
      "Epoch 9 | Batch 2071/2072 | Loss: 0.1389\n",
      "[Epoch 9] Train Acc: 0.9638, Val Acc: 0.9750, Val LogLoss: 0.0672, Loss: 233.1835\n",
      "‚úÖ Model improved. Saved to best_model.pth\n",
      "Epoch 10 | Batch 1/2072 | Loss: 0.0342\n",
      "Epoch 10 | Batch 11/2072 | Loss: 0.0801\n",
      "Epoch 10 | Batch 21/2072 | Loss: 0.0832\n",
      "Epoch 10 | Batch 31/2072 | Loss: 0.3841\n",
      "Epoch 10 | Batch 41/2072 | Loss: 0.0139\n",
      "Epoch 10 | Batch 51/2072 | Loss: 0.0236\n",
      "Epoch 10 | Batch 61/2072 | Loss: 0.0238\n",
      "Epoch 10 | Batch 71/2072 | Loss: 0.0555\n",
      "Epoch 10 | Batch 81/2072 | Loss: 0.0029\n",
      "Epoch 10 | Batch 91/2072 | Loss: 0.0253\n",
      "Epoch 10 | Batch 101/2072 | Loss: 0.0153\n",
      "Epoch 10 | Batch 111/2072 | Loss: 0.0391\n",
      "Epoch 10 | Batch 121/2072 | Loss: 0.0653\n",
      "Epoch 10 | Batch 131/2072 | Loss: 0.0117\n",
      "Epoch 10 | Batch 141/2072 | Loss: 0.1993\n",
      "Epoch 10 | Batch 151/2072 | Loss: 0.0071\n",
      "Epoch 10 | Batch 161/2072 | Loss: 0.0851\n",
      "Epoch 10 | Batch 171/2072 | Loss: 0.0743\n",
      "Epoch 10 | Batch 181/2072 | Loss: 0.2550\n",
      "Epoch 10 | Batch 191/2072 | Loss: 0.2855\n",
      "Epoch 10 | Batch 201/2072 | Loss: 0.0077\n",
      "Epoch 10 | Batch 211/2072 | Loss: 0.0955\n",
      "Epoch 10 | Batch 221/2072 | Loss: 0.0457\n",
      "Epoch 10 | Batch 231/2072 | Loss: 0.0303\n",
      "Epoch 10 | Batch 241/2072 | Loss: 0.0731\n",
      "Epoch 10 | Batch 251/2072 | Loss: 0.3598\n",
      "Epoch 10 | Batch 261/2072 | Loss: 0.0839\n",
      "Epoch 10 | Batch 271/2072 | Loss: 0.0127\n",
      "Epoch 10 | Batch 281/2072 | Loss: 0.0634\n",
      "Epoch 10 | Batch 291/2072 | Loss: 0.0486\n",
      "Epoch 10 | Batch 301/2072 | Loss: 0.1178\n",
      "Epoch 10 | Batch 311/2072 | Loss: 0.5321\n",
      "Epoch 10 | Batch 321/2072 | Loss: 0.0140\n",
      "Epoch 10 | Batch 331/2072 | Loss: 0.0800\n",
      "Epoch 10 | Batch 341/2072 | Loss: 0.2353\n",
      "Epoch 10 | Batch 351/2072 | Loss: 0.0114\n",
      "Epoch 10 | Batch 361/2072 | Loss: 0.0468\n",
      "Epoch 10 | Batch 371/2072 | Loss: 0.0622\n",
      "Epoch 10 | Batch 381/2072 | Loss: 0.1628\n",
      "Epoch 10 | Batch 391/2072 | Loss: 0.1979\n",
      "Epoch 10 | Batch 401/2072 | Loss: 0.2823\n",
      "Epoch 10 | Batch 411/2072 | Loss: 0.0640\n",
      "Epoch 10 | Batch 421/2072 | Loss: 0.0561\n",
      "Epoch 10 | Batch 431/2072 | Loss: 0.1478\n",
      "Epoch 10 | Batch 441/2072 | Loss: 0.0382\n",
      "Epoch 10 | Batch 451/2072 | Loss: 0.2888\n",
      "Epoch 10 | Batch 461/2072 | Loss: 0.0288\n",
      "Epoch 10 | Batch 471/2072 | Loss: 0.0381\n",
      "Epoch 10 | Batch 481/2072 | Loss: 0.2527\n",
      "Epoch 10 | Batch 491/2072 | Loss: 0.0909\n",
      "Epoch 10 | Batch 501/2072 | Loss: 0.1785\n",
      "Epoch 10 | Batch 511/2072 | Loss: 0.0355\n",
      "Epoch 10 | Batch 521/2072 | Loss: 0.0977\n",
      "Epoch 10 | Batch 531/2072 | Loss: 0.1255\n",
      "Epoch 10 | Batch 541/2072 | Loss: 0.0303\n",
      "Epoch 10 | Batch 551/2072 | Loss: 0.1304\n",
      "Epoch 10 | Batch 561/2072 | Loss: 0.0136\n",
      "Epoch 10 | Batch 571/2072 | Loss: 0.2012\n",
      "Epoch 10 | Batch 581/2072 | Loss: 0.0253\n",
      "Epoch 10 | Batch 591/2072 | Loss: 0.0272\n",
      "Epoch 10 | Batch 601/2072 | Loss: 0.1547\n",
      "Epoch 10 | Batch 611/2072 | Loss: 0.0311\n",
      "Epoch 10 | Batch 621/2072 | Loss: 0.0748\n",
      "Epoch 10 | Batch 631/2072 | Loss: 0.0843\n",
      "Epoch 10 | Batch 641/2072 | Loss: 0.1739\n",
      "Epoch 10 | Batch 651/2072 | Loss: 0.0881\n",
      "Epoch 10 | Batch 661/2072 | Loss: 0.2646\n",
      "Epoch 10 | Batch 671/2072 | Loss: 0.0534\n",
      "Epoch 10 | Batch 681/2072 | Loss: 0.0351\n",
      "Epoch 10 | Batch 691/2072 | Loss: 0.0419\n",
      "Epoch 10 | Batch 701/2072 | Loss: 0.0177\n",
      "Epoch 10 | Batch 711/2072 | Loss: 0.0427\n",
      "Epoch 10 | Batch 721/2072 | Loss: 0.0085\n",
      "Epoch 10 | Batch 731/2072 | Loss: 0.0331\n",
      "Epoch 10 | Batch 741/2072 | Loss: 0.0559\n",
      "Epoch 10 | Batch 751/2072 | Loss: 0.0570\n",
      "Epoch 10 | Batch 761/2072 | Loss: 0.2387\n",
      "Epoch 10 | Batch 771/2072 | Loss: 0.3609\n",
      "Epoch 10 | Batch 781/2072 | Loss: 0.0960\n",
      "Epoch 10 | Batch 791/2072 | Loss: 0.1999\n",
      "Epoch 10 | Batch 801/2072 | Loss: 0.0523\n",
      "Epoch 10 | Batch 811/2072 | Loss: 0.1365\n",
      "Epoch 10 | Batch 821/2072 | Loss: 0.0023\n",
      "Epoch 10 | Batch 831/2072 | Loss: 0.3889\n",
      "Epoch 10 | Batch 841/2072 | Loss: 0.0408\n",
      "Epoch 10 | Batch 851/2072 | Loss: 0.0218\n",
      "Epoch 10 | Batch 861/2072 | Loss: 0.1205\n",
      "Epoch 10 | Batch 871/2072 | Loss: 0.0741\n",
      "Epoch 10 | Batch 881/2072 | Loss: 0.0435\n",
      "Epoch 10 | Batch 891/2072 | Loss: 0.0420\n",
      "Epoch 10 | Batch 901/2072 | Loss: 0.0274\n",
      "Epoch 10 | Batch 911/2072 | Loss: 0.0732\n",
      "Epoch 10 | Batch 921/2072 | Loss: 0.3696\n",
      "Epoch 10 | Batch 931/2072 | Loss: 0.0882\n",
      "Epoch 10 | Batch 941/2072 | Loss: 0.0127\n",
      "Epoch 10 | Batch 951/2072 | Loss: 0.1385\n",
      "Epoch 10 | Batch 961/2072 | Loss: 0.1931\n",
      "Epoch 10 | Batch 971/2072 | Loss: 0.0459\n",
      "Epoch 10 | Batch 981/2072 | Loss: 0.0049\n",
      "Epoch 10 | Batch 991/2072 | Loss: 0.0681\n",
      "Epoch 10 | Batch 1001/2072 | Loss: 0.0471\n",
      "Epoch 10 | Batch 1011/2072 | Loss: 0.0934\n",
      "Epoch 10 | Batch 1021/2072 | Loss: 0.0208\n",
      "Epoch 10 | Batch 1031/2072 | Loss: 0.0636\n",
      "Epoch 10 | Batch 1041/2072 | Loss: 0.0033\n",
      "Epoch 10 | Batch 1051/2072 | Loss: 0.0097\n",
      "Epoch 10 | Batch 1061/2072 | Loss: 0.0031\n",
      "Epoch 10 | Batch 1071/2072 | Loss: 0.0661\n",
      "Epoch 10 | Batch 1081/2072 | Loss: 0.0082\n",
      "Epoch 10 | Batch 1091/2072 | Loss: 0.0036\n",
      "Epoch 10 | Batch 1101/2072 | Loss: 0.1914\n",
      "Epoch 10 | Batch 1111/2072 | Loss: 0.2377\n",
      "Epoch 10 | Batch 1121/2072 | Loss: 0.0653\n",
      "Epoch 10 | Batch 1131/2072 | Loss: 0.1127\n",
      "Epoch 10 | Batch 1141/2072 | Loss: 0.1165\n",
      "Epoch 10 | Batch 1151/2072 | Loss: 0.2231\n",
      "Epoch 10 | Batch 1161/2072 | Loss: 0.0488\n",
      "Epoch 10 | Batch 1171/2072 | Loss: 0.1152\n",
      "Epoch 10 | Batch 1181/2072 | Loss: 0.0263\n",
      "Epoch 10 | Batch 1191/2072 | Loss: 0.0350\n",
      "Epoch 10 | Batch 1201/2072 | Loss: 0.0433\n",
      "Epoch 10 | Batch 1211/2072 | Loss: 0.0432\n",
      "Epoch 10 | Batch 1221/2072 | Loss: 0.1709\n",
      "Epoch 10 | Batch 1231/2072 | Loss: 0.0052\n",
      "Epoch 10 | Batch 1241/2072 | Loss: 0.0098\n",
      "Epoch 10 | Batch 1251/2072 | Loss: 0.1224\n",
      "Epoch 10 | Batch 1261/2072 | Loss: 0.3508\n",
      "Epoch 10 | Batch 1271/2072 | Loss: 0.0072\n",
      "Epoch 10 | Batch 1281/2072 | Loss: 0.0939\n",
      "Epoch 10 | Batch 1291/2072 | Loss: 0.2484\n",
      "Epoch 10 | Batch 1301/2072 | Loss: 0.0268\n",
      "Epoch 10 | Batch 1311/2072 | Loss: 0.0103\n",
      "Epoch 10 | Batch 1321/2072 | Loss: 0.0870\n",
      "Epoch 10 | Batch 1331/2072 | Loss: 0.0023\n",
      "Epoch 10 | Batch 1341/2072 | Loss: 0.0447\n",
      "Epoch 10 | Batch 1351/2072 | Loss: 0.0151\n",
      "Epoch 10 | Batch 1361/2072 | Loss: 0.0393\n",
      "Epoch 10 | Batch 1371/2072 | Loss: 0.0388\n",
      "Epoch 10 | Batch 1381/2072 | Loss: 0.0228\n",
      "Epoch 10 | Batch 1391/2072 | Loss: 0.0044\n",
      "Epoch 10 | Batch 1401/2072 | Loss: 0.0099\n",
      "Epoch 10 | Batch 1411/2072 | Loss: 0.0271\n",
      "Epoch 10 | Batch 1421/2072 | Loss: 0.0124\n",
      "Epoch 10 | Batch 1431/2072 | Loss: 0.0838\n",
      "Epoch 10 | Batch 1441/2072 | Loss: 0.2039\n",
      "Epoch 10 | Batch 1451/2072 | Loss: 0.0628\n",
      "Epoch 10 | Batch 1461/2072 | Loss: 0.1952\n",
      "Epoch 10 | Batch 1471/2072 | Loss: 0.0436\n",
      "Epoch 10 | Batch 1481/2072 | Loss: 0.0069\n",
      "Epoch 10 | Batch 1491/2072 | Loss: 0.0411\n",
      "Epoch 10 | Batch 1501/2072 | Loss: 0.0710\n",
      "Epoch 10 | Batch 1511/2072 | Loss: 0.0526\n",
      "Epoch 10 | Batch 1521/2072 | Loss: 0.0114\n",
      "Epoch 10 | Batch 1531/2072 | Loss: 0.1036\n",
      "Epoch 10 | Batch 1541/2072 | Loss: 0.0838\n",
      "Epoch 10 | Batch 1551/2072 | Loss: 0.0024\n",
      "Epoch 10 | Batch 1561/2072 | Loss: 0.1138\n",
      "Epoch 10 | Batch 1571/2072 | Loss: 0.1994\n",
      "Epoch 10 | Batch 1581/2072 | Loss: 0.0477\n",
      "Epoch 10 | Batch 1591/2072 | Loss: 0.2738\n",
      "Epoch 10 | Batch 1601/2072 | Loss: 0.0114\n",
      "Epoch 10 | Batch 1611/2072 | Loss: 0.2881\n",
      "Epoch 10 | Batch 1621/2072 | Loss: 0.2172\n",
      "Epoch 10 | Batch 1631/2072 | Loss: 0.1683\n",
      "Epoch 10 | Batch 1641/2072 | Loss: 0.0325\n",
      "Epoch 10 | Batch 1651/2072 | Loss: 0.0360\n",
      "Epoch 10 | Batch 1661/2072 | Loss: 0.0055\n",
      "Epoch 10 | Batch 1671/2072 | Loss: 0.0405\n",
      "Epoch 10 | Batch 1681/2072 | Loss: 0.0584\n",
      "Epoch 10 | Batch 1691/2072 | Loss: 0.1101\n",
      "Epoch 10 | Batch 1701/2072 | Loss: 0.0073\n",
      "Epoch 10 | Batch 1711/2072 | Loss: 0.2509\n",
      "Epoch 10 | Batch 1721/2072 | Loss: 0.0273\n",
      "Epoch 10 | Batch 1731/2072 | Loss: 0.0088\n",
      "Epoch 10 | Batch 1741/2072 | Loss: 0.2717\n",
      "Epoch 10 | Batch 1751/2072 | Loss: 0.0225\n",
      "Epoch 10 | Batch 1761/2072 | Loss: 0.0150\n",
      "Epoch 10 | Batch 1771/2072 | Loss: 0.1151\n",
      "Epoch 10 | Batch 1781/2072 | Loss: 0.4611\n",
      "Epoch 10 | Batch 1791/2072 | Loss: 0.0329\n",
      "Epoch 10 | Batch 1801/2072 | Loss: 0.0831\n",
      "Epoch 10 | Batch 1811/2072 | Loss: 0.0506\n",
      "Epoch 10 | Batch 1821/2072 | Loss: 0.3118\n",
      "Epoch 10 | Batch 1831/2072 | Loss: 0.1625\n",
      "Epoch 10 | Batch 1841/2072 | Loss: 0.2437\n",
      "Epoch 10 | Batch 1851/2072 | Loss: 0.2002\n",
      "Epoch 10 | Batch 1861/2072 | Loss: 0.0496\n",
      "Epoch 10 | Batch 1871/2072 | Loss: 0.0376\n",
      "Epoch 10 | Batch 1881/2072 | Loss: 0.0906\n",
      "Epoch 10 | Batch 1891/2072 | Loss: 0.0302\n",
      "Epoch 10 | Batch 1901/2072 | Loss: 0.2598\n",
      "Epoch 10 | Batch 1911/2072 | Loss: 0.1785\n",
      "Epoch 10 | Batch 1921/2072 | Loss: 0.3242\n",
      "Epoch 10 | Batch 1931/2072 | Loss: 0.1622\n",
      "Epoch 10 | Batch 1941/2072 | Loss: 0.0159\n",
      "Epoch 10 | Batch 1951/2072 | Loss: 0.0173\n",
      "Epoch 10 | Batch 1961/2072 | Loss: 0.0255\n",
      "Epoch 10 | Batch 1971/2072 | Loss: 0.0714\n",
      "Epoch 10 | Batch 1981/2072 | Loss: 0.0394\n",
      "Epoch 10 | Batch 1991/2072 | Loss: 0.0629\n",
      "Epoch 10 | Batch 2001/2072 | Loss: 0.1928\n",
      "Epoch 10 | Batch 2011/2072 | Loss: 0.4202\n",
      "Epoch 10 | Batch 2021/2072 | Loss: 0.1045\n",
      "Epoch 10 | Batch 2031/2072 | Loss: 0.0206\n",
      "Epoch 10 | Batch 2041/2072 | Loss: 0.4678\n",
      "Epoch 10 | Batch 2051/2072 | Loss: 0.0901\n",
      "Epoch 10 | Batch 2061/2072 | Loss: 0.0232\n",
      "Epoch 10 | Batch 2071/2072 | Loss: 0.0540\n",
      "[Epoch 10] Train Acc: 0.9656, Val Acc: 0.9644, Val LogLoss: 0.1043, Loss: 220.8254\n",
      "‚ö†Ô∏è No improvement. EarlyStopping counter: 1/5\n"
     ]
    }
   ],
   "source": [
    "# Ï¥àÍ∏∞Ìôî\n",
    "best_loss = float('inf')\n",
    "patience = 5  # Î™á epochÍπåÏßÄ Í∞úÏÑ†Ïù¥ ÏóÜÏùÑ Îïå Î©àÏ∂úÏßÄ\n",
    "counter = 0\n",
    "save_path = 'best_model.pth'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Batch {i+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # üîç Validation Accuracy + Log Loss\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=np.arange(num_classes))\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val LogLoss: {val_logloss:.4f}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "    # ‚úÖ Early Stopping Î°úÏßÅ\n",
    "    if val_logloss < best_loss:\n",
    "        best_loss = val_logloss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"‚úÖ Model improved. Saved to {save_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement. EarlyStopping counter: {counter}/{patience}\")\n",
    "        if counter >= patience:\n",
    "            print(\"‚õî Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c06bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: resnet18_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏ Ï†ÄÏû•\n",
    "torch.save(model.state_dict(), \"resnet18_finetuned.pth\")\n",
    "print(\"‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: resnet18_finetuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
