{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OdM98q3YTu9q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_NUM = 1\n",
        "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.set_device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h_z37-sCVW_6"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    \"IMG_SIZE\": 224,\n",
        "    \"NUM_CLASSES\": 396,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 500,\n",
        "    \"LR\": 1e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-2,\n",
        "    \"PATIENCE\": 5,\n",
        "    \"SEED\": 42,\n",
        "    # AMP & Grad‑clip\n",
        "    \"USE_AMP\": False,\n",
        "    \"GRAD_CLIP\": 1.0,\n",
        "}\n",
        "\n",
        "BASE_DIR = Path(r\"D:/dacon_HAI\")\n",
        "PATHS = {\n",
        "    \"BASE\": BASE_DIR,\n",
        "    \"TRAIN\": BASE_DIR / \"open\" / \"train\",\n",
        "    \"TEST\":  BASE_DIR / \"open\" / \"test\",\n",
        "    \"CKPT\":  BASE_DIR / \"checkpoints\",\n",
        "    \"LOG\":   BASE_DIR / \"logs\",\n",
        "    \"SUBMIT\": BASE_DIR / \"submission\",\n",
        "}\n",
        "for p in PATHS.values():\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Normalization stats ---\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FXjhJninT2py"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "seed_everything(CFG[\"SEED\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdr2jbS-Oixh"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = []\n",
        "\n",
        "        if self.is_test:\n",
        "            for fname in sorted(os.listdir(root_dir)):\n",
        "                if fname.lower().endswith('.jpg'):\n",
        "                    img_path = os.path.join(root_dir, fname)\n",
        "                    self.samples.append((img_path,))\n",
        "        else:\n",
        "            self.classes = sorted(os.listdir(root_dir))\n",
        "            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "            for cls_name in self.classes:\n",
        "                cls_folder = os.path.join(root_dir, cls_name)\n",
        "                for fname in os.listdir(cls_folder):\n",
        "                    if fname.lower().endswith('.jpg'):\n",
        "                        img_path = os.path.join(cls_folder, fname)\n",
        "                        label = self.class_to_idx[cls_name]\n",
        "                        self.samples.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            img_path = self.samples[idx][0]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            img_path, label = self.samples[idx]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "spatial_aug = transforms.RandomChoice([\n",
        "    transforms.RandomResizedCrop(CFG[\"IMG_SIZE\"], scale=(0.8, 1.0)),\n",
        "    transforms.CenterCrop(CFG[\"IMG_SIZE\"]),\n",
        "])\n",
        "geom_aug = transforms.RandomChoice([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.RandomVerticalFlip(p=1.0),\n",
        "    transforms.RandomRotation(degrees=(-30, 30), interpolation=transforms.InterpolationMode.BILINEAR, fill=0),\n",
        "])\n",
        "pixel_aug = transforms.RandomChoice([\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 5.0)),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "])\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "    transforms.RandomApply([spatial_aug], p=0.5),\n",
        "    transforms.RandomApply([geom_aug],    p=0.5),\n",
        "    transforms.RandomApply([pixel_aug],   p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKc8X1W9UL1d",
        "outputId": "910f38aa-c1fd-4b4d-dcb5-5604176242e0"
      },
      "outputs": [],
      "source": [
        "full_ds = CustomImageDataset(PATHS[\"TRAIN\"], transform=train_tf, is_test=False)\n",
        "print(f\"Full data: {len(full_ds):,}\")\n",
        "\n",
        "targets = [label for _, label in full_ds.samples]\n",
        "class_names = full_ds.classes\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=CFG['SEED']\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = Subset(CustomImageDataset(PATHS[\"TRAIN\"], transform=train_tf, is_test=False), train_idx)\n",
        "val_dataset = Subset(CustomImageDataset(PATHS[\"TRAIN\"], transform=val_tf, is_test=False), val_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "test_root = PATHS[\"TEST\"]\n",
        "test_dataset = CustomImageDataset(test_root, transform=val_tf, is_test=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(f\"Number of train imgs: {len(train_dataset)}, Number of valid imgs: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EfficientNet‑B7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "EfficientNet                                            [1, 396]                  --\n",
            "├─Sequential: 1-1                                       [1, 2560, 7, 7]           --\n",
            "│    └─Conv2dNormActivation: 2-1                        [1, 64, 112, 112]         --\n",
            "│    │    └─Conv2d: 3-1                                 [1, 64, 112, 112]         1,728\n",
            "│    │    └─BatchNorm2d: 3-2                            [1, 64, 112, 112]         128\n",
            "│    │    └─SiLU: 3-3                                   [1, 64, 112, 112]         --\n",
            "│    └─Sequential: 2-2                                  [1, 32, 112, 112]         --\n",
            "│    │    └─MBConv: 3-4                                 [1, 32, 112, 112]         4,944\n",
            "│    │    └─MBConv: 3-5                                 [1, 32, 112, 112]         1,992\n",
            "│    │    └─MBConv: 3-6                                 [1, 32, 112, 112]         1,992\n",
            "│    │    └─MBConv: 3-7                                 [1, 32, 112, 112]         1,992\n",
            "│    └─Sequential: 2-3                                  [1, 48, 56, 56]           --\n",
            "│    │    └─MBConv: 3-8                                 [1, 48, 56, 56]           21,224\n",
            "│    │    └─MBConv: 3-9                                 [1, 48, 56, 56]           38,700\n",
            "│    │    └─MBConv: 3-10                                [1, 48, 56, 56]           38,700\n",
            "│    │    └─MBConv: 3-11                                [1, 48, 56, 56]           38,700\n",
            "│    │    └─MBConv: 3-12                                [1, 48, 56, 56]           38,700\n",
            "│    │    └─MBConv: 3-13                                [1, 48, 56, 56]           38,700\n",
            "│    │    └─MBConv: 3-14                                [1, 48, 56, 56]           38,700\n",
            "│    └─Sequential: 2-4                                  [1, 80, 28, 28]           --\n",
            "│    │    └─MBConv: 3-15                                [1, 80, 28, 28]           52,588\n",
            "│    │    └─MBConv: 3-16                                [1, 80, 28, 28]           110,580\n",
            "│    │    └─MBConv: 3-17                                [1, 80, 28, 28]           110,580\n",
            "│    │    └─MBConv: 3-18                                [1, 80, 28, 28]           110,580\n",
            "│    │    └─MBConv: 3-19                                [1, 80, 28, 28]           110,580\n",
            "│    │    └─MBConv: 3-20                                [1, 80, 28, 28]           110,580\n",
            "│    │    └─MBConv: 3-21                                [1, 80, 28, 28]           110,580\n",
            "│    └─Sequential: 2-5                                  [1, 160, 14, 14]          --\n",
            "│    │    └─MBConv: 3-22                                [1, 160, 14, 14]          141,460\n",
            "│    │    └─MBConv: 3-23                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-24                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-25                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-26                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-27                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-28                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-29                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-30                                [1, 160, 14, 14]          397,800\n",
            "│    │    └─MBConv: 3-31                                [1, 160, 14, 14]          397,800\n",
            "│    └─Sequential: 2-6                                  [1, 224, 14, 14]          --\n",
            "│    │    └─MBConv: 3-32                                [1, 224, 14, 14]          474,728\n",
            "│    │    └─MBConv: 3-33                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-34                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-35                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-36                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-37                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-38                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-39                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-40                                [1, 224, 14, 14]          793,464\n",
            "│    │    └─MBConv: 3-41                                [1, 224, 14, 14]          793,464\n",
            "│    └─Sequential: 2-7                                  [1, 384, 7, 7]            --\n",
            "│    │    └─MBConv: 3-42                                [1, 384, 7, 7]            1,008,824\n",
            "│    │    └─MBConv: 3-43                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-44                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-45                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-46                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-47                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-48                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-49                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-50                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-51                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-52                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-53                                [1, 384, 7, 7]            2,281,824\n",
            "│    │    └─MBConv: 3-54                                [1, 384, 7, 7]            2,281,824\n",
            "│    └─Sequential: 2-8                                  [1, 640, 7, 7]            --\n",
            "│    │    └─MBConv: 3-55                                [1, 640, 7, 7]            2,835,296\n",
            "│    │    └─MBConv: 3-56                                [1, 640, 7, 7]            6,199,200\n",
            "│    │    └─MBConv: 3-57                                [1, 640, 7, 7]            6,199,200\n",
            "│    │    └─MBConv: 3-58                                [1, 640, 7, 7]            6,199,200\n",
            "│    └─Conv2dNormActivation: 2-9                        [1, 2560, 7, 7]           --\n",
            "│    │    └─Conv2d: 3-59                                [1, 2560, 7, 7]           1,638,400\n",
            "│    │    └─BatchNorm2d: 3-60                           [1, 2560, 7, 7]           5,120\n",
            "│    │    └─SiLU: 3-61                                  [1, 2560, 7, 7]           --\n",
            "├─AdaptiveAvgPool2d: 1-2                                [1, 2560, 1, 1]           --\n",
            "├─Sequential: 1-3                                       [1, 396]                  --\n",
            "│    └─Dropout: 2-10                                    [1, 2560]                 --\n",
            "│    └─Linear: 2-11                                     [1, 396]                  1,014,156\n",
            "=========================================================================================================\n",
            "Total params: 64,801,116\n",
            "Trainable params: 64,801,116\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 5.17\n",
            "=========================================================================================================\n",
            "Input size (MB): 0.60\n",
            "Forward/backward pass size (MB): 640.88\n",
            "Params size (MB): 259.20\n",
            "Estimated Total Size (MB): 900.69\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "weights = EfficientNet_B7_Weights.IMAGENET1K_V1\n",
        "model = efficientnet_b7(weights=weights)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.5, inplace=True),\n",
        "    nn.Linear(in_features, CFG[\"NUM_CLASSES\"])\n",
        ")\n",
        "\n",
        "print(summary(model, input_size=(1, 3, CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"],)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Hyper-params**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
        "\n",
        "steps_per_epoch = len(train_loader)\n",
        "warmup_steps = 5 * steps_per_epoch\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / float(max(1, warmup_steps))\n",
        "    progress = (step - warmup_steps) / float(max(1, CFG[\"EPOCHS\"] * steps_per_epoch - warmup_steps))\n",
        "    return 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress)))\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv6vSEwcYBF2"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 5, delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "        self.stop = False\n",
        "\n",
        "    def __call__(self, metric: float):\n",
        "        if self.best is None or metric < self.best - self.delta:\n",
        "            self.best = metric\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "early_stopper = EarlyStopping(patience=CFG[\"PATIENCE\"], delta=0.001)\n",
        "\n",
        "log_file = PATHS[\"LOG\"] / \"train_efficientnet_b7.log\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(message)s\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(log_file, mode=\"w\"),\n",
        "    ],\n",
        ")\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Train net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:09:56,561 Epoch 1/500 | Time 1353.7s | TrainLoss 5.9780 | ValLoss 5.8960 | ValAcc 2.46% | LogLoss 5.8960\n",
            "2025-06-12 15:09:57,087 [Checkpoint] Saved at epoch 1 (LogLoss 5.8960)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:32:19,163 Epoch 2/500 | Time 1342.1s | TrainLoss 5.6034 | ValLoss 4.3004 | ValAcc 30.48% | LogLoss 4.3006\n",
            "2025-06-12 15:32:19,737 [Checkpoint] Saved at epoch 2 (LogLoss 4.3006)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 15:54:18,941 Epoch 3/500 | Time 1319.2s | TrainLoss 3.3011 | ValLoss 1.5636 | ValAcc 70.88% | LogLoss 1.5635\n",
            "2025-06-12 15:54:19,464 [Checkpoint] Saved at epoch 3 (LogLoss 1.5635)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:16:22,774 Epoch 4/500 | Time 1323.3s | TrainLoss 1.1993 | ValLoss 0.5333 | ValAcc 86.72% | LogLoss 0.5333\n",
            "2025-06-12 16:16:23,313 [Checkpoint] Saved at epoch 4 (LogLoss 0.5333)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 16:38:24,293 Epoch 5/500 | Time 1321.0s | TrainLoss 0.5174 | ValLoss 0.3109 | ValAcc 90.47% | LogLoss 0.3110\n",
            "2025-06-12 16:38:24,838 [Checkpoint] Saved at epoch 5 (LogLoss 0.3110)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:07:17,629 Epoch 6/500 | Time 1732.8s | TrainLoss 0.3230 | ValLoss 0.2286 | ValAcc 92.31% | LogLoss 0.2287\n",
            "2025-06-12 17:07:18,175 [Checkpoint] Saved at epoch 6 (LogLoss 0.2287)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 17:36:51,530 Epoch 7/500 | Time 1773.4s | TrainLoss 0.2270 | ValLoss 0.2192 | ValAcc 92.75% | LogLoss 0.2192\n",
            "2025-06-12 17:36:52,072 [Checkpoint] Saved at epoch 7 (LogLoss 0.2192)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 18:06:18,321 Epoch 8/500 | Time 1766.2s | TrainLoss 0.1746 | ValLoss 0.1809 | ValAcc 93.70% | LogLoss 0.1810\n",
            "2025-06-12 18:06:18,832 [Checkpoint] Saved at epoch 8 (LogLoss 0.1810)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 18:32:32,418 Epoch 9/500 | Time 1573.6s | TrainLoss 0.1494 | ValLoss 0.1710 | ValAcc 93.93% | LogLoss 0.1711\n",
            "2025-06-12 18:32:32,944 [Checkpoint] Saved at epoch 9 (LogLoss 0.1711)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 18:57:41,763 Epoch 10/500 | Time 1508.8s | TrainLoss 0.1313 | ValLoss 0.1720 | ValAcc 94.50% | LogLoss 0.1719\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 19:22:55,913 Epoch 11/500 | Time 1514.1s | TrainLoss 0.1139 | ValLoss 0.1691 | ValAcc 94.64% | LogLoss 0.1692\n",
            "2025-06-12 19:22:56,426 [Checkpoint] Saved at epoch 11 (LogLoss 0.1692)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 19:47:59,105 Epoch 12/500 | Time 1502.7s | TrainLoss 0.1036 | ValLoss 0.1645 | ValAcc 94.82% | LogLoss 0.1646\n",
            "2025-06-12 19:47:59,597 [Checkpoint] Saved at epoch 12 (LogLoss 0.1646)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 20:13:26,891 Epoch 13/500 | Time 1527.3s | TrainLoss 0.0945 | ValLoss 0.1504 | ValAcc 95.80% | LogLoss 0.1504\n",
            "2025-06-12 20:13:27,367 [Checkpoint] Saved at epoch 13 (LogLoss 0.1504)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 20:38:39,418 Epoch 14/500 | Time 1512.0s | TrainLoss 0.0835 | ValLoss 0.1680 | ValAcc 95.30% | LogLoss 0.1681\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 21:03:46,764 Epoch 15/500 | Time 1507.3s | TrainLoss 0.0793 | ValLoss 0.1542 | ValAcc 95.57% | LogLoss 0.1542\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 21:29:10,049 Epoch 16/500 | Time 1523.3s | TrainLoss 0.0772 | ValLoss 0.1672 | ValAcc 95.30% | LogLoss 0.1673\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 21:54:31,185 Epoch 17/500 | Time 1521.1s | TrainLoss 0.0661 | ValLoss 0.1449 | ValAcc 95.95% | LogLoss 0.1449\n",
            "2025-06-12 21:54:31,728 [Checkpoint] Saved at epoch 17 (LogLoss 0.1449)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 22:19:37,102 Epoch 18/500 | Time 1505.4s | TrainLoss 0.0579 | ValLoss 0.1636 | ValAcc 95.80% | LogLoss 0.1637\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 22:44:19,433 Epoch 19/500 | Time 1482.3s | TrainLoss 0.0630 | ValLoss 0.1700 | ValAcc 95.77% | LogLoss 0.1701\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 23:08:50,461 Epoch 20/500 | Time 1471.0s | TrainLoss 0.0557 | ValLoss 0.1618 | ValAcc 96.06% | LogLoss 0.1619\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 23:33:17,527 Epoch 21/500 | Time 1467.1s | TrainLoss 0.0549 | ValLoss 0.1543 | ValAcc 96.13% | LogLoss 0.1544\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 23:57:50,979 Epoch 22/500 | Time 1473.5s | TrainLoss 0.0440 | ValLoss 0.1958 | ValAcc 95.47% | LogLoss 0.1959\n",
            "2025-06-12 23:57:50,981 Early stopping triggered.\n",
            "2025-06-12 23:57:50,982 Total training time: 9:10:28\n"
          ]
        }
      ],
      "source": [
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"[{epoch}/{CFG['EPOCHS']}] Train\", leave=False)\n",
        "    for step, (imgs, labels) in enumerate(pbar, start=1):\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_probs.append(F.softmax(logits, dim=1).cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    class_idx = list(range(CFG[\"NUM_CLASSES\"]))\n",
        "    answer_df = pd.DataFrame({\"ID\": np.arange(len(all_labels)), \"label\": all_labels})\n",
        "    submission_df = pd.DataFrame(all_probs, columns=class_idx)\n",
        "    submission_df.insert(0, \"ID\", submission_df.index)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    logloss = log_loss(answer_df[\"label\"], all_probs, labels=class_idx)\n",
        "\n",
        "    return avg_val_loss, acc, logloss\n",
        "\n",
        "\n",
        "\n",
        "BEST_LOSS = float(\"inf\")\n",
        "start = time.time()\n",
        "for epoch in range(1, CFG[\"EPOCHS\"] + 1):\n",
        "    t0 = time.time()\n",
        "    tr_loss = train_one_epoch(epoch)\n",
        "    val_loss, val_acc, val_logloss = validate(epoch)\n",
        "    epoch_dur = time.time() - t0\n",
        "\n",
        "    logger.info(\n",
        "        f\"Epoch {epoch}/{CFG['EPOCHS']} | \"\n",
        "        f\"Time {epoch_dur:.1f}s | \"\n",
        "        f\"TrainLoss {tr_loss:.4f} | ValLoss {val_loss:.4f} | \"\n",
        "        f\"ValAcc {val_acc:.2f}% | LogLoss {val_logloss:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save best by logloss\n",
        "    if val_logloss < BEST_LOSS:\n",
        "        BEST_LOSS = val_logloss\n",
        "        ckpt_path = PATHS[\"CKPT\"] / \"best_efficientnet_b7_b32.pth\"\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        logger.info(f\"[Checkpoint] Saved at epoch {epoch} (LogLoss {BEST_LOSS:.4f})\")\n",
        "\n",
        "    early_stopper(val_logloss)\n",
        "    if early_stopper.stop:\n",
        "        logger.info(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "logger.info(f\"Total training time: {datetime.timedelta(seconds=int(time.time() - start))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Eval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJJe3UnEgA6y"
      },
      "outputs": [],
      "source": [
        "weights = EfficientNet_B7_Weights.IMAGENET1K_V1\n",
        "model = efficientnet_b7(weights=weights)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.5, inplace=True),\n",
        "    nn.Linear(in_features, CFG[\"NUM_CLASSES\"])\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(PATHS[\"CKPT\"] / \"best_efficientnet_b7_b32.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        for prob in probs.cpu():\n",
        "            result = {\n",
        "                class_names[i]: prob[i].item()\n",
        "                for i in range(len(class_names))\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "pred = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submission\n",
        "submission = pd.read_csv(os.path.join(PATHS[\"SUBMIT\"], 'submission_JN.csv'), encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "class_columns = submission.columns[1:]\n",
        "pred = pred[class_columns]\n",
        "\n",
        "submission[class_columns] = pred.values\n",
        "submission.to_csv(os.path.join(PATHS[\"SUBMIT\"], 'efficientnet_b7_b32_submission_JN.csv'), index=False, encoding='utf-8-sig')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO3li9OAnGka1if4d+usg7h",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1tXZe3gyQHzQnlNjadpS1QxSHl-_dRwZL",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "HAI_project",
      "language": "python",
      "name": "hai_project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
