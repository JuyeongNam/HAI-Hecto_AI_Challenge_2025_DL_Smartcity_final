{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OdM98q3YTu9q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import swin_t, Swin_T_Weights\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_NUM = 1\n",
        "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.set_device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h_z37-sCVW_6"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    \"IMG_SIZE\": 224,\n",
        "    \"NUM_CLASSES\": 396,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 500,\n",
        "    \"LR\": 1e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-2,\n",
        "    \"PATIENCE\": 5,\n",
        "    \"SEED\": 42,\n",
        "    # AMP & Grad‑clip\n",
        "    \"USE_AMP\": False,\n",
        "    \"GRAD_CLIP\": 1.0,\n",
        "}\n",
        "\n",
        "BASE_DIR = Path(r\"D:/dacon_HAI\")\n",
        "PATHS = {\n",
        "    \"BASE\": BASE_DIR,\n",
        "    \"TRAIN\": BASE_DIR / \"open\" / \"train\",\n",
        "    \"TEST\":  BASE_DIR / \"open\" / \"test\",\n",
        "    \"CKPT\":  BASE_DIR / \"checkpoints\",\n",
        "    \"LOG\":   BASE_DIR / \"logs\",\n",
        "    \"SUBMIT\": BASE_DIR / \"submission\",\n",
        "}\n",
        "for p in PATHS.values():\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Normalization stats ---\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FXjhJninT2py"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "seed_everything(CFG[\"SEED\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdr2jbS-Oixh"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = []\n",
        "\n",
        "        if self.is_test:\n",
        "            for fname in sorted(os.listdir(root_dir)):\n",
        "                if fname.lower().endswith('.jpg'):\n",
        "                    img_path = os.path.join(root_dir, fname)\n",
        "                    self.samples.append((img_path,))\n",
        "        else:\n",
        "            self.classes = sorted(os.listdir(root_dir))\n",
        "            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "            for cls_name in self.classes:\n",
        "                cls_folder = os.path.join(root_dir, cls_name)\n",
        "                for fname in os.listdir(cls_folder):\n",
        "                    if fname.lower().endswith('.jpg'):\n",
        "                        img_path = os.path.join(cls_folder, fname)\n",
        "                        label = self.class_to_idx[cls_name]\n",
        "                        self.samples.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            img_path = self.samples[idx][0]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            img_path, label = self.samples[idx]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "spatial_aug = transforms.RandomChoice([\n",
        "    transforms.RandomResizedCrop(CFG[\"IMG_SIZE\"], scale=(0.8, 1.0)),\n",
        "    transforms.CenterCrop(CFG[\"IMG_SIZE\"]),\n",
        "])\n",
        "geom_aug = transforms.RandomChoice([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.RandomVerticalFlip(p=1.0),\n",
        "    transforms.RandomRotation(degrees=(-30, 30), interpolation=transforms.InterpolationMode.BILINEAR, fill=0),\n",
        "])\n",
        "pixel_aug = transforms.RandomChoice([\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 5.0)),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "])\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "    transforms.RandomApply([spatial_aug], p=0.5),\n",
        "    transforms.RandomApply([geom_aug],    p=0.5),\n",
        "    transforms.RandomApply([pixel_aug],   p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKc8X1W9UL1d",
        "outputId": "910f38aa-c1fd-4b4d-dcb5-5604176242e0"
      },
      "outputs": [],
      "source": [
        "full_ds = CustomImageDataset(PATHS[\"TRAIN\"], transform=train_tf, is_test=False)\n",
        "print(f\"Full data: {len(full_ds):,}\")\n",
        "\n",
        "targets = [label for _, label in full_ds.samples]\n",
        "class_names = full_ds.classes\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=CFG['SEED']\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = Subset(CustomImageDataset(PATHS[\"TRAIN\"], transform=train_tf, is_test=False), train_idx)\n",
        "val_dataset = Subset(CustomImageDataset(PATHS[\"TRAIN\"], transform=val_tf, is_test=False), val_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "test_root = PATHS[\"TEST\"]\n",
        "test_dataset = CustomImageDataset(test_root, transform=val_tf, is_test=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(f\"Number of train imgs: {len(train_dataset)}, Number of valid imgs: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Swin Transformer-Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "SwinTransformer                                    [1, 396]                  --\n",
            "├─Sequential: 1-1                                  [1, 7, 7, 768]            --\n",
            "│    └─Sequential: 2-1                             [1, 56, 56, 96]           --\n",
            "│    │    └─Conv2d: 3-1                            [1, 96, 56, 56]           4,704\n",
            "│    │    └─Permute: 3-2                           [1, 56, 56, 96]           --\n",
            "│    │    └─LayerNorm: 3-3                         [1, 56, 56, 96]           192\n",
            "│    └─Sequential: 2-2                             [1, 56, 56, 96]           --\n",
            "│    │    └─SwinTransformerBlock: 3-4              [1, 56, 56, 96]           112,347\n",
            "│    │    └─SwinTransformerBlock: 3-5              [1, 56, 56, 96]           112,347\n",
            "│    └─PatchMerging: 2-3                           [1, 28, 28, 192]          --\n",
            "│    │    └─LayerNorm: 3-6                         [1, 28, 28, 384]          768\n",
            "│    │    └─Linear: 3-7                            [1, 28, 28, 192]          73,728\n",
            "│    └─Sequential: 2-4                             [1, 28, 28, 192]          --\n",
            "│    │    └─SwinTransformerBlock: 3-8              [1, 28, 28, 192]          445,878\n",
            "│    │    └─SwinTransformerBlock: 3-9              [1, 28, 28, 192]          445,878\n",
            "│    └─PatchMerging: 2-5                           [1, 14, 14, 384]          --\n",
            "│    │    └─LayerNorm: 3-10                        [1, 14, 14, 768]          1,536\n",
            "│    │    └─Linear: 3-11                           [1, 14, 14, 384]          294,912\n",
            "│    └─Sequential: 2-6                             [1, 14, 14, 384]          --\n",
            "│    │    └─SwinTransformerBlock: 3-12             [1, 14, 14, 384]          1,776,492\n",
            "│    │    └─SwinTransformerBlock: 3-13             [1, 14, 14, 384]          1,776,492\n",
            "│    │    └─SwinTransformerBlock: 3-14             [1, 14, 14, 384]          1,776,492\n",
            "│    │    └─SwinTransformerBlock: 3-15             [1, 14, 14, 384]          1,776,492\n",
            "│    │    └─SwinTransformerBlock: 3-16             [1, 14, 14, 384]          1,776,492\n",
            "│    │    └─SwinTransformerBlock: 3-17             [1, 14, 14, 384]          1,776,492\n",
            "│    └─PatchMerging: 2-7                           [1, 7, 7, 768]            --\n",
            "│    │    └─LayerNorm: 3-18                        [1, 7, 7, 1536]           3,072\n",
            "│    │    └─Linear: 3-19                           [1, 7, 7, 768]            1,179,648\n",
            "│    └─Sequential: 2-8                             [1, 7, 7, 768]            --\n",
            "│    │    └─SwinTransformerBlock: 3-20             [1, 7, 7, 768]            7,091,928\n",
            "│    │    └─SwinTransformerBlock: 3-21             [1, 7, 7, 768]            7,091,928\n",
            "├─LayerNorm: 1-2                                   [1, 7, 7, 768]            1,536\n",
            "├─Permute: 1-3                                     [1, 768, 7, 7]            --\n",
            "├─AdaptiveAvgPool2d: 1-4                           [1, 768, 1, 1]            --\n",
            "├─Flatten: 1-5                                     [1, 768]                  --\n",
            "├─Linear: 1-6                                      [1, 396]                  304,524\n",
            "====================================================================================================\n",
            "Total params: 27,823,878\n",
            "Trainable params: 27,823,878\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 33.90\n",
            "====================================================================================================\n",
            "Input size (MB): 0.60\n",
            "Forward/backward pass size (MB): 91.52\n",
            "Params size (MB): 76.63\n",
            "Estimated Total Size (MB): 168.75\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "weights = Swin_T_Weights.IMAGENET1K_V1        # Tiny, 224×224\n",
        "model   = swin_t(weights=weights)\n",
        "model.head = nn.Linear(model.head.in_features, CFG[\"NUM_CLASSES\"])\n",
        "\n",
        "print(summary(model, input_size=(1, 3, CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"],)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Hyper-params**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv6vSEwcYBF2"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
        "\n",
        "steps_per_epoch = len(train_loader)\n",
        "warmup_steps = 5 * steps_per_epoch\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / float(max(1, warmup_steps))\n",
        "    progress = (step - warmup_steps) / float(max(1, CFG[\"EPOCHS\"] * steps_per_epoch - warmup_steps))\n",
        "    return 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress)))\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 5, delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "        self.stop = False\n",
        "\n",
        "    def __call__(self, metric: float):\n",
        "        if self.best is None or metric < self.best - self.delta:\n",
        "            self.best = metric\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "early_stopper = EarlyStopping(patience=CFG[\"PATIENCE\"], delta=0.001)\n",
        "\n",
        "\n",
        "log_file = PATHS[\"LOG\"] / \"train_Swin_tiny_250612.log\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(message)s\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(log_file, mode=\"w\"),\n",
        "    ],\n",
        ")\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Train net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRkN_fcGYs0J",
        "outputId": "5868d889-ffbe-4cc5-996f-8ad648e210d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 09:29:44,206 Epoch 1/500 | Time 555.2s | TrainLoss 5.9819 | ValLoss 5.9771 | ValAcc 0.36% | LogLoss 5.9764\n",
            "2025-06-12 09:29:44,355 [Checkpoint] Saved at epoch 1 (LogLoss 5.9764)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 09:38:57,289 Epoch 2/500 | Time 552.9s | TrainLoss 5.8281 | ValLoss 5.4299 | ValAcc 2.45% | LogLoss 5.3212\n",
            "2025-06-12 09:38:57,457 [Checkpoint] Saved at epoch 2 (LogLoss 5.3212)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 09:48:15,156 Epoch 3/500 | Time 557.7s | TrainLoss 5.1878 | ValLoss 4.3825 | ValAcc 13.54% | LogLoss 3.9798\n",
            "2025-06-12 09:48:15,316 [Checkpoint] Saved at epoch 3 (LogLoss 3.9798)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 09:57:16,612 Epoch 4/500 | Time 541.3s | TrainLoss 4.3794 | ValLoss 3.4300 | ValAcc 33.32% | LogLoss 2.7183\n",
            "2025-06-12 09:57:16,793 [Checkpoint] Saved at epoch 4 (LogLoss 2.7183)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 10:06:12,561 Epoch 5/500 | Time 535.8s | TrainLoss 3.7319 | ValLoss 2.8494 | ValAcc 51.22% | LogLoss 1.9369\n",
            "2025-06-12 10:06:12,725 [Checkpoint] Saved at epoch 5 (LogLoss 1.9369)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 10:16:24,738 Epoch 6/500 | Time 612.0s | TrainLoss 3.2860 | ValLoss 2.5124 | ValAcc 62.59% | LogLoss 1.4930\n",
            "2025-06-12 10:16:24,919 [Checkpoint] Saved at epoch 6 (LogLoss 1.4930)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 10:25:38,106 Epoch 7/500 | Time 553.2s | TrainLoss 2.9785 | ValLoss 2.3780 | ValAcc 66.49% | LogLoss 1.3888\n",
            "2025-06-12 10:25:38,262 [Checkpoint] Saved at epoch 7 (LogLoss 1.3888)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 10:34:20,059 Epoch 8/500 | Time 521.8s | TrainLoss 2.7598 | ValLoss 2.1836 | ValAcc 74.28% | LogLoss 1.1758\n",
            "2025-06-12 10:34:20,219 [Checkpoint] Saved at epoch 8 (LogLoss 1.1758)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 10:43:15,747 Epoch 9/500 | Time 535.5s | TrainLoss 2.6164 | ValLoss 2.0972 | ValAcc 77.36% | LogLoss 1.0901\n",
            "2025-06-12 10:43:15,901 [Checkpoint] Saved at epoch 9 (LogLoss 1.0901)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 10:52:07,042 Epoch 10/500 | Time 531.1s | TrainLoss 2.4919 | ValLoss 2.0020 | ValAcc 80.13% | LogLoss 0.9425\n",
            "2025-06-12 10:52:07,419 [Checkpoint] Saved at epoch 10 (LogLoss 0.9425)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:01:07,108 Epoch 11/500 | Time 539.7s | TrainLoss 2.3746 | ValLoss 1.9401 | ValAcc 82.40% | LogLoss 0.8839\n",
            "2025-06-12 11:01:07,270 [Checkpoint] Saved at epoch 11 (LogLoss 0.8839)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:09:51,850 Epoch 12/500 | Time 524.6s | TrainLoss 2.2936 | ValLoss 1.9092 | ValAcc 83.05% | LogLoss 0.9048\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:18:35,364 Epoch 13/500 | Time 523.5s | TrainLoss 2.2163 | ValLoss 1.8539 | ValAcc 85.49% | LogLoss 0.8496\n",
            "2025-06-12 11:18:35,522 [Checkpoint] Saved at epoch 13 (LogLoss 0.8496)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:27:28,814 Epoch 14/500 | Time 533.3s | TrainLoss 2.1556 | ValLoss 1.8170 | ValAcc 86.28% | LogLoss 0.7920\n",
            "2025-06-12 11:27:28,983 [Checkpoint] Saved at epoch 14 (LogLoss 0.7920)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:36:07,149 Epoch 15/500 | Time 518.2s | TrainLoss 2.1081 | ValLoss 1.7924 | ValAcc 86.52% | LogLoss 0.7841\n",
            "2025-06-12 11:36:07,301 [Checkpoint] Saved at epoch 15 (LogLoss 0.7841)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:44:10,514 Epoch 16/500 | Time 483.2s | TrainLoss 2.0571 | ValLoss 1.7540 | ValAcc 87.75% | LogLoss 0.7655\n",
            "2025-06-12 11:44:10,670 [Checkpoint] Saved at epoch 16 (LogLoss 0.7655)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:51:56,519 Epoch 17/500 | Time 465.8s | TrainLoss 2.0051 | ValLoss 1.7492 | ValAcc 87.79% | LogLoss 0.7644\n",
            "2025-06-12 11:51:56,683 [Checkpoint] Saved at epoch 17 (LogLoss 0.7644)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 11:59:56,964 Epoch 18/500 | Time 480.3s | TrainLoss 1.9713 | ValLoss 1.7322 | ValAcc 88.95% | LogLoss 0.7808\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:07:44,989 Epoch 19/500 | Time 468.0s | TrainLoss 1.9443 | ValLoss 1.6986 | ValAcc 89.35% | LogLoss 0.7021\n",
            "2025-06-12 12:07:45,143 [Checkpoint] Saved at epoch 19 (LogLoss 0.7021)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:15:29,781 Epoch 20/500 | Time 464.6s | TrainLoss 1.8958 | ValLoss 1.6905 | ValAcc 89.92% | LogLoss 0.7237\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:23:34,231 Epoch 21/500 | Time 484.5s | TrainLoss 1.8802 | ValLoss 1.6709 | ValAcc 89.92% | LogLoss 0.7010\n",
            "2025-06-12 12:23:34,386 [Checkpoint] Saved at epoch 21 (LogLoss 0.7010)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:31:20,044 Epoch 22/500 | Time 465.7s | TrainLoss 1.8437 | ValLoss 1.6387 | ValAcc 91.01% | LogLoss 0.6321\n",
            "2025-06-12 12:31:20,198 [Checkpoint] Saved at epoch 22 (LogLoss 0.6321)\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:39:24,762 Epoch 23/500 | Time 484.6s | TrainLoss 1.8204 | ValLoss 1.6680 | ValAcc 90.50% | LogLoss 0.7321\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:47:11,624 Epoch 24/500 | Time 466.9s | TrainLoss 1.8003 | ValLoss 1.6440 | ValAcc 91.24% | LogLoss 0.7118\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 12:55:04,340 Epoch 25/500 | Time 472.7s | TrainLoss 1.7804 | ValLoss 1.6269 | ValAcc 91.33% | LogLoss 0.6852\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 13:02:57,515 Epoch 26/500 | Time 473.2s | TrainLoss 1.7589 | ValLoss 1.6140 | ValAcc 91.48% | LogLoss 0.6654\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:302: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  self.y_type_ = type_of_target(y, input_name=\"y\")\n",
            "c:\\Users\\USPD\\anaconda3\\envs\\hai_env\\lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
            "  ys_types = set(type_of_target(x) for x in ys)\n",
            "2025-06-12 13:10:43,573 Epoch 27/500 | Time 466.1s | TrainLoss 1.7437 | ValLoss 1.6064 | ValAcc 91.49% | LogLoss 0.6473\n",
            "2025-06-12 13:10:43,574 Early stopping triggered.\n",
            "2025-06-12 13:10:43,575 Total training time: 3:50:14\n"
          ]
        }
      ],
      "source": [
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"[{epoch}/{CFG['EPOCHS']}] Train\", leave=False)\n",
        "    for step, (imgs, labels) in enumerate(pbar, start=1):\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_probs.append(F.softmax(logits, dim=1).cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # LogLoss\n",
        "    class_idx = list(range(CFG[\"NUM_CLASSES\"]))\n",
        "    answer_df = pd.DataFrame({\"ID\": np.arange(len(all_labels)), \"label\": all_labels})\n",
        "    submission_df = pd.DataFrame(all_probs, columns=class_idx)\n",
        "    submission_df.insert(0, \"ID\", submission_df.index)\n",
        "    logloss = log_loss(answer_df[\"label\"], all_probs, labels=class_idx)\n",
        "\n",
        "    return val_loss / len(val_loader), acc, logloss\n",
        "\n",
        "\n",
        "BEST_LOSS = float(\"inf\")\n",
        "start = time.time()\n",
        "for epoch in range(1, CFG[\"EPOCHS\"] + 1):\n",
        "    t0 = time.time()\n",
        "    tr_loss = train_one_epoch(epoch)\n",
        "    val_loss, val_acc, val_logloss = validate(epoch)\n",
        "    epoch_dur = time.time() - t0\n",
        "\n",
        "    logger.info(\n",
        "        f\"Epoch {epoch}/{CFG['EPOCHS']} | \"\n",
        "        f\"Time {epoch_dur:.1f}s | \"\n",
        "        f\"TrainLoss {tr_loss:.4f} | ValLoss {val_loss:.4f} | \"\n",
        "        f\"ValAcc {val_acc:.2f}% | LogLoss {val_logloss:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save best by logloss\n",
        "    if val_logloss < BEST_LOSS:\n",
        "        BEST_LOSS = val_logloss\n",
        "        ckpt_path = PATHS[\"CKPT\"] / \"best_Swin_tiny_250612_b32.pth\"\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        logger.info(f\"[Checkpoint] Saved at epoch {epoch} (LogLoss {BEST_LOSS:.4f})\")\n",
        "\n",
        "    early_stopper(val_logloss)\n",
        "    if early_stopper.stop:\n",
        "        logger.info(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "logger.info(f\"Total training time: {datetime.timedelta(seconds=int(time.time() - start))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Eval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJJe3UnEgA6y"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "# 저장된 모델 로드\n",
        "weights = Swin_T_Weights.IMAGENET1K_V1        # Tiny, 224×224\n",
        "model   = swin_t(weights=weights)\n",
        "model.head = nn.Linear(model.head.in_features, CFG[\"NUM_CLASSES\"])\n",
        "\n",
        "model.load_state_dict(torch.load(PATHS[\"CKPT\"] / \"best_Swin_tiny_250612_b32.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        for prob in probs.cpu():\n",
        "            result = {\n",
        "                class_names[i]: prob[i].item()\n",
        "                for i in range(len(class_names))\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "pred = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submission\n",
        "submission = pd.read_csv(os.path.join(PATHS[\"SUBMIT\"], 'submission_JN.csv'), encoding='utf-8-sig')\n",
        "\n",
        "class_columns = submission.columns[1:]\n",
        "pred = pred[class_columns]\n",
        "\n",
        "submission[class_columns] = pred.values\n",
        "submission.to_csv(os.path.join(PATHS[\"SUBMIT\"], 'Swin_tiny_250612_b32_submission_JN.csv'), index=False, encoding='utf-8-sig')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO3li9OAnGka1if4d+usg7h",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1tXZe3gyQHzQnlNjadpS1QxSHl-_dRwZL",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "HAI_project",
      "language": "python",
      "name": "hai_project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
